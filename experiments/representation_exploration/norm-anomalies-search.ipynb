{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Anomalies by looking at the representation norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdhelper import SD\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "from matplotlib.colors import LogNorm\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from collections import Counter, defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = Path('../random_images_flux/')\n",
    "# dataset = [Image.open(p) for p in sorted(dataset_path.glob('*.jpg'))]\n",
    "dataset = [x['image'] for x in load_dataset('JonasLoos/imagenet_subset', split='train')]\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representations_noise_0 = sd.img2repr(dataset, extract_positions=sd.available_extract_positions, step=0)\n",
    "representations_noise_50 = sd.img2repr(dataset, extract_positions=sd.available_extract_positions, step=50)\n",
    "representations_noise_200 = sd.img2repr(dataset, extract_positions=sd.available_extract_positions, step=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highest Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 'up_blocks[1]'\n",
    "top_n = 20\n",
    "representations = representations_noise_50\n",
    "\n",
    "# Calculate norms for\n",
    "norms = torch.stack([r[pos].squeeze(0) for r in representations]).norm(dim=1)\n",
    "\n",
    "# Get the indices of the top 5 images with highest norms\n",
    "top_n_indices = norms.flatten().argsort(descending=True)[:top_n]\n",
    "\n",
    "# Plot the top 5 images\n",
    "fig, axes = plt.subplots(int(np.ceil(top_n/5)), 5, figsize=(5*2, int(np.ceil(top_n/5))*2+1))\n",
    "axes = axes.flatten()\n",
    "fig.suptitle(f\"Top {top_n} images with highest norms in {pos}\")\n",
    "\n",
    "for i, idx in enumerate(top_n_indices):\n",
    "    img_idx = idx // norms.shape[1]**2\n",
    "    top_image = dataset[img_idx]\n",
    "    \n",
    "    axes[i].imshow(top_image, extent=(0, 1, 0, 1))\n",
    "    axes[i].set_title(f\"{img_idx} - {norms.flatten()[idx]:.2f}\")\n",
    "    \n",
    "    tmp = idx % norms.shape[1]**2\n",
    "    axes[i].plot(tmp % norms.shape[1] / norms.shape[1], \n",
    "                 1 - tmp // norms.shape[1] / norms.shape[1], \n",
    "                 'ro', markersize=10, markeredgecolor='white')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot high norm spot sizes\n",
    "\n",
    "pos = 'up_blocks[1]'\n",
    "representations = representations_noise_50\n",
    "norm_threshold = 0.9\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "reprs_tmp = torch.stack([r[pos].squeeze(0) for r in representations]).to('cuda')\n",
    "norms = reprs_tmp.norm(dim=1).cpu()\n",
    "\n",
    "# get all connected components of high norm tokens\n",
    "for norm_threshold in [0.7, 0.8, 0.9, 0.95, 0.98]:\n",
    "    counter = Counter()\n",
    "    images = defaultdict(list)\n",
    "    for i in trange(len(representations)):\n",
    "        highest_norm = norms[i].max()\n",
    "        argmax = norms[i].argmax().item()\n",
    "        n = norms.shape[1]\n",
    "        k,l = argmax//n, argmax%n\n",
    "        size = 0\n",
    "        neighbors = [(k,l)]\n",
    "        visited = set([(k,l)])\n",
    "        while neighbors:\n",
    "            k,l = neighbors.pop()\n",
    "            if norms[i, k, l] < highest_norm * norm_threshold:\n",
    "                continue\n",
    "            size += 1\n",
    "            for dk, dl in [(-1,0), (1,0), (0,-1), (0,1)]:\n",
    "                k2, l2 = k+dk, l+dl\n",
    "                if 0 <= k2 < n and 0 <= l2 < n:\n",
    "                    if (k2, l2) not in visited:\n",
    "                        visited.add((k2, l2))\n",
    "                        neighbors.append((k2, l2))\n",
    "        counter[size] += 1\n",
    "        images[size].append(i)\n",
    "    plt.bar(counter.keys(), counter.values(), alpha=0.6, label=f'norm threshold {norm_threshold}')\n",
    "    # plt.yscale('log')\n",
    "    plt.xlim(0, 50)\n",
    "plt.title(f'High norm spot sizes for {pos} with norm threshold {norm_threshold}')\n",
    "plt.xlabel('Spot size')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# print first image with norm\n",
    "for key, value in sorted(images.items()):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(2*2+1, 2+1), width_ratios=[1, 1, 0.1])\n",
    "    i = random.choice(value)\n",
    "    fig.suptitle(f'High norm spot of size {key} ({len(value)} images)', fontsize=16)\n",
    "    axs[0].imshow(dataset[i])\n",
    "    axs[0].axis('off')\n",
    "    im = axs[1].imshow(norms[i])\n",
    "    axs[1].axis('off')\n",
    "    \n",
    "    # Make colorbar as high as imshow\n",
    "    cbar = fig.colorbar(im, cax=axs[2], label='Norm')\n",
    "    cbar.ax.set_box_aspect(10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm-norm scatter plot\n",
    "\n",
    "pos = 'up_blocks[1]'\n",
    "\n",
    "\n",
    "axs = plt.subplots(1, 2, figsize=(8, 5), width_ratios=[2, 1])[1]\n",
    "representations = representations_noise_50\n",
    "\n",
    "reprs_tmp = torch.stack([r[pos].squeeze(0).flatten(start_dim=1).T for r in representations]).to('cuda')\n",
    "norms = reprs_tmp.norm(dim=2).cpu()\n",
    "\n",
    "# Plot (selection of) all norms\n",
    "x, y = [], []\n",
    "indices = torch.randint(reprs_tmp.shape[0]*reprs_tmp.shape[1], (10000,))\n",
    "for i in tqdm(indices):\n",
    "    others = reprs_tmp[torch.arange(reprs_tmp.shape[0])!=i//reprs_tmp.shape[1]].flatten(0,1)\n",
    "    argmax = torch.cosine_similarity(reprs_tmp.flatten(0,1)[i][None, :], others, dim=1).argmax().cpu()\n",
    "    x.append(norms.flatten()[i] / norms[i//norms.shape[1]].max())\n",
    "    y.append((norms.flatten()[argmax] / norms[argmax//norms.shape[1],:].max().item()))\n",
    "\n",
    "axs[0].scatter(x, y, alpha=0.25, s=2)\n",
    "axs[1].hist(y, bins=50, orientation='horizontal', density=True)\n",
    "\n",
    "\n",
    "# Plot highest norms\n",
    "x, y = [], []\n",
    "for i in trange(len(reprs_tmp)):\n",
    "    j = norms[i].argmax()\n",
    "    others = reprs_tmp[torch.arange(reprs_tmp.shape[0])!=i].flatten(0,1)\n",
    "    argmax = torch.cosine_similarity(reprs_tmp[i][j][None, :], others, dim=1).argmax().cpu()\n",
    "    x.append(1)\n",
    "    y.append(norms.flatten()[argmax] / norms[argmax//norms.shape[1],:].max().item())\n",
    "\n",
    "axs[0].scatter(x, y, alpha=0.25, s=2)\n",
    "axs[1].hist(y, bins=50, orientation='horizontal', density=True, alpha=0.5)\n",
    "\n",
    "\n",
    "# configure plot\n",
    "axs[0].set_xlabel('Relative Norm of first')\n",
    "axs[0].set_ylabel('Relative Norm of second')\n",
    "# axs[0].set_yscale('log')\n",
    "# axs[0].set_xscale('log')\n",
    "axs[0].set_ylim(0, 1.05)\n",
    "axs[0].set_xlim(0, 1.05)\n",
    "axs[0].set_title('Norms of tokens and their most similar matches')\n",
    "axs[0].legend(['all', 'highest norm only'])\n",
    "\n",
    "axs[1].set_xlabel('Relative Frequency')\n",
    "axs[1].set_yticks([])\n",
    "axs[1].set_ylim(0, 1.05)\n",
    "axs[1].set_title('Distribution of norms')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot norms of tokens that match with highest norm tokens\n",
    "\n",
    "pos = 'up_blocks[1]'\n",
    "representations = representations_noise_50\n",
    "\n",
    "x, y = [], []\n",
    "reprs_tmp = torch.stack([r[pos].squeeze(0).flatten(start_dim=1).T for r in representations])\n",
    "norms = reprs_tmp.norm(dim=2).cpu()\n",
    "for i in trange(len(reprs_tmp)):\n",
    "    j = norms[i].argmax()\n",
    "    others = reprs_tmp[torch.arange(reprs_tmp.shape[0])!=i].flatten(0,1)\n",
    "    argmax = torch.cosine_similarity(reprs_tmp[i][j][None, :], others, dim=1).argmax().cpu()\n",
    "    x.append(norms[i, j])\n",
    "    y.append(norms.flatten()[argmax])\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('Norm of first')\n",
    "plt.ylabel('Norm of second')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Norms vs Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "\n",
    "histograms: dict[str, list[torch.Tensor|None]] = {pos: [None, None, None] for pos in sd.available_extract_positions}\n",
    "for pos in tqdm(sd.available_extract_positions):\n",
    "    for i, representations in enumerate([representations_noise_0, representations_noise_50, representations_noise_200]):\n",
    "        # calculate norms\n",
    "        norms = torch.stack([r[pos].squeeze(0).flatten(start_dim=1).T for r in representations]).norm(dim=2).cpu()\n",
    "        x_min, x_max = -0.5, 1.0\n",
    "        y_min, y_max = norms.min().item(), norms.max().item()\n",
    "\n",
    "        # calculate histogram\n",
    "        # do this in a loop to avoid memory issues\n",
    "        histogram = torch.zeros(n, n)\n",
    "        for r1, n1 in zip(tqdm(representations), norms):\n",
    "            for r2, n2 in zip(representations, norms):\n",
    "                if (np.random.random() > 0.01 and pos in ['conv_in', 'up_blocks[2]', 'up_blocks[3]', 'conv_out']) or (np.random.random() > 0.1 and pos in ['down_blocks[0]', 'up_blocks[1]']):\n",
    "                    continue\n",
    "                # TODO: correct normalization of histogram\n",
    "                data = torch.stack([\n",
    "                    r1.at(pos).cosine_similarity(r2.at(pos)).flatten().cpu(),\n",
    "                    (n1[None,:]+n2[:,None]).flatten()/2\n",
    "                ], dim=1)\n",
    "                histogram += torch.histogramdd(data, bins=n, range=(x_min, x_max, y_min, y_max)).hist.T.flip(0)\n",
    "        histograms[pos][i] = histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pos in sd.available_extract_positions:\n",
    "\n",
    "    plt.figure(figsize=(15, 4))  # Increased width to accommodate colorbar\n",
    "    for i, (representations, noise_level) in enumerate([(representations_noise_0, 0), (representations_noise_50, 50), (representations_noise_200, 200)]):\n",
    "        # calculate norms\n",
    "        norms = torch.stack([r[pos].squeeze(0).flatten(start_dim=1).T for r in representations]).norm(dim=2).cpu()\n",
    "        x_min, x_max = -0.5, 1.0\n",
    "        y_min, y_max = norms.min().item(), norms.max().item()\n",
    "\n",
    "        histogram = histograms[pos][i]\n",
    "        if histogram is None: continue\n",
    "        # plot histogram\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.imshow(histogram/histogram.sum(), norm=LogNorm(vmin=1e-12, vmax=1e-2), cmap='YlOrRd', extent=(x_min, x_max, y_min, y_max), aspect='auto')\n",
    "        plt.ylabel('Average Norm')\n",
    "        plt.xlabel('Cosine Similarity')\n",
    "        plt.title(f'{pos} - Noise: {noise_level}')\n",
    "    sm = plt.cm.ScalarMappable(cmap='YlOrRd', norm=LogNorm(vmin=1e-12, vmax=1e-2))\n",
    "    sm.set_array([])\n",
    "    fig = plt.gcf()\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.015, 0.75])  # [left, bottom, width, height]\n",
    "    cbar = plt.colorbar(sm, cax=cbar_ax)\n",
    "    cbar.set_label('Normalized Frequency')\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 1])  # Adjust layout to make room for colorbar\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(torch.stack([torch.stack(h) for h in histograms.values()]), '../histograms_norm_cossim_sd15.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_limits = torch.zeros(len(sd.available_extract_positions), 3, 2)\n",
    "for pos_i, pos in enumerate(sd.available_extract_positions):\n",
    "    for i, (representations, noise_level) in enumerate([(representations_noise_0, 0), (representations_noise_50, 50), (representations_noise_200, 200)]):\n",
    "        norms = torch.stack([r[pos].squeeze(0).flatten(start_dim=1).T for r in representations]).norm(dim=2).cpu()\n",
    "        y_min, y_max = norms.min().item(), norms.max().item()\n",
    "        y_limits[pos_i, i, 0] = y_min\n",
    "        y_limits[pos_i, i, 1] = y_max\n",
    "# torch.save(y_limits, '../y_limits_norm_cossim_sd15.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
