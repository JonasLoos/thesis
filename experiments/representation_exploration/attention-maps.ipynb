{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot attention maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdhelper import SD\n",
    "from PIL import Image\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SD()\n",
    "\n",
    "if 'flux' in sd.model_name.lower():\n",
    "    sd.quantize(model_cpu_offload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sd.pipeline.unet if hasattr(sd.pipeline, 'unet') else sd.pipeline.transformer\n",
    "\n",
    "# attention related modules in the unet\n",
    "for x in model.named_modules():\n",
    "    if 'attn' in x[0]:\n",
    "        print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '../random_images_flux/0.jpg'\n",
    "Image.open(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get attention blocks\n",
    "attentions = {}\n",
    "for block_name in sd.available_extract_positions:\n",
    "    block = eval(f'model.{block_name}', {'model': model}, None)\n",
    "    if not hasattr(block, 'attentions'):\n",
    "        continue\n",
    "    for j, attention in enumerate(block.attentions):\n",
    "        for k, transformer_block in enumerate(attention.transformer_blocks):\n",
    "            for l, module in enumerate(transformer_block.named_modules()):\n",
    "                if not re.search(r'^attn\\d$', module[0]):\n",
    "                    continue\n",
    "                name = f'{block_name}.attentions[{j}].transformer_blocks[{k}].{module[0]}'\n",
    "                attentions[name] = module[1]\n",
    "\n",
    "# get attention q and k\n",
    "extract_positions = [f'{a}.{l}' for a in attentions.keys() for l in ['to_q', 'to_k']]\n",
    "reprs = sd.img2repr(img_path, extract_positions, 50)\n",
    "\n",
    "for name, attn in attentions.items():\n",
    "    # from: diffusers.models.attention_processor.AttnProcessor.__call__\n",
    "    query = attn.head_to_batch_dim(reprs[name + '.to_q'])\n",
    "    key = attn.head_to_batch_dim(reprs[name + '.to_k'])\n",
    "    attention_probs = attn.get_attention_scores(query, key)\n",
    "    assert attention_probs.ndim == 3\n",
    "\n",
    "    n = int(attention_probs.shape[1]**.5)\n",
    "    print(name, tuple(attention_probs.shape), n)\n",
    "\n",
    "    i0s = [0, 0, 1, 3, 7]\n",
    "    if attention_probs.shape[-1] == 77:\n",
    "        # text attention\n",
    "        i2s = [0, 10, 0, 42, 76]\n",
    "        attn_probs = [(f'[{i0}, :, {i2}]', attention_probs[i0, :, i2]) for i0, i2 in zip(i0s, i2s)]\n",
    "    else:\n",
    "        # image attention\n",
    "        i1s = [0, n-1, 0, n//2*n+n//2, n//3*n+n//3]\n",
    "        i2s = [0, n-1, 0, n//2*n+n//2, n//3*n+n//3]\n",
    "        attn_probs = [(f'[{i0}, {i1}, :]', attention_probs[i0, i1, :]) for i0, i1 in zip(i0s, i1s)] + [(f'[{i0}, :, {i2}]', attention_probs[i0, :, i2]) for i0, i2 in zip(i0s, i2s)]\n",
    "\n",
    "    fig, axes = plt.subplots(len(attn_probs)//5, 5, figsize=(5*3, 3*len(attn_probs)//5))\n",
    "    axes = axes.flatten()\n",
    "    for i, (title, attn_prob) in enumerate(attn_probs):\n",
    "        att_map = attn_prob.reshape(n, n).detach().cpu().numpy()  # works well\n",
    "        im = axes[i].imshow(att_map, cmap='viridis')\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].axis('off')\n",
    "        # plt.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.suptitle(name)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
