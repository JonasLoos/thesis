{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForText2Image\n",
    "from diffusers.models import AutoencoderKL\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset, Dataset, Features, Array3D, concatenate_datasets\n",
    "from datasets.arrow_dataset import logging\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from contextlib import ExitStack\n",
    "from functools import partial\n",
    "from trainplot.trainplot import TrainPlotPlotlyExperimental as TrainPlot\n",
    "from datetime import datetime\n",
    "from torch import nn, optim\n",
    "from time import sleep\n",
    "\n",
    "logging.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, channel_size=1280, spatial_size=8, num_classes=1000):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(channel_size, num_classes)\n",
    "        self.pool = nn.AvgPool2d(spatial_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(self.pool(x), start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SimpleCNNClassifier(nn.Module):\n",
    "    def __init__(self, channel_size=1280, spatial_size=8, num_classes=1000):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(channel_size, num_classes, kernel_size=1, padding=0)\n",
    "        self.pool = nn.MaxPool2d(spatial_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(self.pool(x), start_dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "def check_nans(*args, **kwargs):\n",
    "    for k, v in [*enumerate(args),*kwargs.items()]:\n",
    "        if torch.isnan(v).any():\n",
    "            raise ValueError(f\"Nans in {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process_images(images, pipe, extract_positions: list[str], noise: float) -> dict[str, torch.Tensor]:\n",
    "    pipe.unet.config.addition_embed_type = 'nothing_at_all_lol'\n",
    "    if next(pipe.vae.modules()).dtype == torch.float16:\n",
    "        latents = pipe.vae.encode(images.half().cuda()).latent_dist.sample()\n",
    "    else:\n",
    "        latents = pipe.vae.encode(images.cuda()).latent_dist.sample().half()\n",
    "    check_nans(latents=latents)\n",
    "    # TODO: is normal(0,1) the right noise distribution?\n",
    "    if noise > 0: latents = (1-noise) * latents + noise * torch.randn_like(latents)\n",
    "    # TODO: is an empty prompt the right way to do this?\n",
    "    prompt_embeds, *_ = pipe.encode_prompt(prompt=\"\", device=\"cuda\", num_images_per_prompt=latents.shape[0], do_classifier_free_guidance=False)\n",
    "    check_nans(prompt_embeds=prompt_embeds)\n",
    "\n",
    "    # Run inference with representation extraction hooks\n",
    "    representations = {}\n",
    "    with ExitStack() as stack, torch.no_grad():\n",
    "        for extract_position in extract_positions:\n",
    "            def hook_fn(module, input, output, extract_position):\n",
    "                # print(extract_position, print_shape(output))\n",
    "                if isinstance(output, tuple):\n",
    "                    output = output[0]  # TODO: is it good to always take the first output and ignore the rest?\n",
    "                representations[extract_position] = output\n",
    "            # eval is unsafe. Do not use in production.\n",
    "            stack.enter_context(eval(f'pipe.unet.{extract_position}', {'__builtins__': {}, 'pipe': pipe}).register_forward_hook(partial(hook_fn, extract_position=extract_position)))\n",
    "        # TODO: is this the right number of timesteps?\n",
    "        # TODO: setup sdxl-turbo\n",
    "        pipe.unet(latents, pipe.scheduler.config.num_train_timesteps, encoder_hidden_states=prompt_embeds, return_dict=False)\n",
    "\n",
    "    return representations\n",
    "\n",
    "\n",
    "def transform_to_tensor(ds):\n",
    "    # TODO: maybe batch this\n",
    "    transform_pipeline = transforms.Compose([\n",
    "        transforms.CenterCrop(min(ds['image'].size)),\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    ds['image'] = transform_pipeline(ds['image'].convert(\"RGB\") if ds['image'].mode != \"RGB\" else ds['image'])\n",
    "    return ds\n",
    "\n",
    "\n",
    "def sd_dataset_generator(dataset, pipe, extract_positions, noise_levels, batch_size):\n",
    "    dataloader = DataLoader(dataset.to_iterable_dataset().map(transform_to_tensor), batch_size=batch_size)\n",
    "    for batch in dataloader:\n",
    "        representations = {noise: batch_process_images(batch['image'], pipe, extract_positions, noise) for noise in noise_levels}\n",
    "        for i, label in enumerate(batch['label']):\n",
    "            data = {noise: {pos: representations[noise][pos][i] for pos in extract_positions} for noise in noise_levels}\n",
    "            yield label, data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_loss = TrainPlot(update_period=1)\n",
    "tp_accuracy = TrainPlot(update_period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "generation_batch_size = 4\n",
    "training_batch_size = 64\n",
    "dataset_name = \"cifar100\"\n",
    "dataset_column_rename = {'img':'image', 'fine_label': 'label'}\n",
    "model_name = 'runwayml/stable-diffusion-v1-5'  # e.g. stabilityai/sdxl-turbo or runwayml/stable-diffusion-v1-5\n",
    "model_classes = [SimpleClassifier, SimpleCNNClassifier]\n",
    "optimizer_name = optim.Adam\n",
    "extract_positions = ['down_blocks[0]','down_blocks[3]','mid_block','up_blocks[0]','up_blocks[3]']\n",
    "noise_levels = [0., .5]\n",
    "\n",
    "# load model and dataset\n",
    "dataset = load_dataset(dataset_name, split='train')\n",
    "dataset = dataset.rename_columns(dataset_column_rename)\n",
    "# AutoencoderKL.from_pretrained(\"stabilityai/sdxl-vae\")  # 32 bit AutoencoderKL is required for sdxl\n",
    "pipe = AutoPipelineForText2Image.from_pretrained(model_name, torch_dtype=torch.float16).to(\"cuda\")\n",
    "representation_iter = sd_dataset_generator(dataset, pipe, extract_positions, noise_levels, generation_batch_size)\n",
    "\n",
    "# setup classification models to train\n",
    "tmp = next(sd_dataset_generator(load_dataset(dataset_name, split='train').rename_columns(dataset_column_rename), pipe, extract_positions, [0.], 1))[1][0.]\n",
    "representation_shapes = {k: v.shape for k, v in tmp.items() if k != 'label'}\n",
    "models = {}\n",
    "for model_cls in model_classes:\n",
    "    for pos in extract_positions:\n",
    "        for noise in noise_levels:\n",
    "            models[f'{model_cls.__name__}-{pos}-{noise}'] = model_cls(\n",
    "                channel_size = representation_shapes[pos][0],\n",
    "                spatial_size = representation_shapes[pos][-1],\n",
    "            ).to(\"cuda\")\n",
    "optimizers = {name: optimizer_name(model.parameters(), lr=1e-3) for name, model in models.items()}\n",
    "\n",
    "# train\n",
    "losses = {name: [] for name in models.keys()}\n",
    "accuracies = {name: [] for name in models.keys()}\n",
    "try:\n",
    "    for step in trange(dataset.num_rows // training_batch_size):\n",
    "        labels, representations = zip(*[next(representation_iter) for _ in range(training_batch_size)])\n",
    "        for noise in noise_levels:\n",
    "            for pos in extract_positions:\n",
    "                x = torch.stack([r[noise][pos] for r in representations]).float()\n",
    "                y = torch.tensor(labels).cuda()\n",
    "                check_nans(x=x, y=y)\n",
    "                for cls_name in model_classes:\n",
    "                    name = f'{cls_name.__name__}-{pos}-{noise}'\n",
    "                    model = models[name]\n",
    "                    check_nans(*model.parameters())\n",
    "                    optimizers[name].zero_grad()\n",
    "                    model.train()\n",
    "                    y_hat = model(x)\n",
    "                    loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "                    check_nans(y_hat=y_hat, loss=loss)\n",
    "                    loss.backward()\n",
    "                    optimizers[name].step()\n",
    "                    check_nans(*model.parameters())\n",
    "                    losses[name].append(loss.item())\n",
    "                    accuracies[name].append((y_hat.argmax(dim=1) == y).float().mean().item())\n",
    "                    tp_loss(step=step, **{name: np.mean(losses[name][-100:])})\n",
    "                    tp_accuracy(step=step, **{name: np.mean(accuracies[name][-100:])})\n",
    "except KeyboardInterrupt:\n",
    "    print('Got Keyboard Interrupt')\n",
    "finally:\n",
    "    if step > 1:\n",
    "        print('Saving models...')\n",
    "        folder = Path(f'../classifier-models/onfly-{datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")}')\n",
    "        print(f'Saving models to `{folder}`')\n",
    "        folder.mkdir(parents=True, exist_ok=True)\n",
    "        for name, model in models.items():\n",
    "            torch.save(model.state_dict(), folder / f'{name}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
