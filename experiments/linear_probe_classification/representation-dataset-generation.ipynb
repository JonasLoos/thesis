{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForText2Image, AutoencoderKL\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset, load_from_disk, Dataset, Features, Array3D, concatenate_datasets, DatasetDict\n",
    "from datasets.arrow_dataset import logging\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from contextlib import ExitStack\n",
    "from functools import partial\n",
    "import tempfile\n",
    "import itertools\n",
    "\n",
    "logging.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shape(x):\n",
    "    if isinstance(x, tuple):\n",
    "        return '(' + ', '.join(print_shape(y) for y in x) + ')'\n",
    "    else:\n",
    "        return str(x.shape)\n",
    "\n",
    "\n",
    "def batch_process_images(images, pipe, extract_positions: list[str], noise: float) -> dict[str, np.ndarray]:\n",
    "    pipe.unet.config.addition_embed_type = 'nothing_at_all_lol'\n",
    "    if next(pipe.vae.modules()).dtype == torch.float16:\n",
    "        latents = pipe.vae.encode(images.half().cuda()).latent_dist.sample()\n",
    "    else:\n",
    "        latents = pipe.vae.encode(images.cuda()).latent_dist.sample().half()\n",
    "    # TODO: is normal(0,1) the right noise distribution?\n",
    "    if noise > 0: latents = (1-noise) * latents + noise * torch.randn_like(latents)\n",
    "    # TODO: is an empty prompt the right way to do this?\n",
    "    prompt_embeds, *_ = pipe.encode_prompt(prompt=\"\", device=\"cuda\", num_images_per_prompt=latents.shape[0], do_classifier_free_guidance=False)\n",
    "\n",
    "    # Run inference with representation extraction hooks\n",
    "    representations = {}\n",
    "    with ExitStack() as stack, torch.no_grad():\n",
    "        for extract_position in extract_positions:\n",
    "            def hook_fn(module, input, output, extract_position):\n",
    "                # print(extract_position, print_shape(output))\n",
    "                if isinstance(output, tuple):\n",
    "                    output = output[0].cpu().numpy()  # TODO: is it good to always take the first output and ignore the rest?\n",
    "                representations[extract_position] = output\n",
    "            # eval is unsafe. Do not use in production.\n",
    "            stack.enter_context(eval(f'pipe.unet.{extract_position}', {'__builtins__': {}, 'pipe': pipe}).register_forward_hook(partial(hook_fn, extract_position=extract_position)))\n",
    "        # TODO: is this the right number of timesteps?\n",
    "        # TODO: setup sdxl-turbo\n",
    "        pipe.unet(latents, pipe.scheduler.config.num_train_timesteps, encoder_hidden_states=prompt_embeds, return_dict=False)\n",
    "\n",
    "    return representations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def transform_to_tensor(ds):\n",
    "    # TODO: maybe batch this\n",
    "    transform_pipeline = transforms.Compose([\n",
    "        transforms.CenterCrop(min(ds['image'].size)),\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    ds['image'] = transform_pipeline(ds['image'].convert(\"RGB\") if ds['image'].mode != \"RGB\" else ds['image'])\n",
    "    return ds\n",
    "\n",
    "\n",
    "def sd_dataset_generator(dataloader, pipe, extract_positions, noise, max_samples=None):\n",
    "    status = tqdm(total=max_samples, desc=\"Generating representations\")\n",
    "    for batch in dataloader:\n",
    "        status.update(len(batch['image']))\n",
    "        representations = batch_process_images(batch['image'], pipe, extract_positions, noise)\n",
    "        for i, label in enumerate(batch['label']):\n",
    "            yield {\"label\": label} | {pos: x[i] for pos, x in representations.items()}\n",
    "\n",
    "\n",
    "def create_sd_dataset(\n",
    "        output_path: str = 'sd_representations_dataset',\n",
    "        model_name: str = 'runwayml/stable-diffusion-v1-5',\n",
    "        dataset_name: str = 'imagenet-1k',\n",
    "        extract_positions: list[str] = ['mid_block'],\n",
    "        batch_size: int = 8,\n",
    "        max_samples_per_split: dict[str, int] = {},\n",
    "        noise: float = 0.0,\n",
    "        dataset_column_rename: dict[str, str] = {},\n",
    "        vae = None,\n",
    "    ) -> Dataset:\n",
    "    '''Create a dataset of representations from a Stable Diffusion model.\n",
    "\n",
    "    Notes:\n",
    "        This code is unsafe, do not use in production.\n",
    "\n",
    "    Args:\n",
    "        output_path: Path to save the dataset to.\n",
    "        model_name: Name of the Stable Diffusion model to use.\n",
    "        dataset_name: Name of the dataset to use.\n",
    "        split: Split of the dataset to use.\n",
    "        extract_positions: List of positions in the U-Net to extract representations from. If the output is a tuple, only the first element is used.\n",
    "        batch_size: Batch size to use.\n",
    "        max_samples: Maximum number of samples to use. If None, use the entire dataset.\n",
    "        noise: Noise to add to the latent space. Must be between 0 and 1.\n",
    "    '''\n",
    "    # check arguments\n",
    "    if Path(output_path).exists():\n",
    "        raise ValueError('Output path already exists')\n",
    "    if not 0 <= noise <= 1:\n",
    "        raise ValueError('Noise must be between 0 and 1')\n",
    "\n",
    "    # load model\n",
    "    print(f'loading model `{model_name}`')\n",
    "    if 'sdxl' in model_name:\n",
    "        # sdxl needs 32bit vae\n",
    "        vae = AutoencoderKL.from_pretrained('stabilityai/sdxl-vae')\n",
    "        pipe = AutoPipelineForText2Image.from_pretrained(model_name, torch_dtype=torch.float16, vae=vae).to(\"cuda\")\n",
    "    else:\n",
    "        pipe = AutoPipelineForText2Image.from_pretrained(model_name, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "    # create new dataset\n",
    "    print(f'Setting up new dataset with featues `{extract_positions+[\"label\"]}`')\n",
    "    tmp_dataset_dict = load_dataset(dataset_name)\n",
    "    tmp_dataset = list(tmp_dataset_dict.values())[0].rename_columns(dataset_column_rename)\n",
    "    tmp = batch_process_images(next(iter(DataLoader(tmp_dataset.to_iterable_dataset().take(1).map(transform_to_tensor), batch_size=1)))['image'], pipe, extract_positions, noise)\n",
    "    total_max_samples = sum(max_samples_per_split.get(split, len(tmp_dataset_dict[split])) for split in tmp_dataset_dict.keys())\n",
    "    print(f'The new dataset will be roughly {sum(x.nbytes for x in tmp.values()) * total_max_samples / 1e9:.2f} GB')\n",
    "    features = Features({\n",
    "        **{pos: Array3D(shape=x.shape[1:], dtype=\"float16\") for pos, x in tmp.items() if pos != \"label\"},\n",
    "        \"label\": tmp_dataset.features[\"label\"],\n",
    "    })\n",
    "\n",
    "    # load dataset\n",
    "    with tempfile.TemporaryDirectory() as tmp_ouput_path:\n",
    "        # go through the all splits\n",
    "        tmp_paths = {}\n",
    "        for split in tmp_dataset_dict.keys():\n",
    "            print(f'loading source dataset `{dataset_name}` ({split} split)')\n",
    "            original_dataset = load_dataset(dataset_name, split=split)\n",
    "            max_samples = max_samples_per_split.get(split, len(original_dataset))\n",
    "            dataset = original_dataset.to_iterable_dataset().take(max_samples).rename_columns(dataset_column_rename).map(transform_to_tensor)\n",
    "            dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "            print(f'Generating new dataset for split `{split}`')\n",
    "            tmp_paths[split] = []\n",
    "            for i, batch in enumerate(tqdm(dataloader, total = max_samples//batch_size, desc = \"Generating representations\")):\n",
    "                tmp = batch_process_images(batch['image'], pipe, extract_positions, noise)\n",
    "                tmp_dataset = Dataset.from_dict({'label': batch['label']} | tmp, features=features)\n",
    "                path = Path(tmp_ouput_path) / f'{split}-{i:05d}'\n",
    "                tmp_dataset.save_to_disk(path)\n",
    "                tmp_paths[split].append(path)\n",
    "            print('Concatenating generated datasets')\n",
    "        \n",
    "        new_dataset = DatasetDict({\n",
    "            split: concatenate_datasets([load_from_disk(x) for x in tmp_paths[split]])\n",
    "            for split in tmp_dataset_dict.keys()\n",
    "        })\n",
    "\n",
    "        # save new dataset\n",
    "        print(f'saving new dataset to `{output_path}`')\n",
    "        new_dataset.save_to_disk(output_path)\n",
    "\n",
    "    return new_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sd_dataset = create_sd_dataset(\n",
    "#     output_path='../repr_dataset_test123',\n",
    "#     model_name='runwayml/stable-diffusion-v1-5',\n",
    "#     dataset_name='zh-plus/tiny-imagenet',\n",
    "#     extract_positions=['mid_block'],\n",
    "#     batch_size=4,\n",
    "#     noise=0.,\n",
    "#     # dataset_column_rename={'img':'image', 'fine_label': 'label'},  # for cifar100\n",
    "# )\n",
    "# sd_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['runwayml/stable-diffusion-v1-5','stabilityai/sd-turbo']#,'stabilityai/sdxl-turbo']\n",
    "noise_levels = [0.0]#, 0.01, 0.1,0.2,0.5,0.8]\n",
    "dataset_names_and_column_renames = [\n",
    "    ('mnist', {}),\n",
    "    ('cifar10', {'img':'image'}),\n",
    "    ('cifar100', {'img':'image', 'fine_label': 'label'}),\n",
    "    ('zh-plus/tiny-imagenet', {}),\n",
    "]\n",
    "count = np.prod([len(x) for x in [model_names, noise_levels, dataset_names_and_column_renames]])\n",
    "for model_name, noise_level, (dataset_name, column_rename) in tqdm(itertools.product(model_names, noise_levels, dataset_names_and_column_renames), total=count, desc='Generating datasets'):\n",
    "    print('#'*80)\n",
    "    print(f'Creating dataset for model `{model_name}`, dataset `{dataset_name}`, noise `{noise_level}`')\n",
    "    print('#'*80)\n",
    "    sd_dataset = create_sd_dataset(\n",
    "        output_path='../datasets-tmp/'+f'{model_name}-{dataset_name}-{noise_level}'.replace('/', '-'),\n",
    "        model_name=model_name,\n",
    "        dataset_name=dataset_name,\n",
    "        extract_positions=['mid_block'],\n",
    "        batch_size=4,\n",
    "        max_samples_per_split={'train': 60, 'test': 10, 'valid': 10},\n",
    "        noise=noise_level,\n",
    "        dataset_column_rename=column_rename,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
