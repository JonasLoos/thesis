{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sdhelper import SD\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "\n",
    "torch.set_float32_matmul_precision('high')  # for better performance (got a warning without this during torch compile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"0jl/NYUv2\", trust_remote_code=True, split=\"train\")\n",
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SD()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_blocks_separated = [[\n",
    "        'conv_in',\n",
    "    ],[\n",
    "        'down_blocks[0].resnets[0]',\n",
    "        'down_blocks[0].attentions[0]',\n",
    "        'down_blocks[0].resnets[1]',\n",
    "        'down_blocks[0].attentions[1]',\n",
    "        'down_blocks[0].downsamplers[0]',\n",
    "    ],[\n",
    "        'down_blocks[1].resnets[0]',\n",
    "        'down_blocks[1].attentions[0]',\n",
    "        'down_blocks[1].resnets[1]',\n",
    "        'down_blocks[1].attentions[1]',\n",
    "        'down_blocks[1].downsamplers[0]',\n",
    "    ],[\n",
    "        'down_blocks[2].resnets[0]',\n",
    "        'down_blocks[2].attentions[0]',\n",
    "        'down_blocks[2].resnets[1]',\n",
    "        'down_blocks[2].attentions[1]',\n",
    "        'down_blocks[2].downsamplers[0]',\n",
    "    ],[\n",
    "        'down_blocks[3].resnets[0]',\n",
    "        'down_blocks[3].resnets[1]',\n",
    "    ],[\n",
    "        'mid_block.resnets[0]',\n",
    "        'mid_block.attentions[0]',\n",
    "        'mid_block.resnets[1]',\n",
    "    ],[\n",
    "        'up_blocks[0].resnets[0]',\n",
    "        'up_blocks[0].resnets[1]',\n",
    "        'up_blocks[0].upsamplers[0]',\n",
    "    ],[\n",
    "        'up_blocks[1].resnets[0]',\n",
    "        'up_blocks[1].attentions[0]',\n",
    "        'up_blocks[1].resnets[1]',\n",
    "        'up_blocks[1].attentions[1]',\n",
    "        'up_blocks[1].resnets[2]',\n",
    "        'up_blocks[1].attentions[2]',\n",
    "        'up_blocks[1].upsamplers[0]',\n",
    "    ],[\n",
    "        'up_blocks[2].resnets[0]',\n",
    "        'up_blocks[2].attentions[0]',\n",
    "        'up_blocks[2].resnets[1]',\n",
    "        'up_blocks[2].attentions[1]',\n",
    "        'up_blocks[2].resnets[2]',\n",
    "        'up_blocks[2].attentions[2]',\n",
    "        'up_blocks[2].upsamplers[0]',\n",
    "    ],[\n",
    "        'up_blocks[3].resnets[0]',\n",
    "        'up_blocks[3].attentions[0]',\n",
    "        'up_blocks[3].resnets[1]',\n",
    "        'up_blocks[3].attentions[1]',\n",
    "        'up_blocks[3].resnets[2]',\n",
    "        'up_blocks[3].attentions[2]',\n",
    "    ],[\n",
    "        'conv_out',\n",
    "    ]\n",
    "]\n",
    "all_blocks = [b for blocks_list in all_blocks_separated for b in blocks_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "n_steps = 10000\n",
    "batch_size = 64\n",
    "noise_step = 50\n",
    "seed = 42\n",
    "model_lr = 1e-3\n",
    "limit_data_to = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_limited = data.select(range(limit_data_to)) if len(data) > limit_data_to else data\n",
    "depths_full = torch.tensor([x['depth'] for x in tqdm(data_limited, desc='loading depths')], dtype=torch.float32, device='cuda')\n",
    "n, w_orig, h_orig = depths_full.shape\n",
    "n_train = int(n * 0.8)\n",
    "n_val = n - n_train\n",
    "depths_train = depths_full[:n_train]\n",
    "depths_val = depths_full[n_train:]\n",
    "print(f'{n = }, {w_orig = }, {h_orig = }')\n",
    "images = [x['image'] for x in tqdm(data_limited, desc='loading images')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "up1_anomalies = np.load('../data/data_labeler/high_norm_anomalies_nyuv2_step50_seed42.npy')\n",
    "up1_scale_factor = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_losses = {}\n",
    "models = {}\n",
    "for blocks in all_blocks_separated:\n",
    "    # empty cache and move pipeline to cuda for representation extraction\n",
    "    torch.cuda.empty_cache()\n",
    "    # sd.pipeline = sd.pipeline.to('cuda')\n",
    "\n",
    "    # extract representations only for the layers of the current block to save memory\n",
    "    repr_raw = sd.img2repr(images, extract_positions=blocks, step=noise_step, seed=seed)\n",
    "\n",
    "    # move pipeline back to cpu to save vram\n",
    "    # sd.pipeline = sd.pipeline.to('cpu')\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "    for block in blocks:\n",
    "        print('-'*100)\n",
    "        print(f'{block = }')\n",
    "\n",
    "        # convert representation to torch tensor\n",
    "        repr_torch = torch.stack([x[block].squeeze(0) for x in repr_raw]).permute(0, 2, 3, 1).to(dtype=torch.float32, device='cuda')\n",
    "        print(f'{repr_torch.shape = }')\n",
    "        repr_train = repr_torch[:n_train]\n",
    "        repr_val = repr_torch[n_train:]\n",
    "        _, h, w, features = repr_torch.shape\n",
    "\n",
    "        # setup model\n",
    "        model = torch.nn.Linear(features, 1).to('cuda')\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=model_lr)\n",
    "        losses = []\n",
    "\n",
    "        # train\n",
    "        for i in (tr:=trange(n_steps, desc=f'training')):\n",
    "            idx = torch.randint(0, n_train, (batch_size,))\n",
    "            repr = repr_train[idx]\n",
    "            depths = depths_train[idx]\n",
    "\n",
    "            pred = model(repr).squeeze(-1).unsqueeze(1)\n",
    "            pred_full = F.interpolate(pred, (w_orig, h_orig), mode='bilinear').squeeze(1)\n",
    "            loss = F.huber_loss(pred_full, depths)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            tr.set_postfix(loss=loss.item())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        # plot loss\n",
    "        plt.plot(losses)\n",
    "        plt.yscale('log')\n",
    "        plt.title(f'{block} train loss')\n",
    "        plt.xlabel('step')\n",
    "        plt.ylabel('huber loss')\n",
    "        plt.show()\n",
    "\n",
    "        # test\n",
    "        with torch.no_grad():\n",
    "            pred_all = torch.cat([model(repr_torch[i:i+batch_size]).squeeze(-1) for i in range(0, n, batch_size)]).unsqueeze(1)\n",
    "            pred_full = F.interpolate(pred_all, (w_orig, h_orig), mode='bilinear').squeeze(1)\n",
    "\n",
    "            rmse_train = F.mse_loss(pred_full[:n_train], depths_train).item()**.5\n",
    "            print(f'rmse train: {rmse_train}')\n",
    "            huber_loss_train = F.huber_loss(pred_full[:n_train], depths_train).item()\n",
    "            print(f'huber train: {huber_loss_train}')\n",
    "\n",
    "            rmse_test = F.mse_loss(pred_full[n_train:], depths_val).item()**.5\n",
    "            print(f'rmse val: {rmse_test}')\n",
    "            huber_loss_test = F.huber_loss(pred_full[n_train:], depths_val).item()\n",
    "            print(f'huber val: {huber_loss_test}')\n",
    "\n",
    "        # compute up1 anomaly metrics\n",
    "        rmse_train_anomaly = []\n",
    "        huber_train_anomaly = []\n",
    "        rmse_test_anomaly = []\n",
    "        huber_test_anomaly = []\n",
    "        for j, tmp in enumerate(tqdm(up1_anomalies)):\n",
    "            img_idx, w_idx, h_idx = tmp.tolist()\n",
    "            if img_idx >= len(images): continue\n",
    "            pred_anomaly = pred_full[img_idx, h_idx:h_idx+up1_scale_factor, w_idx:w_idx+up1_scale_factor]\n",
    "            depth_anomaly = depths_full[img_idx, h_idx:h_idx+up1_scale_factor, w_idx:w_idx+up1_scale_factor]\n",
    "            if img_idx < n_train:\n",
    "                rmse_train_anomaly += F.mse_loss(pred_anomaly, depth_anomaly).item()**.5,\n",
    "                huber_train_anomaly += F.huber_loss(pred_anomaly, depth_anomaly).item(),\n",
    "            else:\n",
    "                rmse_test_anomaly += F.mse_loss(pred_anomaly, depth_anomaly).item()**.5,\n",
    "                huber_test_anomaly += F.huber_loss(pred_anomaly, depth_anomaly).item(),\n",
    "        \n",
    "        # compute corner metrics\n",
    "        rmse_train_corner = sum(F.mse_loss(pred_full[:n_train, -i, -j], depths_train[:, -i, -j]).item()**.5 for i in range(2) for j in range(2)) / 4\n",
    "        huber_train_corner = sum(F.huber_loss(pred_full[:n_train, -i, -j], depths_train[:, -i, -j]).item() for i in range(2) for j in range(2)) / 4\n",
    "        rmse_test_corner = sum(F.mse_loss(pred_full[n_train:, -i, -j], depths_val[:, -i, -j]).item()**.5 for i in range(2) for j in range(2)) / 4\n",
    "        huber_test_corner = sum(F.huber_loss(pred_full[n_train:, -i, -j], depths_val[:, -i, -j]).item() for i in range(2) for j in range(2)) / 4\n",
    "\n",
    "        # compute border metrics\n",
    "        rmse_train_border = F.mse_loss(\n",
    "            torch.cat([pred_full[:n_train, :, -1], pred_full[:n_train, -1, :], pred_full[:n_train, :, 0], pred_full[:n_train, 0, :]], dim=1),\n",
    "            torch.cat([depths_train[:, :, -1], depths_train[:, -1, :], depths_train[:, :, 0], depths_train[:, 0, :]], dim=1),\n",
    "        ).item()**.5\n",
    "        huber_train_border = F.huber_loss(\n",
    "            torch.cat([pred_full[:n_train, :, -1], pred_full[:n_train, -1, :], pred_full[:n_train, :, 0], pred_full[:n_train, 0, :]], dim=1),\n",
    "            torch.cat([depths_train[:, :, -1], depths_train[:, -1, :], depths_train[:, :, 0], depths_train[:, 0, :]], dim=1),\n",
    "        ).item()\n",
    "        rmse_test_border = F.mse_loss(\n",
    "            torch.cat([pred_full[n_train:, :, -1], pred_full[n_train:, -1, :], pred_full[n_train:, :, 0], pred_full[n_train:, 0, :]], dim=1),\n",
    "            torch.cat([depths_val[:, :, -1], depths_val[:, -1, :], depths_val[:, :, 0], depths_val[:, 0, :]], dim=1),\n",
    "        ).item()**.5\n",
    "        huber_test_border = F.huber_loss(\n",
    "            torch.cat([pred_full[n_train:, :, -1], pred_full[n_train:, -1, :], pred_full[n_train:, :, 0], pred_full[n_train:, 0, :]], dim=1),\n",
    "            torch.cat([depths_val[:, :, -1], depths_val[:, -1, :], depths_val[:, :, 0], depths_val[:, 0, :]], dim=1),\n",
    "        ).item()\n",
    "\n",
    "        # store loss / performance\n",
    "        final_losses[block] = [\n",
    "            rmse_train,\n",
    "            huber_loss_train,\n",
    "            rmse_test,\n",
    "            huber_loss_test,\n",
    "            np.mean(rmse_train_anomaly).item(),\n",
    "            np.mean(huber_train_anomaly).item(),\n",
    "            np.mean(rmse_test_anomaly).item(),\n",
    "            np.mean(huber_test_anomaly).item(),\n",
    "            rmse_train_corner,\n",
    "            huber_train_corner,\n",
    "            rmse_test_corner,\n",
    "            huber_test_corner,\n",
    "            rmse_train_border,\n",
    "            huber_train_border,\n",
    "            rmse_test_border,\n",
    "            huber_test_border,\n",
    "        ]\n",
    "        models[block] = model.to('cpu')\n",
    "\n",
    "        del repr_torch, repr_train, repr_val\n",
    "    del repr_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(models, open('depth_estimation_nyu_like_beyond_surface_statistics_models.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_losses_np = np.array(list(final_losses.values()))\n",
    "np.save('depth_estimation_nyu_like_beyond_surface_statistics_final_losses.npy', final_losses_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_rmse, y_train_huber, y_test_rmse, y_test_huber, y_train_rmse_anomaly, y_train_huber_anomaly, y_test_rmse_anomaly, y_test_huber_anomaly, y_train_rmse_corner, y_train_huber_corner, y_test_rmse_corner, y_test_huber_corner, y_train_rmse_border, y_train_huber_border, y_test_rmse_border, y_test_huber_border = final_losses_np.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "x = np.arange(len(final_losses))\n",
    "lines = []\n",
    "lines += ax1.plot(x, y_train_rmse, label='train', color='tab:green', linestyle='-')\n",
    "lines += ax1.plot(x, y_test_rmse, label='test', color='tab:green', linestyle='--')\n",
    "\n",
    "lines += ax1.plot(x, y_train_rmse_anomaly, label='train anomaly', color='tab:red', linestyle='-')\n",
    "lines += ax1.plot(x, y_test_rmse_anomaly, label='test anomaly', color='tab:red', linestyle='--')\n",
    "\n",
    "# plot x ticks\n",
    "ticks = ['attn' if 'attentions' in block else 'res' if 'resnets' in block else 'down' if 'downsamplers' in block else 'up' if 'upsamplers' in block else 'conv' if 'conv' in block else '?' for block in all_blocks]\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(ticks, rotation=90)\n",
    "\n",
    "# compute main blocks names and positions\n",
    "main_blocks = []\n",
    "main_block_positions = []\n",
    "tmp = 0\n",
    "for block_list in all_blocks_separated:\n",
    "    if 'mid' in block_list[0]:\n",
    "        name = 'mid'\n",
    "    elif 'conv' in block_list[0]:\n",
    "        name = block_list[0][5:]\n",
    "    else:\n",
    "        a, b, *_ = block_list[0].split('[')\n",
    "        name = a.replace('_blocks','') + b.split(']')[0]\n",
    "    main_blocks.append(name)\n",
    "    main_block_positions.append(tmp)\n",
    "    tmp += len(block_list)\n",
    "\n",
    "# lines between main blocks\n",
    "for p in main_block_positions[1:]:\n",
    "    ax1.axvline(x=p-0.5, color='black', linestyle='--', c='lightgray')\n",
    "ax_x3 = ax1.secondary_xaxis(location=0)\n",
    "ax_x3.set_xticks([p-0.5 for p in main_block_positions[1:]], labels=[])\n",
    "ax_x3.tick_params(axis='x', length=34, width=1.5, color='lightgray')\n",
    "\n",
    "ax_x2 = ax1.secondary_xaxis(location=0)\n",
    "ax_x2.set_xticks([p+len(bl)/2-0.5 for p, bl in zip(main_block_positions, all_blocks_separated)], labels=[f'\\n\\n\\n{b}' for b in main_blocks], ha='center')\n",
    "ax_x2.tick_params(length=0)\n",
    "\n",
    "\n",
    "ax1.set_ylabel('rmse')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_yticks([2**i for i in range(-1,2)])\n",
    "ax1.set_yticklabels([f'{y:.1f}' for y in ax1.get_yticks()])\n",
    "ax1.yaxis.set_minor_formatter(plt.NullFormatter())\n",
    "\n",
    "# Combine legends from both axes\n",
    "labs = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labs)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green']\n",
    "lines = []\n",
    "for i, block_type in enumerate(['attention', 'resnet', 'samplers']):\n",
    "    x = [i for i, b in enumerate(all_blocks) if block_type in b]\n",
    "    y_tmp = [y_train_huber[i] for i in x]\n",
    "    lines.extend(ax1.plot(x, y_tmp, label=f'{block_type} train', color=colors[i], linestyle='-'))\n",
    "    y_tmp = [y_test_huber[i] for i in x]\n",
    "    lines.extend(ax1.plot(x, y_tmp, label=f'{block_type} test', color=colors[i], linestyle='--'))\n",
    "\n",
    "ax1.set_xticks(np.arange(len(all_blocks)))\n",
    "ax1.set_xticklabels(all_blocks, rotation=90)\n",
    "ax1.set_ylabel('huber loss')\n",
    "\n",
    "# Combine legends from both axes\n",
    "ax1.legend(lines, [l.get_label() for l in lines])\n",
    "\n",
    "ax1.set_title('huber loss')\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_data = np.array([y_train_huber, y_test_huber, y_test_rmse])\n",
    "# np.save('depth_estimation_nyu_like_beyond_surface_statistics_all_loss_data.npy', loss_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
