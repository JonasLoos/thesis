---
format:
  html:
    title: "An analysis of representation similarities in latent diffusion models and implications for representation extraction"
    subtitle: Master's Thesis
    author:
      - name: Jonas Loos
        affiliations: TU Berlin
---

```{python}
from assets.helper_functions import cache_mpl_plot, reset_cached_figures, clear_unused_cached_figures, display_figure, sd15_all_blocks
import os
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np

os.environ['TQDM_DISABLE'] = '1'  # should disable all tqdm output, but does not reliably work
reset_cached_figures()  # reinit figure cache, so that unused figures can be deleted at the end

# Default noise / time step for representation extraction
# * SC - "A tale of two features" uses 100
# * SC - "Emergent Correspondence from Image Diffusion" uses 261
# * SC - "Unsupervised Semantic Correspondence Using Stable Diffusion" uses 8/50 (=160?)
# * SC - "Probing the 3D Awareness of Visual Foundation Models" uses 1
# * SC - "SD4Match: Learning to Prompt Stable Diffusion Model for Semantic Matching" uses 261 for training and 50 for inference
# * DE - "Unleashing Text-to-Image Diffusion Models for Visual Perception" uses 0
# * LPC - "Denoising Diffusion Autoencoders are Unified Self-supervised Learners" find 11 or 45 to be good
# * LPC - "Diffusion Models Beat GANs on Image Classification" uses 90
# * LPC - "Your Diffusion Model is Secretly a Zero-Shot Classifier" uses 500
# * SS - "Three Things We Need to Know About Transferring Stable Diffusion to Visual Dense Prediction Tasks" uses 10
# * SS - "diffcut: Catalyzing Zero-Shot Semantic Segmentation with Diffusion Features and Recursive Normalized Cut" uses 50
DEFAULT_NOISE = 50
```

::: {.content-visible when-format="pdf"}
# \phantom{Front Matter} {.unnumbered .unlisted}
:::

## Abstract {.unnumbered .unlisted}

Diffusion models have become a cornerstone of generative modeling, achieving state-of-the-art performance in producing high-quality outputs across diverse modalities, especially in image generation. Beyond their generative capabilities, these models encode meaningful semantic representations that can facilitate various downstream tasks, such as classification, semantic correspondence, and depth estimation. This thesis investigates the properties of diffusion model representations and their similarities, revealing biases that include sensitivity to absolute image positions, prioritization of texture and color over semantic content, and anomalies with high representation norms. By evaluating the representations on downstream tasks, we quantify the impact of these biases and their implications for representation quality. Our findings provide new insights and guidelines for leveraging latent diffusion models as representation learners in computer vision.


::: {.content-visible when-format="pdf"}

\vfill
\noindent\begin{center}\textit{This is the public version of the thesis and not identical to the submitted document.}\end{center}

<!-- TOC -->
\clearpage {
  \hypersetup{linkcolor=black}
  \setcounter{tocdepth}{3}
  \tableofcontents
}

:::

:::: {.content-visible when-format="html"}
<details>
<summary>Definitions and Abbreviations</summary>
::: {#details-definitions-abbreviations}

\clearpage
## Important Definitions {.unnumbered .unlisted}

* **Stable Diffusion** (SD): a series of open-source latent diffusion models for image generation [@rombach2022highresolution]
* **U-Net**: a U-shaped neural network using an encoder-decoder architecture with skip connections [@ronneberger2015unet]
* **Block**: one submodule of the SD U-Net, e.g. `mid`, or `up[1]`; can also refer to the output of a block
* **Upper/Lower Blocks**: the blocks that are higher in the U-Net, i.e. closer to input/output (`conv-in`, `down[0]`, `up[3]`, ...), or lower, i.e. near the center (`down[3]`, `mid`, `up[0]`, ...); the transition is gradual, i.e. there is no sharp separation
* **Layer**: a part of a block, e.g. the `mid` block of SD-1.5 has the one attention layer and two ResNet layers; can also refer to the output of a layer
* **Representation**: the output of a block/layer, with shape (channels, height, width), e.g. (1280, 8, 8) for SD-1.5 `mid` block at the default image size; also called "feature map" in the literature [@tang2023emergent;@zhang2023tale]
* **(Representation) Token**: a vector containing the channels values of a representation at a specific spatial position
* **Colorwheel**: an image containing a cyclic color gradient with the goal of better visualizing which regions of an image are mapped where

## Common Abbreviations {.unnumbered .unlisted}

* **SD**: Stable Diffusion
* **VAE**: Variational Autoencoder
* **PCK**: Percentage of Correct Keypoints
* **PCK@0.1$_{\text{bbox}}$**: PCK at a threshold of 10% of the bounding box size
* **MSE**: Mean Squared Error
* **RMSE**: Root Mean Square Error
* **RGB**: Red Green Blue (image color channels)
* **AI**: Artificial Intelligence
<!-- 
* PCA: Principal Component Analysis
 -->

:::

</details>

::::

::: {.content-visible when-format="pdf"}

\clearpage
## Important Definitions {.unnumbered .unlisted}

* **Stable Diffusion** (SD): a series of open-source latent diffusion models for image generation [@rombach2022highresolution]
* **U-Net**: a U-shaped neural network using an encoder-decoder architecture with skip connections [@ronneberger2015unet]
* **Block**: one submodule of the SD U-Net, e.g. `mid`, or `up[1]`; can also refer to the output of a block
* **Upper/Lower Blocks**: the blocks that are higher in the U-Net, i.e. closer to input/output (`conv-in`, `down[0]`, `up[3]`, ...), or lower, i.e. near the center (`down[3]`, `mid`, `up[0]`, ...); the transition is gradual, i.e. there is no sharp separation
* **Layer**: a part of a block, e.g. the `mid` block of SD-1.5 has the one attention layer and two ResNet layers; can also refer to the output of a layer
* **Representation**: the output of a block/layer, with shape (channels, height, width), e.g. (1280, 8, 8) for SD-1.5 `mid` block at the default image size; also called "feature map" in the literature [@tang2023emergent;@zhang2023tale]
* **(Representation) Token**: a vector containing the channels values of a representation at a specific spatial position
* **Colorwheel**: an image containing a cyclic color gradient with the goal of better visualizing which regions of an image are mapped where

## Common Abbreviations {.unnumbered .unlisted}

* **SD**: Stable Diffusion
* **VAE**: Variational Autoencoder
* **PCK**: Percentage of Correct Keypoints
* **PCK@0.1$_{\text{bbox}}$**: PCK at a threshold of 10% of the bounding box size
* **MSE**: Mean Squared Error
* **RMSE**: Root Mean Square Error
* **RGB**: Red Green Blue (image color channels)
* **AI**: Artificial Intelligence

:::



# Introduction {#sec-introduction}

Diffusion models [@pmlr-v37-sohl-dickstein15;@ho2020denoising] have rapidly advanced the field of generative modeling, achieving state-of-the-art sample quality and training stability across various modalities, such as images [@dhariwal2021diffusion], video [@videoworldsimulators2024], and audio [@kong2021diffwave], but can also be used in other contexts like text or code generation [@singh2023codefusion]. Especially in the field of image generation, diffusion models are currently state-of-the-art [@dhariwal2021diffusion;@rombach2022highresolution;@nichol2022glidephotorealisticimagegeneration;@podell2023sdxl;@betker2023improving;@blackforestlabs2024fluxAnnouncement], which raises the question, if they also learn semantic representations of various concepts present in the training images. If yes, these representations could be used for various tasks in computer vision, such as semantic correspondence, semantic segmentation, depth estimation, and more. In this thesis, we set out to investigate the properties of the learned representations of diffusion models, the similarities between them, and their usefulness for downstream tasks.

Many tasks in computer vision would greatly benefit from meaningful representations that effectively capture the semantic information of input data, however, the task of learning such representations remains a key challenge in the field. There are different approaches to learning these meaningful representations of images, such as self-supervised learning with self-distillation in the case of the DINOv2 model [@oquab2024dinov], or contrastive learning in the case of CLIP models [@radford2021learning]. Other options include using internal representations of vision models trained on other tasks, such as image classification models or diffusion models for image generation.

For these diffusion models, proprietary implementations lead the leaderboard in terms of image generation quality, but there are also multiple open-source diffusion models in the top 10 [@artificialanalysis2024quality]. The most prominent series of open-source diffusion models for image generation is Stable Diffusion (SD), introduced by @rombach2022highresolution. SD models sample images by iteratively transforming noise towards the target image distribution in the latent space of a pre-trained autoencoder. This transformation is achieved by removing noise using a U-Net [@ronneberger2015unet] architecture, which is a U-shaped neural network.^[While the U-Net architecture is used for most SD models, it is not the only option, and the most recent models use a transformer instead, see @sec-future-work.]

SD already received broad attention as foundation models providing representations that can be used for various tasks. Several works use SD U-Net representations for tasks such as semantic correspondence [@zhang2023tale;@zhang2024telling;@banani2024probing;@tang2023emergent;@luo2023dhf;@hedlin2023unsupervised;@li2023sd4match;@stracke2024clean;@fundel2024distillationdiffusionfeaturessemantic;@mariotti2024improving;@kim2025matchme], classification [@xiang2023denoising;@clark2024text;@li2023diffusion;@mukhopadhyay2023diffusion;@hudson2023soda;@stracke2024clean], depth estimation [@Chen2023BeyondSS;@Patni2024ECoDepth;@zhao2023unleashing;@zhang2025three;@stracke2024clean], semantic segmentation [@baranchuk2022labelefficient;@ji2024diffusion;@couairon2024zeroshot;@zhao2023unleashing;@tian2024diffuse;@zhang2025three;@couairon2024diffcutcatalyzingzeroshotsemantic;@Yang2023Diffusion], robot control [@gupta2024pretrained;@shridhar2024generativeimageactionmodels;@tsagkas2024clickgraspzeroshotprecise], and more [@ye2024stablenormalreducingdiffusionvariance;@de2024genziqa]. However, comprehensive studies exploring the properties, visual biases, and anomalies of diffusion models remain scarce [@park2023understanding;@jaini2024intriguing;@tang2023emergent], leaving significant gaps in the understanding of SD representations.

These representations are typically extracted from different parts of the U-Net, whereby the quality and usefulness of the representations heavily depends on the details of the representation extraction process [@fuest2024diffusionmodelsrepresentationlearning;@tang2023emergent]. The shape of the representations is dependent on the position in the U-Net, and consists of spatial dimensions that are relative to the size of the input image, downscaled by a power of two, and a channel dimension. We can compare the feature vectors at specific spatial positions, i.e. the different representation tokens, and analyze the similarities between them. The resulting similarity maps ideally show the semantic relationships between the different parts of the image and can be used for downstream tasks such as semantic correspondence.

In this thesis, we investigate the properties of the learned representations of SD, their similarities, and their usefulness for downstream tasks^[Our code is available on GitHub: [github.com/jonasloos/thesis](https://github.com/jonasloos/thesis).]. A significant part is dedicated to investigating different biases, such as position bias, texture and color bias, and anomalous tokens with high norm in the learned representations. For this, we introduce novel approaches to demonstrate and quantify these biases. Our experiments often build upon the similarities between representations, but also include investigations of the representation norms and downstream tasks utilizing linear probes. The results tend to follow the structure of first observing and qualitatively studying a found property, followed by a quantitative analysis and an investigation of the impact on downstream tasks.

The main contributions of this thesis, sorted by relevance, are:

* An extensive investigation of different biases in the learned representations, including position bias, texture and color bias, and anomalies in the learned representations. To the best of our knowledge, this is the first work to systematically study the position bias and describe the high-norm anomalies occurring in the SD representations.
* A detailed evaluation of the performance of the learned SD representations on the tasks of linear probe classification and semantic correspondence.
* The development of representation extraction and exploration tools^[The representation similarity explorer is available with precomputed representations at [sd-similarities.jloos.de](https://sd-similarities.jloos.de).] to facilitate our analyses.

The general structure of this thesis is as follows: After introducing relevant concepts and existing methods in @sec-related-work, we start by describing our approach to representation extraction, and the different tasks and datasets used in the experiments, in @sec-methods. We then detail our tools and results for representation extraction and exploration in @sec-representation-extraction-exploration. Next, we evaluate the performance of the learned SD representations on the tasks of linear probe classification and semantic correspondence, in @sec-downstream-tasks. Following this, we investigate the different biases qualitatively and quantitatively in @sec-biases. Finally, we discuss our results, the limitations, and future work in @sec-discussion and present final conclusions in @sec-conclusion.
The appendix contains supplementary information about experiment setup, models, and additional results.

<!-- 
potential additional topics:
* more representation learning
* XAI
* foundation models
-->


## Related work {#sec-related-work}

This section details key literature on diffusion models, focusing on their use for representation learning. First, we introduce the general concept of diffusion models and SD in particular. Then, we discuss self-supervised representation learning, how diffusion models can be employed for this, and how the learned SD representations can be used for downstream tasks.


### Diffusion Models {.unnumbered .unlisted}

Initially inspired by the physical process of diffusion, diffusion models iteratively transform a distribution of noise into a desired target distribution through a sequence of learned reverse steps [@pmlr-v37-sohl-dickstein15;@ho2020denoising]. In 2015, @pmlr-v37-sohl-dickstein15 introduced diffusion probabilistic models, but only in 2020 @ho2020denoising showed that the diffusion process can be used to train a generative model for generating realistic high quality images using *Denoising Diffusion Probabilistic Models* (DDPM). They propose a parameterized Markov chain that reverses a gradual noising process over 1000 steps. The architecture of the denoising model they used is a U-Net [@ronneberger2015unet], a U-shaped neural network, as visualized in @fig-unet. It consists of a series of down- and up-sampling blocks, which are connected by skip connections and a mid block at the lowest level. The outputs of the down blocks progressively increase in the channel dimensionality while the spatial dimensionality decreases, whereas this is reversed for the up blocks.
<!-- Maybe mention DDIM: Replacing the Markov chain during sampling with a non-markovian forward process iteratively removing noise can lead to up to 50x faster sampling [@song2022denoising]. -->

In 2021, @dhariwal2021diffusion showed that, for image generation, diffusion models can outperform the previous state-of-the-art, generative adversarial networks [@goodfellow2020generative], in terms of sample quality and training stability. @nichol2022glidephotorealisticimagegeneration showed that the diffusion process can be guided by text prompts, improving the usability of the models. Furthermore, @rombach2022highresolution introduced the first version of SD, which employs the U-Net in the latent space of a pretrained variational autoencoder to increase computational efficiency. Following works have improved the quality of the generated images [@podell2023sdxl], and computational efficiency [@sauer2023adversarial;@lin2024sdxllightning].

In general, there is a lot of progress in the field of image generation, with recent diffusion models reaching a level where general users cannot distinguish AI-generated art from human-created art anymore [@ha2024organicdiffuseddistinguishhuman]. New alternative frameworks such as flow matching [@lipman2023flow] promise to improve training and sampling speed, and new model architectures such as diffusion transformers [@peebles2023scalable;@esser2024scaling] allow for better image quality, prompt adherence, and scalability.
Furthermore, diffusion based video generation models have been released, such as SORA [@videoworldsimulators2024], which are able to generate high quality videos.


### Stable Diffusion {.unnumbered .unlisted}

SD is a series of latent diffusion models for image generation [@rombach2022highresolution]. Initially introduced in 2022, many follow-up checkpoints and models have been released by the original authors and others. The models SD-1.1 to SD-1.4 [@rombach2022highresolution] were released by CompVis^[CompVis is the 
Computer Vision & Learning Group at the Ludwig Maximilian University of Munich.], SD-1.5 [@rombach2022highresolution] by Runway^[Runway AI, Inc. is a US-based company developing AI systems for audiovisual media.], and SD-2.0 [@rombach2022highresolution], SD-2.1 [@rombach2022highresolution], SD-Turbo [@sauer2023adversarial], SDXL [@podell2023sdxl], and SDXL-Turbo [@sauer2023adversarial] by Stability AI^[Stability AI Ltd. is a company developing generative AI systems.]. The main differences between the models are the number of parameters, the used text-encoders, the training schedule and the training data. They share a similar architecture and in particular have a similar U-Net structure. For more information on the different U-Net based models, see @sec-appendix-sd-models.

Most SD models have been made publicly available as open-source, e.g. on Hugging Face or GitHub. Different Hugging Face `diffusers` [@vonplaten2022diffusers] pipelines allow the application of SD models for different tasks, such as text-to-image, image-to-image, inpainting, depth-to-image, and super-resolution [@hfsd].

Recently, SD-3, SD-3.5 [@esser2024scaling;@stabilityai2024sd35], and state-of-the-art in image generation more generally, shifted towards diffusion transformers instead of U-Net based models [@peebles2023scalable;@esser2024scaling;@blackforestlabs2024fluxRepo;@blackforestlabs2024fluxAnnouncement;@yu2024representationalignmentgenerationtraining]. For more information see @sec-future-work.


### Self-Supervised Representation Learning {.unnumbered .unlisted}

Self-supervised learning allows for the training of large-scale foundation models, including diffusion models, without explicit labels. Such foundation models can produce features rivaling those of fully supervised systems across a variety of downstream tasks, including both zero-shot and fine-tuning scenarios [@he2022maskedautoencoders;@oquab2024dinov;@radford2021learning]. One can train these models on different objectives, such as inpainting [@he2022maskedautoencoders], predicting transformations [@gidaris2018unsupervisedrepresentationlearningpredicting], or reordering patches [@misra2020selfsupervised;@noroozi2016unsupervised;@doersch2015unsupervised], as well as discriminative and contrastive learning strategies [@misra2020selfsupervised;@caron2021emerging;@oquab2024dinov;@chen2021exploring]. Generative models, including generative adversarial networks [@goodfellow2020generative] and diffusion-based approaches, reveal that internal representations learned through the synthesis of images can also yield high-quality representations [@chen2016infogan;@baranchuk2022labelefficient].

Among these paradigms, DINO [@caron2021emerging] and DINOv2 [@oquab2024dinov] are particularly notable for producing semantically coherent, dense descriptors via a self-distillation mechanism that enforces consistency under varying augmentations. Meanwhile, CLIP [@radford2021learning] exemplifies how contrastive learning across modalities (image-text) enables zero-shot generalization and robust transfer. Both approaches build upon vision transformers that are shown to contain artifacts in their attention maps, which slightly degrade performance in downstream tasks, but can be remedied by additional register tokens [@darcet2024vision].

By learning from unlabeled data at scale, self-supervised representation learning sets the stage for general-purpose representations that excel in a broad range of downstream applications.
<!-- Maybe add references to the recent sucesses in unsupervised language model training-->


### Diffusion Models for Representation Learning {.unnumbered .unlisted}

Diffusion models, and SD in particular, have been analyzed and used for representation learning in a variety of downstream tasks. While some works modify the model architecture or training process specifically for representation learning [@hudson2023soda;@chen2024deconstructingdenoisingdiffusionmodels], most works instead use the learned intermediate representations. Common SD versions used in the literature are SD-1.5 and SD-2.1 [@luo2023dhf;@zhao2023unleashing;@zhang2023tale;@zhang2024telling;@stracke2024clean;@linhardt2024analysis]. An interesting observation is that the performance on downstream tasks tends to increase with the extend of pretraining [@zhang2025three;@zhao2023unleashing].

**Localized Representations.** Diffusion model representations are dense visual descriptors, where the information at a given spatial position in the representations corresponds to the image content at the corresponding position in the image [@zhang2023tale;@tang2023emergent;@luo2023dhf;@hedlin2023unsupervised;@li2023sd4match;@stracke2024clean;@fundel2024distillationdiffusionfeaturessemantic;@kim2025matchme;@ji2024diffusion;@tian2024diffuse]. This is the base for many downstream tasks, such as semantic correspondence, semantic segmentation, depth estimation, and surface normal estimation.

**Choice of layer.** Recent surveys [@wang2024diffusionmodels3dvision;@fuest2024diffusionmodelsrepresentationlearning] provide comprehensive overviews of diffusion models for representation learning and their applications across various domains. Notably, most works find that the up-blocks of the U-Net contain the most useful representations for downstream tasks [@linhardt2024analysis;@zhang2023tale;@banani2024probing;@stracke2024clean]. However, this is not universal across all applications - for instance, @gupta2024pretrained find that down-blocks were more effective for robot control tasks and @couairon2024diffcutcatalyzingzeroshotsemantic use down block outputs for semantic segmentation. @tang2023emergent suggest that up-blocks lower in the U-Net yield more semantically-aware representations, while up-blocks higher in the U-Net focus more on more low-level details. Instead of the output of U-Net blocks, some works also use the attention maps of the attention layers inside the blocks instead [@zhao2023unleashing;@hedlin2023unsupervised].

**Choice of noise level.** Before extracting the representations from the U-Net, the noise is usually added to the latent image, based on the time step $t$ for the noise-scheduler, ranging between $0$ and $1000$ for SD, with higher values indicating more noise [@rombach2022highresolution]. Different works extract representations at varying noise levels, ranging from time step $0$ to $500$. While @zhao2023unleashing use no noise ($t=0$), others use high noise levels of $t=261$ [@tang2023emergent;@li2023sd4match] or even $500$ [@li2023diffusion]. Most works fall somewhere in between, with common time steps ranging from $50$ to $100$ [@banani2024probing;@zhang2025three;@xiang2023denoising;@mukhopadhyay2023diffusion;@zhang2023tale;@couairon2024diffcutcatalyzingzeroshotsemantic]. Some approaches even use different noise levels for training versus inference [@li2023sd4match], learn $t$ via reinforcement learning [@Yang2023Diffusion], or introduce a fine-tuning method to remove the need for noising [@stracke2024clean].

**Representation quality improvements.** Many approaches have been proposed to further enhance the quality of the representations and to improve their suitability for downstream tasks.
@luo2023dhf consolidate multi-scale and multi-timestep representations intro per-pixel feature descriptors.
@li2023sd4match optimize the prompt conditioning to improve representation quality.
@zhang2023tale fuse the SD representations with DINOv2 [@oquab2024dinov] and apply dimensionality reduction. Building on this, @zhang2024telling propose a test-time adaptive pose alignment strategy.
@stracke2024clean introduce an unsupervised fine-tuning method to reduce noise in the representations and to allow extraction of high quality representations at time step 0.
@fundel2024distillationdiffusionfeaturessemantic distill SD and DINOv2 representations into a student model, improving efficiency.

Notably, learning to generate images does not only lead to semantically coherent representations, but the connection also seems to go the other way around. @yu2024representationalignmentgenerationtraining show that regularizing diffusion model training by including external visual representations leads to improvements in efficiency and quality.

Overall, representations of diffusion models and especially SD have been used widely, but there is no clear consensus on the optimal block and noise level for representation extraction. Many methods have been proposed to improve their quality and suitability for downstream tasks.


### SD Representations for Downstream Tasks {.unnumbered .unlisted}

Various works have investigated different aspects of the learned representations, finding that semantic information is captured in the bottleneck layers of the U-Net [@kwon2023diffusion;@park2023unsupervised]. Further works have explored the suitability of extracted representations for classification [@xiang2023denoising;@clark2024text;@li2023diffusion;@mukhopadhyay2023diffusion;@stracke2024clean], semantic correspondence [@zhang2023tale;@zhang2024telling;@banani2024probing;@tang2023emergent;@luo2023dhf;@hedlin2023unsupervised;@li2023sd4match;@stracke2024clean;@fundel2024distillationdiffusionfeaturessemantic;@mariotti2024improving;@kim2025matchme], semantic segmentation [@baranchuk2022labelefficient;@ji2024diffusion;@couairon2024zeroshot;@zhao2023unleashing;@tian2024diffuse;@zhang2025three], depth estimation [@Chen2023BeyondSS;@Patni2024ECoDepth;@zhao2023unleashing;@zhang2025three;@stracke2024clean], and for modification of the image generation process [@park2023unsupervised;@jeong2024trainingfree;@haas2023discovering;@gambashidze2024aligningdiffusionmodelsnoiseconditioned;@hudson2023soda;@park2023understanding]. Other works investigated their alignment to human representations and human-like shape bias [@linhardt2024analysis;@jaini2024intriguing].
The representations have also been used for robotics tasks, such as robot control [@gupta2024pretrained;@shridhar2024generativeimageactionmodels] and grasping [@tsagkas2024clickgraspzeroshotprecise]. Additionally, they have been applied to 3D scene understanding [@Man2024Lexicon3DPV], surface normal estimation [@Ke2024RepurposingDI;@Lee2024ExploitingDI;@xu2024diffusionmodelstrainedlarge;@ye2024stablenormalreducingdiffusionvariance], and image quality assessment [@de2024genziqa].

In this thesis, we evaluate the SD representations on the tasks of linear probe classification, monocular depth estimation, and unsupervised zero-shot semantic correspondence.

**Linear probe classification** utilizes a trainable linear layer on top of the representations for classification and is primarily only used to evaluate the quality of the representations. Common evaluation datasets are \nameref{sec-datasets-cifar} [@Krizhevsky2009LearningML] and \nameref{sec-datasets-imagenet} [@ILSVRC15]. @mukhopadhyay2023diffusion achieve an accuracy of up to 65.18% on \nameref{sec-datasets-imagenet}-1k.

**Monocular depth estimation** is the task of estimating the depth information (distance relative to camera) for each pixel based on a single RGB image. This can also be done using a linear probe [@stracke2024clean;@Chen2023BeyondSS], more complex methods on top of the SD representations [@zhang2025three;@zhao2023unleashing;@Patni2024ECoDepth], or even by finetuning or modifying the SD model itself [@Lee2024ExploitingDI;@He2024LotusDV;@Ke2024RepurposingDI;@xu2024diffusionmodelstrainedlarge;@fu2024geowizard;@zhang2024betterdepthplugandplaydiffusionrefiner;@zhang2024atlantis]. @stracke2024clean report a root mean squared error (RMSE) of 0.469 on the \nameref{sec-datasets-nyu-v2} [@Silberman2012Indoor] dataset using a linear probe on the SD-2.1 representations, while @Patni2024ECoDepth report a RMSE of as low as 0.218 for their method using additional contextual embeddings and representation upsampling.

**Unsupervised zero-shot semantic correspondence** is the task of finding corresponding keypoints between two images without any examples or further training. Notable methods using SD representations include DIFT [@tang2023emergent], which achieves a performance of 61.2 percentage correct keypoints (PCK) on the \nameref{sec-datasets-spair} [@min2019spair71k] dataset. Pairing the SD representations with DINOv2 representations [@zhang2023tale] increases the performance to 64.0 PCK and adding test-time pose alignment [@zhang2024telling] leads to 69.6 PCK. Adding fine-tuning for representation denoising [@stracke2024clean] results in 69.99 PCK, the current state-of-the-art in unsupervised zero-shot semantic correspondence on the \nameref{sec-datasets-spair} dataset.

<!-- Could do: add dense correspondence (maybe not necessary, as it can be considered a special case of semantic correspondence) -->

```{python}
# Overview of SD representations for downstream tasks (not all use representation extraction):
# * overview/survey: [@wang2024diffusionmodels3dvision;@fuest2024diffusionmodelsrepresentationlearning]
# * modification of image generation [@park2023unsupervised;@jeong2024trainingfree;@haas2023discovering;@gambashidze2024aligningdiffusionmodelsnoiseconditioned;@hudson2023soda;@park2023understanding]
# * classification [@xiang2023denoising;@clark2024text;@li2023diffusion;@mukhopadhyay2023diffusion;@hudson2023soda;@stracke2024clean]
# * semantic correspondence [@zhang2023tale;@zhang2024telling;@banani2024probing;@tang2023emergent;@luo2023dhf;@hedlin2023unsupervised;@li2023sd4match;@stracke2024clean;@fundel2024distillationdiffusionfeaturessemantic;@mariotti2024improving;@kim2025matchme]
# * semantic segmentation with representations [@baranchuk2022labelefficient;@ji2024diffusion;@couairon2024zeroshot;@zhao2023unleashing;@tian2024diffuse;@zhang2025three;@couairon2024diffcutcatalyzingzeroshotsemantic] without representations [@yu2024highprecisiondichotomousimagesegmentation;@burgert2023peekabootextimagediffusion;@xu2023open;@tan2023diffssdiffusionmodelfewshot]
# * depth estimation with representations [@Chen2023BeyondSS;@Patni2024ECoDepth;@zhao2023unleashing;@zhang2025three;@stracke2024clean] with finetuning/other [@Lee2024ExploitingDI;@He2024LotusDV;@Ke2024RepurposingDI;@xu2024diffusionmodelstrainedlarge;@fu2024geowizard;@zhang2024betterdepthplugandplaydiffusionrefiner;@zhang2024atlantis]
# * surface normal estimation [@He2024LotusDV;@Ke2024RepurposingDI;@Lee2024ExploitingDI;@fu2024geowizard;@xu2024diffusionmodelstrainedlarge;@ye2024stablenormalreducingdiffusionvariance]
# * Image Quality Assessment [@de2024genziqa]
# * robot control [@gupta2024pretrained;@shridhar2024generativeimageactionmodels;@tsagkas2024clickgraspzeroshotprecise]
# * human alignment [@linhardt2024analysis]
# * 3d scene understanding [@Man2024Lexicon3DPV]
# * (counterfactual explanations [@schrodi2024latent])
# * further exploration of representation space [@park2023understanding]
# * more general: diffusion models for self-supervised learning [@chen2024deconstructingdenoisingdiffusionmodels;@hudson2023soda]

# SC citation counts as of 2024-12-13
# * [@tang2023emergent] - 165
# * [@zhang2023tale] - 116
# * [@luo2023dhf] - 82
# * [@hedlin2023unsupervised] - 58
# * [@banani2024probing] - 48
# * [@zhang2024telling] - 16
# * [@li2023sd4match] - 11
# * [@mariotti2024improving] - 10
# * [@stracke2024clean] - 1
# * [@fundel2024distillationdiffusionfeaturessemantic] - 1
# * [@kim2025matchme] - 0
```


# Methods {#sec-methods}

In this thesis, we primarily extract representations from SD-1.5, partly complemented by SD-2.1, SD-Turbo, SDXL, and SDXL-Turbo to show the generalizability of our findings. The extracted representations consist of representation tokens over the different spatial positions, over which different similarity measures can be evaluated. We use either these similarities or linear probes as a base for downstream tasks on various datasets. The downstream tasks we use to assess the performance of the SD models are linear probe classification, semantic correspondence, dense correspondence, and depth estimation. In the following, we first describe diffusion models, representation extraction, and representation similarities. Subsequently, we detail the downstream tasks and datasets used in this thesis.


## Diffusion Models {#sec-methods-diffusion-models}

Diffusion models define a generative process that transforms noise into complex samples, in our case images, by reversing a predefined forward diffusion process. Following the DDPM formulation by @ho2020denoising, let $\mathbf{x}_0 \in \mathbb{R}^d$ be an input image. The forward process transforms $\mathbf{x}_0$ into a noisy version $\mathbf{x}_t$ at step $t$ through linear interpolation with Gaussian noise:

$$\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\,\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\,\boldsymbol{\epsilon},$$

with $\boldsymbol{\epsilon}\sim \mathcal{N}(0,\mathbf{I})$ and $\bar{\alpha}_t=\prod_{s=1}^t \alpha_s$, given a noise schedule $\{\alpha_t\}_{t=1}^T$ where $0<\alpha_t<1$. As $t \to T$, $\mathbf{x}_t$ approaches pure noise.

The goal is to approximate the reverse process that removes noise step-by-step:

$$p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \approx q(\mathbf{x}_{t-1} \mid \mathbf{x}_t,\mathbf{x}_0),$$

where $q$ denotes the true (but intractable) reverse distribution that is approximated by the parameterized Gaussian distribution $p_\theta$, with $\theta$ being the model weights. To achieve this, a model $\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,t)$ is trained to predict the noise $\boldsymbol{\epsilon}$ present at each step, from which the mean of $p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)$ can be derived. A common training objective is to minimize the mean squared error between the predicted and true noise:

$$\mathcal{L}(\theta) = \mathbb{E}_{\mathbf{x}_0,\boldsymbol{\epsilon},t}\bigl[\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,t)\|^2\bigr],$$

where $\mathbf{x}_0$ is sampled from the training data distribution, $t$ is typically chosen uniformly at random from $\{1,\dots,T\}$, and $\boldsymbol{\epsilon}$ is drawn from a standard normal distribution $\mathcal{N}(0,\mathbf{I})$. Given these samples, we form the noisy image $\mathbf{x}_t$ using the forward process. After training, new samples are generated by starting from pure noise $\mathbf{x}_T \sim \mathcal{N}(0,\mathbf{I})$ and iteratively applying the learned reverse transitions until $\mathbf{x}_0$ is obtained.

In the case of SD, the denoising model $\boldsymbol{\epsilon}_\theta$ is typically a U-Net [@ronneberger2015unet;@rombach2022highresolution]. As SD is a series of latent diffusion models, the generative process is not applied in pixel space, but rather in the latent space of a pretrained variational autoencoder (VAE) [@rombach2022highresolution]. This means that $x_0$ is the output of the VAE encoder.


## Representation Extraction {#sec-methods-representation-extraction}

```{python}
#| label: fig-unet
#| fig-cap: SD U-Net architecture with variational autoencoder (VAE) in the representation extraction pipeline. The VAE encoder is not used during image generation, while the VAE decoder is not used during representation extraction for existing images. Representations are extracted at the end of each block.

# created using diagrams.net

# Potential improvements:
# * add "latents"-label, recursion-connection
# * add "VAE decoder" to diagram

display_figure('assets/figures/SD_representation_extraction.jpg', 7, 3)
```

For our analyses, we primarily focus on SD-1.5, partly complemented by SD-2.1, SD-Turbo, SDXL, or/and SDXL-Turbo. This decision is led by the popularity of SD-1.5 and that running all models for all experiments would be computationally expensive. If not stated otherwise, results are for SD-1.5.

The SD U-Net, visualized in @fig-unet, consists of a series of down- and up-blocks, which are connected by skip connections and a mid block at the lowest level. Each block consists of a combination of ResNet and attention layers, and a final down- or up-sampling operation where applicable. The output of these blocks are the representations we use. They have shape $(w, h, c) \in \mathbb{N}^3$, with $w$ and $h$ being width and height, and $c$ being the number of channels. One can also extract the representations from the different layers in the blocks, however, we primarily focus on the block outputs. The number of spatial dimensions and channels depends on the input image size and the block, as described in @tbl-repr-shapes. We refer to the representation at a given spatial position as a token, with shape $(c)$.

\begingroup
\scriptsize

| Block | SD-1.5 | SD-2.1 | SD-Turbo | SDXL | SDXL-Turbo |
|:-|:-:|:-:|:-:|:-:|:-:|
| `Input` | 512, 512, 3 | 768, 768, 3 | 512, 512, 3 | 1024, 1024, 3 | 512, 512, 3 |
| `conv-in` | 64, 64, 320 | 96, 96, 320 | 64, 64, 320 | 128, 128, 320 | 64, 64, 320 |
| `down[0]` | 32, 32, 320 | 48, 48, 320 | 32, 32, 320 | 64, 64, 320 | 32, 32, 320 |
| `down[1]` | 16, 16, 640 | 24, 24, 640 | 16, 16, 640 | 32, 32, 640 | 16, 16, 640 |
| `down[2]` | 8, 8, 1280 | 12, 12, 1280 | 8, 8, 1280 | 32, 32, 1280 | 16, 16, 1280 |
| `down[3]` | 8, 8, 1280 | 12, 12, 1280 | 8, 8, 1280 | - | - |
| `mid` | 8, 8, 1280 | 12, 12, 1280 | 8, 8, 1280 | 32, 32, 1280 | 16, 16, 1280 |
| `up[0]` | 16, 16, 1280 | 24, 24, 1280 | 16, 16, 1280 | 64, 64, 1280 | 32, 32, 1280 |
| `up[1]` | 32, 32, 1280 | 48, 48, 1280 | 32, 32, 1280 | 128, 128, 640 | 64, 64, 640 |
| `up[2]` | 64, 64, 640 | 96, 96, 640 | 64, 64, 640 | 128, 128, 320 | 64, 64, 320 |
| `up[3]` | 64, 64, 320 | 96, 96, 320 | 64, 64, 320 | - | - |
| `conv-out` | 64, 64, 4 | 96, 96, 4 | 64, 64, 4 | 4, 128, 128 | 4, 64, 64 |
: Representation shapes (width, height, channels) for different SD models and blocks. All values are for the default image sizes of the respective models, which is noted as `Input` in the table. SDXL and SDXL-Turbo only have 3 `down` and `up` blocks. {#tbl-repr-shapes}

\endgroup

The SD U-Net can be broadly categorized into upper blocks (`conv-in`, `down[0]`, `down[1]`, `up[2]`, `up[3]`), and lower blocks (`down[2]`, `down[3]`, `mid`, `up[0]`, `up[1]`). While upper and lower blocks tend to have different properties and biases, the distinction is usually not clear-cut, but rather a continuum. The output of `conv-out` is not used for any downstream tasks, as it does not directly contain semantic information, but rather the noise prediction. We therefore exclude it from most of our experiments.

As indicated in @fig-unet, to extract representations of a given image, the image is first fed through the encoder of the VAE. Then, Gaussian noise is added according to a given time step for the noise-scheduler, ranging between 0 and 1000 for SD, with higher values indicating more noise, as visualized in @fig-noise-levels. Finally, it is fed into the U-Net, and the representations are extracted from the desired blocks. For example, an image of shape $(512, 512, 3)$ fed into SD-1.5 has shape $(64, 64, 4)$ in the latent space of the VAE, and shape $(8, 8, 1280)$ at the `mid` block.

```{python}
#| label: fig-noise-levels
#| fig-cap: Example of different noise levels, i.e. time steps $t$. The image is encoded using the VAE, interpolated with Gaussian noise, and then decoded back to image space.

@cache_mpl_plot
def plot_noise_levels():
    from sdhelper import SD
    import torch
    torch.manual_seed(42)

    sd = SD(disable_progress_bar=True)
    img = Image.open("assets/images/cat_on_roof.jpg")
    n = 6
    fig, axs = plt.subplots(1, n, figsize=(n*0.8, 0.8+0.2))
    latent = sd.encode_latents([img.resize((512, 512))])
    noise = torch.randn_like(latent, device=sd.device)
    for i, x in enumerate(np.linspace(0, 1, n)):
        result = sd.vae_decode((1-x) * latent + x * noise)
        axs[i].imshow(result[0].cpu().permute(1, 2, 0).float().clamp(0, 1))
        axs[i].axis('off')
        axs[i].set_title(f'$t = {x*1000:.0f}$', fontsize=8)
    plt.tight_layout()
```

As this thesis focuses on the representations, when referring to a U-Net block, we are not talking about the block itself, i.e. its architecture or parameters, but rather about the representations extracted from it.

The amount of noise added to the latent image is a hotly debated topic in the literature. As described in @sec-related-work, time steps $t$ between $0$ and $500$ are used, with common values ranging between $50$ and $100$. For our experiments presented in @sec-downstream-tasks, we find time step $t=`{python} DEFAULT_NOISE`$ to be a good choice. However, slightly lower or moderately higher values tend to yield similar results. Unless stated otherwise, we use time step `{python} DEFAULT_NOISE` as default for all our experiments.


## Representation Similarities {#sec-representation-similarities}

As described in @sec-methods-representation-extraction, SD representations have shape $(w, h, c)$. Representation tokens, i.e. the channel vectors at given spatial positions, correspond semantically to the image content at the corresponding position in the image. This property is widely described in the literature, as detailed in @sec-related-work, and visualized in @sec-similarities. It is the basis for using similarities between tokens as a measure of semantic similarity between image regions.

Following common practice [@amir2022deepvitfeaturesdense;@muttenthaler2023human;@stracke2024clean], we use cosine similarity to measure the similarity between the representation tokens, which is defined as

$$ \text{cosine similarity}(R_{x,y}, R'_{x',y'}) = \frac{R_{x,y} \cdot R'_{x',y'}}{||R_{x,y}|| \cdot ||R'_{x',y'}||} $$

where $R$ and $R'$ are the two representations of the same block, but potentially of different images. $R_{x,y}$ is the token at spatial position $(x, y) \in \{1,...,w\}\times\{1,...,h\}$ of representation $R$.

Instead of cosine similarity, also other similarity measures or distance metrics can be used. Options include cosine similarity without normalization, i.e. the dot product, or cosine similarity with centering the representations first, i.e. subtracting the mean of each representation. When using distance metrics, their results need to be inverted to be used as a similarity measure, so that higher values indicate higher similarity. Common distance metrics are the Euclidean distance ($L_2$), the Manhattan distance ($L_1$), the Chebyshev distance ($L_\infty$), or more generally the distance metrics given by the $L_p$ norm, defined as

$$ L_p(R_{x,y}, R'_{x',y'}) = \left| \sum_{i=1}^c (R_{x,y,i} - R'_{x',y',i})^p \right|^\frac{1}{p} $$

where $p \in [1, \infty)$ is the order of the norm.

Instead of calculating the similarity between the tokens of one representation, one can also concatenate multiple representations along the channel dimension. If the spatial representations shapes differ, we use nearest neighbor upsampling on the smaller representations. For this, tokens are duplicated until the representation shapes match. While this approach can combine the semantic information of the different representations, the resulting representations are often very large, which can be a disadvantage in resource-constrained environments.

Measuring the similarity between the representation tokens is the basis for several downstream tasks. In this thesis, we use cosine similarity as a basis for semantic correspondence and dense correspondence tasks, as described in @sec-methods-downstream-tasks.


## Downstream Tasks {#sec-methods-downstream-tasks}

To evaluate the quality and properties of the extracted representations, we use several downstream tasks. These tasks either work on the similarity maps between representations, this includes semantic and dense correspondence, or using a linear probe on the representations, which is the case for classification and depth estimation.


### Linear Probe Classification {#sec-methods-lpc}

A conceptually simple way to evaluate the extracted representations from the U-Net of diffusion models, is the usage of a linear probe classifier [@xiang2023denoising;@hudson2023soda;@mukhopadhyay2023diffusion]. The key idea is to assess how well the representations encode semantic information, while keeping the classifier architecture minimal to focus on the quality of the representations themselves.

We use the datasets \nameref{sec-datasets-cifar}-10 [@Krizhevsky2009LearningML] with 10 classes, \nameref{sec-datasets-cifar}-100 [@Krizhevsky2009LearningML] with 100 classes, and Tiny-\nameref{sec-datasets-imagenet} [@ILSVRC15] with 200 classes. We upscale images to 512\times512 px and extract representations from a given block of the U-Net. As the representations have both spatial and channel dimensions, we average over the spatial dimensions to obtain a single feature vector per image [@xiang2023denoising;@linhardt2024analysis].

We then train a linear layer that maps from these averaged representations to class probabilities. For training, we use cross-entropy loss and the Adam optimizer [@kingma2017adam]. The linear layer consists of a weight matrix $W$ and bias vector $b$, with the number of output neurons matching the number of classes in the dataset. The class prediction $\hat{y}$ for a representation $R$ is defined as follows:

$$ \hat{y} = \underset{i}{\text{argmax}} \left( \left( \frac{1}{w \cdot h} \sum_{x,y} R_{x,y} \right) \cdot W + b \right)_i $$

The advantage of a linear probe lies in its limited complexity - it can only learn linear combinations of the input features. This restriction means that if the classifier achieves good performance, the original representations must already encode the relevant semantic information in a linearly separable way. Conversely, if a linear classifier struggles despite sufficient training data, this suggests that the probed class concepts are either not well captured or are encoded in a way that requires more complex transformations to be useful for classification.


### Semantic Correspondence {#sec-methods-sc}

Semantic correspondence is the task of finding the corresponding keypoint in a target image for a given keypoint in a source image. For example, the left ear of a cat in a source image should be matched to the left ear of a cat in a target image. This task requires a semantic understanding of image content and object positions. We primarily use \nameref{sec-datasets-spair} [@min2019spair71k] in our experiments, one of the most commonly used datasets for semantic correspondence.

To find corresponding points between two images, we first extract representations from both images at a specific block of the U-Net. For each source keypoint $(x_\text{src}, y_\text{src}) \in \{1,...,w\}\times\{1,...,h\}$ in the source image's representation $R_\text{src} \in \mathbb{R}^{w \times h \times c}$, we then compute the cosine similarity with all spatial positions in the target image's representation $R_\text{trg} \in \mathbb{R}^{w \times h \times c}$. The position with the highest similarity score is predicted as the corresponding point $(\hat{x}_\text{trg}, \hat{y}_\text{trg}) \in \{1,...,w\}\times\{1,...,h\}$.

$$ (\hat{x}_\text{trg}, \hat{y}_\text{trg}) = \underset{(x, y)}{\text{argmax}}\ \frac{R_{\text{src},x_\text{src}, y_\text{src}} \cdot R_{\text{trg}, x, y}}{ ||R_{\text{src},x_\text{src}, y_\text{src}}|| \cdot ||R_{\text{trg}, x, y}|| } $$

As we only use argmax on the cosine similarity of the representations to determine the correspondences, the method can be considered unsupervised and zero-shot.

The performance in the semantic correspondence task is usually reported in terms of the percentage of correct keypoints (PCK). PCK measures the percentage of predicted keypoints $(\hat{x}_\text{trg}, \hat{y}_\text{trg})$ with a distance to the target keypoints $(x_\text{trg}, y_\text{trg}) \in \mathbb{R}^{w \times h}$ below a certain fraction $\alpha$ of the bounding box size $(w_\text{bbox}, h_\text{bbox}) \in \{1,...,w\}\times\{1,...,h\}$ of the objects (PCK@$\alpha_\text{bbox}$), typically with $\alpha=0.1$ [@zhang2023tale;@zhang2024telling;@tang2023emergent;@luo2023dhf;@hedlin2023unsupervised;@li2023sd4match]. Instead of the bounding box size, some works also report the PCK for a fraction of the image size (PCK@$\alpha_\text{img}$) [@luo2023dhf;@li2023sd4match;@stracke2024clean].

$$ \text{PCK@}\alpha_\text{bbox} = \frac{1}{n} \sum_{i=1}^n \mathbb{1}\left( \left( (x_\text{trg} - \hat{x}_\text{trg})^2 + (y_\text{trg} - \hat{y}_\text{trg})^2 \right)^\frac{1}{2} \leq \alpha\cdot\text{max}(w_\text{bbox}, h_\text{bbox})\right) $$

We report PCK@0.1$_\text{bbox}$ for all our semantic correspondence experiments.

Note that in practice, the source and target images might have different sizes, and that the keypoints and bounding box sizes are given in pixels, which necessitates conversion between the pixel and representation space. This is achieved by linear scaling according to the ratio between the image and representation size.


### Dense Correspondence {#sec-methods-dc}

In dense correspondence, the task is to match not only specific keypoints, but rather larger regions or even the full content between two images. It can be considered a special case of semantic correspondence, where the keypoints are at every token position. Its applications are, for example, object tracking in videos and estimation of optical flow [@zhang2023tale;@zhang2025difftracker]. We use the dense correspondence task to evaluate failures and biases in the representations, see @sec-biases. Due to the dense matching, inconsistencies in the representations can be better visualized.

The correspondences are found using the same approach as for semantic correspondence, but instead of PCK, we use accuracy as performance metric, i.e. the percentage of correct correspondences. This setup is equivalent to semantic correspondence with keypoints at every token position and PCK@0.


### Depth Estimation {#sec-methods-depth-estimation}

Monocular depth estimation is the task of predicting depth values for each pixel in a single RGB image. This is a challenging task as it requires understanding the 3D structure of a scene from a single 2D view. The task has applications for example in robotics and scene understanding [@Silberman2012Indoor].

We use the \nameref{sec-datasets-nyu-v2} dataset [@Silberman2012Indoor] to evaluate the depth estimation capabilities of SD representations. Following @Chen2023BeyondSS, we train a simple linear probe on the representations to predict depth values. The linear probe maps from the representation channels to a single depth value for each spatial position. This allows us to assess how well depth information is encoded in different blocks of the model while keeping the probe's complexity minimal.

We train the linear probe using the Huber loss, following @Chen2023BeyondSS, with threshold $\delta = 1$, also known as SmoothL1loss [@pytorch2023huberloss]. The Huber loss is defined as 

$$ \mathcal{L}_i = \begin{cases} 0.5\cdot(d_i - \hat{d}_i)^2, & \text{if } |d_i - \hat{d}_i| < \delta \\ \delta \cdot (|d_i - \hat{d}_i| - 0.5 \cdot \delta), & \text{otherwise} \end{cases} $$

where $d_i$ is the ground truth depth and $\hat{d}_i$ is the predicted depth at pixel $i$.

To align with common practice in the depth estimation literature, we evaluate the performance using root mean squared error (RMSE) [@Chen2023BeyondSS;@Patni2024ECoDepth;@stracke2024clean]. Lower RMSE values indicate better depth predictions:

$$ \text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n (d_i - \hat{d}_i)^2} $$


## Datasets {#sec-datasets}

For evaluating the representations and their similarities, we use multiple different datasets, depending on the task:

* classification: \nameref{sec-datasets-cifar}, \nameref{sec-datasets-imagenet}
* semantic correspondence: \nameref{sec-datasets-spair}
* depth estimation: \nameref{sec-datasets-nyu-v2}
* anomaly evaluation: \nameref{sec-datasets-imagenet-subset}, \nameref{sec-datasets-nyu-v2}

For some tasks we also use task specific (synthetic) datasets, which are further described in the respective sections.


### CIFAR {#sec-datasets-cifar .unlisted .unnumbered}

We utilize the CIFAR-10 and CIFAR-100 datasets by @Krizhevsky2009LearningML for linear probe classification (see @sec-linear-probe-classification). The datasets contain 60,000 images (50,000 training, 10,000 test) of size $32\times32$ pixels with 10 and 100 classes respectively. We use the versions hosted on Hugging Face^[CIFAR-10 on Hugging Face: [huggingface.co/datasets/cifar10](https://huggingface.co/datasets/cifar10)] ^[CIFAR-100 on Hugging Face: [huggingface.co/datasets/cifar100](https://huggingface.co/datasets/cifar100)].


### Imagenet {#sec-datasets-imagenet .unlisted .unnumbered}

ImageNet by @ILSVRC15 is a large-scale image recognition dataset. A common variant is the imagenet-1k (ILSVRC) 2012 version hosted on Hugging Face^[ImageNet-1k on Hugging Face: [huggingface.co/datasets/imagenet-1k](https://huggingface.co/datasets/imagenet-1k)]. It consists of 1000 classes and contains 1,281,167 training, 50,000 validation and 100,000 test images, which differ in resolution and aspect ratio.

Due to the large size of the dataset, we run our experiments on a smaller variant, Tiny ImageNet^[Tiny ImageNet on Hugging Face: [huggingface.co/datasets/zh-plus/tiny-imagenet](https://huggingface.co/datasets/zh-plus/tiny-imagenet)], which contains 200 classes and 100,000 images of size $64\times64$ pixels. Each class has 500 images for training, 50 for validation, and 50 for testing. 
For some tasks, we also utilize a custom subset of ImageNet, as described below, with fewer but larger images, compared to Tiny ImageNet.


### ImageNet Subset {#sec-datasets-imagenet-subset .unlisted .unnumbered}

For several of our statistical analyses of the SD representations, we use a small subset of ImageNet [@ILSVRC15] (imagenet-1k variant from Hugging Face), containing 500 images of 5 different classes, i.e. 100 images per class. The images are center-cropped and resized to $512\times512$ pixels. The size of the dataset is a trade-off between statistical significance and the computational complexity of running state-of-the-art diffusion models.

The original ImageNet ids and class names used in the subset are:

* 235: German shepherd, German shepherd dog, German police dog, alsatian
* 242: boxer
* 282: tiger cat
* 717: pickup, pickup truck
* 980: volcano


### SPair-71k {#sec-datasets-spair .unlisted .unnumbered}

The SPair-71k dataset by @min2019spair71k is a large-scale benchmark for evaluating semantic correspondence algorithms. It contains nearly 71,000 image pairs annotated with semantically meaningful correspondences and covers diverse categories, varying object scales and challenging viewpoint variations. The dataset is widely used for advancing research in visual correspondence and matching tasks.

For better usability, we created and published a dataset loading script that can be used with the Hugging Face `datasets` library^[Our SPair-71k page on the Hugging Face Hub: [huggingface.co/datasets/0jl/SPair-71k](https://huggingface.co/datasets/0jl/SPair-71k)].

While working with the SPair-71k dataset, we found that the keypoints usually relate to a specific position on an object, but sometimes they are ambiguous. For example, for a keypoint on a rotational symmetric flowerpot, the semantically correct target position on a second flowerpot object is ambiguous. This effect could cause semantically correct predictions to be rejected. However, as this problem is model independent, we consider the impact to be negligible for our purposes of comparing models and representations.


### NYU Depth v2 {#sec-datasets-nyu-v2 .unlisted .unnumbered}

The NYU Depth v2 dataset contains RGB and depth images of indoor scenes and was introduced by @Silberman2012Indoor. It contains 1449 densely labeled and curated pairs of RGB and depth images. There is also a larger variant with unlabeled pairs, however, we only use the densely labeled and curated pairs. The images were recorded using Microsoft Kinect cameras in different locations and cities. Similar as for the \nameref{sec-datasets-spair} dataset, we created and published a dataset loading script that can be used with the Hugging Face `datasets` library^[Our NYU Depth v2 page on the Hugging Face Hub: [huggingface.co/datasets/0jl/NYUv2](https://huggingface.co/datasets/0jl/NYUv2)].


# Representation Extraction and Exploration {#sec-representation-extraction-exploration}

A significant part of the work for this thesis was dedicated to the exploration of the extracted representations, and the engineering work of developing and optimizing the representation extraction and exploration tools. Both the exploration and the tools provide the basis and inspiration for the analyses and results we present in @sec-downstream-tasks and @sec-biases. This chapter describes the `sdhelper` package, which provides a unified interface to extract representations from different SD models, and the *Representation Similarity Explorer*, which allows for interactive exploration of the extracted representations. Finally, we present results and insights of the exploration of the representation similarities, which lay the foundation for the following chapters.


## `sdhelper` Package {#sec-sdhelper}

We developed and published the `sdhelper` Python package to simplify representation extraction and analysis, offering a unified interface for various SD models^[The `sdhelper` package is available on GitHub: [github.com/JonasLoos/sdhelper](https://github.com/JonasLoos/sdhelper) and can be installed with `pip install sdhelper`.]. In the background, it uses the Hugging Face `diffusers` library [@vonplaten2022diffusers] and SD text-to-image pipeline, which provides a flexible interface for interacting with intermediate outputs [@hfsd]. Representations can be extracted either during image generation from noise or for existing images. We use the `sdhelper` package as a basis for most of our experiments on the representations.

When initializing an `SD` object with a model name, the `sdhelper` package loads the corresponding model from Hugging Face or the local cache. The `SD` object then allows to generate images, extract representations for existing images, and access model properties. The library also simplifies further processing of the representations, including the computation of similarities.

Example usage:

\AddToHookNext{env/Highlighting/begin}{\footnotesize}
```python
from sdhelper import SD

# load model
sd = SD('SD-1.5')

# generate image
img = sd('a beautiful landscape').result_image

# extract representations from the `up[1]` block at time step 50
r = sd.img2repr(img, extract_positions=['up_blocks[1]'], step=50)

# compute similarity between all pairs of tokens in `r`
similarities = r.cosine_similarity(r)
```


## Representation Similarity Explorer {#sec-repr-sim-explorer}

```{python}
#| label: fig-repr-sim-explorer
#| fig-cap: Screenshot of the Representation Similarity Explorer ([sd-similarities.jloos.de](https://sd-similarities.jloos.de)). The selected reference token is highlighted in orange (on the cat's eye).
#| wrapfigure: R 0.5

display_figure('assets/figures/repr-sim-explorer.jpg', 4, 6)
```

To better understand the properties of the extracted representations and the similarities between their tokens, we developed the *Representation Similarity Explorer*. It is a tool allowing to interactively visualize the similarities between the tokens of different blocks and spatial positions for different images and models. For a screenshot of the tool, see @fig-repr-sim-explorer. We provide a public web version at [sd-similarities.jloos.de](https://sd-similarities.jloos.de) that includes precomputed representations for given images. For the upload and exploration of arbitrary images, running the tool locally is required^[Code and setup instructions are available on GitHub: [github.com/JonasLoos/sd_representation_similarity_explorer](https://github.com/JonasLoos/sd_representation_similarity_explorer)].

To use the tool for representation similarity analysis, open the link in a browser and select one or more images to analyze. Now, hover over the images or similarity maps to show the similarities to the token at the current cursor position. For a good first example of interesting semantic correspondences, select two images that contain a human or animal and hover over the position of an eye. For further exploration change the SD model, the block (position), the similarity measure, and the noise level.

The tool allows for uncomplicated exploration of the representation similarities and properties, which helps us to identify interesting patterns and directions for further analyses. For example, many of the biases described in @sec-biases were discovered using it. Available options include:

* custom image upload (locally)
* various SD models
* all blocks of the selected SD model
* various similarity measures, including cosine similarity and $L_p$ norms
* different noise levels (time steps)

Architecturally, the representation similarity explorer is split into a Flask-based backend that computes the representations for uploaded images using the `sdhelper` package (see @sec-sdhelper) and a simple frontend built with HTML, CSS, and JavaScript. To improve the performance and interactivity, we developed an asynchronous webworker in Rust, that computes the shown similarities in the browser in real time.

@luo2023dhf provide a similar interactive demo, in the form of a Jupyter notebook, to showcase their semantic correspondence results. Compared to their demo, our tool is significantly more responsive and provides additional options for exploring the representation similarities.


## Similarity Exploration {#sec-similarities}

```{python}
#| label: fig-similarities-example
#| fig-cap: Cosine similarity maps over the spatial dimensions of selected representations of different SD models. Cosine similarity is relative to the token at position (12,17) in image 1, which is at the position of the cat's eye. The similarities are computed separately for each model.

@cache_mpl_plot
def plot_similarities_example():
    from sdhelper import SD

    # define models and images
    models = ['SD1.5', 'SD2.1', 'SD-Turbo', 'SDXL', 'SDXL-Turbo']
    blocks = [('up_blocks[0]' if 'SDXL' in m else 'up_blocks[1]') for m in models]
    images = [
        'assets/images/cat_on_roof.jpg',
        'assets/images/dog_in_snow.jpg',
        'assets/images/cat_next_to_house.png',
    ]

    # extract representations
    representations = []
    for model, block in zip(models, blocks):
        sd = SD(model, disable_progress_bar=True)
        representations.append(sd.img2repr(images, [block], 50, resize=512, seed=42))

    # plot
    fig, axs = plt.subplots(len(images), len(models)+1, figsize=((len(models)+1)*0.8+0.7, len(images)*0.8+0.5))
    for i, img in enumerate(images):
        axs[i, 0].imshow(Image.open(img))
        axs[i, 0].axis('off')
    for i, r in enumerate(representations[0]):
        for j, rep in enumerate(representations, 1):
            sim = rep[i].cosine_similarity(rep[0]) 
            axs[i, j].imshow(sim[:,:,12,17])
            axs[i, j].axis('off')
        axs[i, 0].text(-0.1, 0.5, f'Image {i+1}', ha='right', va='center', fontsize=8, transform=axs[i, 0].transAxes)
    axs[0, 0].text(0.5, 1.1, 'Image', ha='center', va='bottom', fontsize=8, transform=axs[0, 0].transAxes)
    for j, model in enumerate(models, 1):
        axs[0, j].text(0.5, 1.1, f'{model}\n{blocks[j-1].replace("_blocks", "")}', ha='center', va='bottom', fontsize=8, transform=axs[0, j].transAxes)
    plt.tight_layout()
    plt.subplots_adjust(wspace=0.05, hspace=0.05)
```

```{python}
#| label: fig-similarities-example-blocks
#| fig-cap: "Cosine similarity maps over the spatial dimensions of the blocks of SD-1.5. Cosine similarity is relative to the token at the position of the cat's eye in image 1. The similarities are computed separately for each block."

@cache_mpl_plot
def plot_similarities_example_blocks():
    from sdhelper import SD

    # define models and images
    images = [
        'assets/images/cat_on_roof.jpg',
        'assets/images/cat_going_right.png',
        # 'assets/images/dog_in_snow.jpg',
        # 'assets/images/cat_next_to_house.png',
    ]

    # load model and extract representations
    sd = SD('SD15', disable_progress_bar=True)
    blocks = list(sd15_all_blocks.keys())
    representations = sd.img2repr(images, blocks, 50, resize=512, seed=42)

    # plot
    pos = np.array([12, 17])/32
    fig, axs = plt.subplots(len(images), len(blocks)+1, figsize=((len(blocks)+1)*0.6+0.7, len(images)*0.6+0.34))
    for i, img in enumerate(images):
        axs[i, 0].imshow(Image.open(img))
        axs[i, 0].axis('off')
    for i, r in enumerate(representations):
        for j, block in enumerate(blocks,1):
            sim = r.at(block).cosine_similarity(representations[0].at(block))
            axs[i, j].imshow(sim[:,:,*(pos*len(sim)).astype(int)])
            axs[i, j].axis('off')
        axs[i, 0].text(-0.1, 0.5, f'Image {i+1}', ha='right', va='center', fontsize=8, transform=axs[i, 0].transAxes)
    axs[0, 0].text(0.5, 1.1, 'Image', ha='center', va='bottom', fontsize=8, transform=axs[0, 0].transAxes)
    for j, block in enumerate(blocks, 1):
        block_name = block.replace('_blocks', '').replace('_block', '').replace('_', '-')
        axs[0, j].text(0.5, 1.1, f'{block_name}', ha='center', va='bottom', fontsize=8, transform=axs[0, j].transAxes)
    plt.tight_layout()
    plt.subplots_adjust(wspace=0.05, hspace=0.05)
```

```{python}
#| label: fig-kmeans-norms-pca
#| fig-cap: "Different visualizations of the `up[1]` representations of SD-1.5: K-means clustering with 6 clusters, L2 norms, and the first 3 principal components (PCs) as color channels."
#| wrapfigure: R 0.5

# relevant notebook: h-space-exploration/h-space-clustering.ipynb

@cache_mpl_plot
def plot_kmeans_norms_pca():
    from sdhelper import SD
    from sklearn.cluster import KMeans
    from sklearn.decomposition import PCA

    # load image and representations
    img = Image.open("assets/images/cat_on_roof.jpg")
    block = 'up_blocks[1]'
    sd = SD(disable_progress_bar=True)
    r = sd.img2repr(img.resize((512, 512)), [block], 50, seed=42)[block]

    fig, axs = plt.subplots(2, 2, figsize=(3, 3))

    # img
    axs[0,0].imshow(img)
    axs[0,0].axis('off')
    axs[0,0].set_title('Image', fontsize=8)

    # kmeans
    kmeans = KMeans(n_clusters=6, random_state=42)
    kmeans.fit(r[0].flatten(1).T)
    labels = kmeans.labels_.reshape(r.shape[2], r.shape[3])
    axs[0,1].imshow(labels)
    axs[0,1].axis('off')
    axs[0,1].set_title('K-means', fontsize=8)

    # norms
    axs[1,0].imshow(r[0].norm(dim=0))
    axs[1,0].axis('off')
    axs[1,0].set_title('Norms', fontsize=8)

    # pca
    pca_img = PCA(n_components=3).fit_transform(r[0].flatten(1).T).reshape(r.shape[2], r.shape[3], 3)
    pca_img = (pca_img - pca_img.min()) / (pca_img.max() - pca_img.min()) # normalize to [0,1] range
    axs[1,1].imshow(pca_img)
    axs[1,1].axis('off')
    axs[1,1].set_title('Top 3 PC', fontsize=8)

    plt.tight_layout()
```

Using the representation similarity explorer and `sdhelper` package, we explore the similarities between the representations of various images for the different SD models. The goal is to qualitatively compare the similarity behavior of the representations with our intuitions and expectations to find unexpected behavior and further investigate known properties.

**Cosine Similarity Maps.** As can be seen in @fig-similarities-example, the cosine similarity between representation tokens highlights semantic correspondences, in this case especially between the tokens at the positions of the eyes of the animals. Notably, this similarity is not only meaningful within the same image, but also across different images. These semantic similarities can be observed across all tested models, but some differences are visible. The representations used in @fig-similarities-example are the output of the blocks that do not only look the most semantically meaningful, but which also perform best in semantic correspondence, as will be described in @sec-semantic-correspondence. In @fig-similarities-example-blocks, we visualize the similarities over the different blocks of the SD-1.5 U-Net. The upper U-Net blocks (`conv-in`, `down[0]`, `up[3]`) are relatively noisy, and `conv-out` does not appear to be useful for representation extraction. Some lower blocks (`down[2]`, `down[3]`, `mid`) have such a low spatial resolution that the image content is not well recognizable in their similarity maps. Their cross-image similarities, however, appear to be meaningful. In the remaining similarity maps, the semantic content and correspondence to the reference token is visible. Thereby, the `up[1]` representations look particularly relevant, due to their good semantic similarities at a resolution that is high enough to recognize the image content.

**Alternative Visualizations.** @fig-kmeans-norms-pca visualizes the spatial correspondences of the representations using k-means clustering, norms, and principal component analysis. Similar to the cosine similarity maps from @fig-similarities-example, the content of the image is reflected in the visualizations. They show that the representations contain spatially accurate and semantically meaningful information, that can be visualized in different ways. For example, both k-means clustering and the principal components separate between foreground and background. For the L2 norm map, the separation is less clear, but some structure is still visible, and the highest norm is at the position of the cat's face, which might be considered as the most important part of the image. While this observation is very subjective, we find it to be a tendency which is relatively consistent across different images for SD-1.5, SD-2.1, and SD-Turbo. Additional visualizations of the representation norms can be found in @sec-appendix-repr-norms.

Overall, all tested models show semantically meaningful cosine similarity maps, but the meaningfulness highly depends on the block. Not only cosine similarity shows that the representations contain information about the image content, but so do different visualization methods, such as k-means clustering, L2 norm, and principal components. Following these qualitative observations, we will present more quantitative results in the following chapters.


# Performance Evaluation on Downstream Tasks {#sec-downstream-tasks}

To evaluate the quality of the representations for downstream tasks, we use linear probe classification (see @sec-methods-lpc) and semantic correspondence (see @sec-methods-sc) on the datasets described in @sec-datasets. Liner probe classification is good for assessing the overall quality of the representations due to the spatial averaging. Semantic correspondence supplements this, as it additionally relies on accurate spatial correspondence between the representations and the image.


## Linear Probe Classification {#sec-linear-probe-classification}

```{python}
def create_lpc_table(data_path, output_format='pdf'):
    '''create a table for the linear probe classifier results'''
    from IPython.display import display, Latex, HTML
    import json

    with open(data_path) as f:
        all_data = json.load(f)
    rows = ['conv_in', 'down_blocks[0]', 'down_blocks[1]', 'down_blocks[2]', 'down_blocks[3]', 'mid_block', 'up_blocks[0]', 'up_blocks[1]', 'up_blocks[2]', 'up_blocks[3]', 'conv_out']
    sd_names = {'sd15': 'SD-1.5', 'sd21': 'SD-2.1', 'sd-turbo': 'SD-Turbo', 'sdxl': 'SDXL', 'sdxl-turbo': 'SDXL-Turbo'}
    sd_names = {k:v for k,v in sd_names.items() if k in [x[0] for x in all_data]}
    dataset_names = {'cifar10': 'C10', 'cifar100': 'C100', 'zh-plus/tiny-imagenet': 'T-IN'}
    dataset_names = {k:v for k,v in dataset_names.items() if k in [x[1] for x in all_data]}
    step_values = list(sorted({x[2] for x in all_data}))
    columns = {(a1, b1, c1): (a2, b2, c1) for a1, a2 in sd_names.items() for b1, b2 in dataset_names.items() for c1 in step_values}
    matrix = np.zeros((len(rows), len(columns)))
    for *i, data in all_data:
        i = tuple(i)
        if i not in columns:
            print(f'Unknown dataset: {i}')
            continue
        j = list(columns.keys()).index(i)
        for pos, val in data.items():
            i = rows.index(pos)
            matrix[i, j] = val
    max_val = np.max(matrix)

    if output_format == 'pdf':
        # generate fancy latex table
        table = '\\setlength{\\tabcolsep}{3pt}'
        table += '\\small' if len(columns) < 15 else '\\footnotesize'  # sizes: tiny, scriptsize, footnotesize, small, normalsize, ...
        table += '\\begin{tabular}{l' + ''.join(['|' + 'c'*len(dataset_names)*len(step_values)] * len(sd_names)) + '|}\n'
        table += ' & ' + ' & '.join([f'\\multicolumn{{{len(dataset_names)*len(step_values)}}}{{c|}}{{{sd_name}}}' for sd_name in sd_names.values()]) + ' \\\\\n'
        if len(dataset_names) > 1:
            table += ' & ' + ' & '.join([f'{ds_name}' for sd_name, ds_name, step in columns.values()]) + ' \\\\\n'
        if len(step_values) > 1:
            table += ' & ' + ' & '.join([f'{step}' for sd_name, ds_name, step in columns.values()]) + ' \\\\\n'
        table += '\\hline\n'
        max_per_col = np.max(matrix, axis=0)

        for i, row in enumerate(rows):
            row_name = row.replace('_blocks', '').replace('_block', '').replace('_', '-')
            table += f'{row_name}'
            for j in range(matrix.shape[1]):
                val = matrix[i, j]
                color = f'{int(255 - val/max_val * 100)}, {int(255 - val/max_val * 100)}, 255'
                val_str = f'{val:.2f}'[1:] if val > 0 else '-'
                # Make bold if it's the max value in this column
                if val.round(2) == max_per_col[j].round(2) and val > 0:
                    val_str = f'\\textbf{{{val_str}}}'
                table += f' & \\cellcolor[RGB]{{{color}}} {val_str}'
            table += '\\\\\n'
        table += '\\end{tabular}\n'
        display(Latex(table))

    elif output_format == 'html':
        # generate html table
        table = '<table>\n'
        table += '<tr><td></td>' + ''.join([f'<td colspan="{len(dataset_names)*len(step_values)}" style="text-align: center; border-left: 1px solid black;">{sd_name}</td>' for sd_name in sd_names.values()]) + '</tr>\n'
        if len(dataset_names) > 1:
            table += '<tr><td></td>' + ''.join([f'<td style="text-align: center;">{ds_name}</td>' for sd_name, ds_name, step in columns.values()]) + '</tr>\n'
        if len(step_values) > 1:
            table += '<tr><td></td>' + ''.join([f'<td style="text-align: center;">{step}</td>' for sd_name, ds_name, step in columns.values()]) + '</tr>\n'
        max_per_col = np.max(matrix, axis=0)
        for i, row in enumerate(rows):
            row_name = row.replace('_blocks', '').replace('_block', '').replace('_', '-')
            table += f'<tr><td>{row_name}</td>'
            for j in range(matrix.shape[1]):
                val = matrix[i, j]
                color = f'rgb(0, 0, 255, {0.5*val/max_val})'
                val_str = f'{val:.2f}'[1:] if val > 0 else '-'
                # Make bold if it's the max value in this column
                if val.round(2) == max_per_col[j].round(2) and val > 0:
                    val_str = f'<strong>{val_str}</strong>'
                border_style = 'border-left: 1px solid black;' if j % (len(dataset_names)*len(step_values)) == 0 else ''
                table += f'<td style="text-align: center; background-color: {color}; {border_style}">{val_str}</td>'
            table += '</tr>\n'
        table += '</table>'
        display(HTML(table))

    else:
        raise ValueError(f'Unknown output format: {output_format}')
```

:::: {#tbl-lpc}
::: {.content-visible when-format="pdf"}
```{python}
# create_lpc_table('assets/data/lpc_data/step0.json')
# create_lpc_table('assets/data/lpc_data/step50.json')
# create_lpc_table('assets/data/lpc_data/step50_resize128.json')
# create_lpc_table('assets/data/lpc_data/step50_resize256.json')
# create_lpc_table('assets/data/lpc_data/step50_resize256_old.json')
create_lpc_table('assets/data/lpc_data/step50_resize512.json')  # best
# create_lpc_table('assets/data/lpc_data/step100.json')
# create_lpc_table('assets/data/lpc_data/step500.json')
```
:::

::: {.content-visible when-format="html"}
```{python}
create_lpc_table('assets/data/lpc_data/step50_resize512.json', output_format='html')
```
:::

Linear Probe Classification Results for different SD models and blocks on the \nameref{sec-datasets-cifar}-10 (C10), \nameref{sec-datasets-cifar}-100 (C100), and Tiny-\nameref{sec-datasets-imagenet} (T-IN) datasets. Input images are resized to $512\times512$ pixels and representations are averaged over the spatial dimensions. SDXL and SDXL-Turbo do not have `down[3]` and `up[3]` blocks.
::::

@tbl-lpc presents linear probe classification results for \nameref{sec-datasets-cifar}-10, \nameref{sec-datasets-cifar}-100, and Tiny-\nameref{sec-datasets-imagenet}. These results are for time step 50, which we found to perform the best for linear probe classification. They show that the representations contain information useful for classification that is usable by a simple linear probe. However, the results are far from perfect, with a maximum accuracy for SD-1.5 of 83% on \nameref{sec-datasets-cifar}-10, 57% on \nameref{sec-datasets-cifar}-100, and 38% on Tiny-\nameref{sec-datasets-imagenet}. Other SD models achieve similar results. The lower blocks in the U-Net, in the center rows of @tbl-lpc, tend to achieve higher results than the upper blocks. Interestingly, which block achieves the highest results differs between the models. In the case of SD-1.5, the `mid` block achieves the highest results, while for SD-2.1, `down[2]`, `down[3]`, and `up[1]` achieve the best results. In SD-Turbo, all lower blocks from `down[2]` to `up[1]` perform similarly good. For SDXL, and SDXL-Turbo, `down[2]` and `mid` are the best suited blocks. Notably, there is no clear pattern favoring either down or up blocks, suggesting a more nuanced picture of the semantic information distribution across blocks. Interestingly, the results of the distilled models [@sauer2023adversarial], SD-Turbo and SDXL-Turbo, are slightly better than the results of their non-distilled counterparts, SD-2.1 and SDXL, respectively.

We can conclude from these results that the lower blocks in the U-Net contain more high level information useful for classification and that the different models only differ slightly in their performance.

Compared to supervised methods [@dosovitskiy2021an], our results are relatively low. Potential reasons for the suboptimal performance could be the spatial averaging of the representations, which merges foreground and background tokens without distinction. For example, if an image contains more background than foreground, the averaged representation will likely be more biased towards the background. Additionally, the possibly non-linear nature of the learned features might also play a role. @mukhopadhyay2023diffusion indeed find that more complex mappings from the representations to the labels, e.g. CNN- or attention-based, can improve the performance significantly.

Another possible problem with the used datasets is the low resolution of the images. They are resized from 32\times32 in the case of \nameref{sec-datasets-cifar} and 64\times64 in the case of Tiny-\nameref{sec-datasets-imagenet} to a resolution of 512\times512 pixels. The resulting blurring is likely out of distribution compared to the training data of the models. In fact, as discussed in @sec-texture-color-bias and visualized in @fig-appendix-texture-bias-dense-correspondence-blur, blurring the input images can lead to a significant decrease in performance.


## Semantic Correspondence {#sec-semantic-correspondence}

As described in @sec-methods-sc, we evaluate the performance of the representations for unsupervised zero-shot semantic correspondence on the \nameref{sec-datasets-spair} dataset.

```{python}
#| label: fig-sc-pck-over-blocks-noise
#| fig-cap: Percentage correct keypoints (PCK@0.1$_{\text{bbox}}$) for semantic correspondence on \nameref{sec-datasets-spair} for different blocks and time steps.

# Thoughts
# - 512px is lower than default for SD-2.1, and SDXL

@cache_mpl_plot
def plot_sc_pck_over_steps_blocks():

    # load data
    models = ['SD15', 'SD21', 'SD-Turbo', 'SDXL', 'SDXL-Turbo']
    model_names = ['SD-1.5', 'SD-2.1', 'SD-Turbo', 'SDXL', 'SDXL-Turbo']
    block_names = ['conv-in', 'down[0]', 'down[1]', 'down[2]', 'down[3]', 'mid', 'up[0]', 'up[1]', 'up[2]', 'up[3]', 'conv-out']
    noise_steps = [0, 10, 25, 50, 75, 100, 150, 200, 300, 500, 800]
    colors = plt.cm.rainbow(np.linspace(0.0, 1.0, len(block_names)))

    fig, axs = plt.subplots(2, 3, figsize=(9,5))
    for i, model in enumerate(models):
        pck = np.load(f'assets/data/sc_pck_spair_over_blocks_noise_{model}.npy')
        i1, i2 = divmod(i, 3)
        j = 0  # index for pck
        for k, block_name in enumerate(block_names):
            if 'SDXL' in model and '3' in block_name: continue  # SDXL based models don't have `down[3]` and `up[3]`
            axs[i1, i2].plot(noise_steps, pck[j,:]*100, color=colors[k], label=block_name, marker='o', alpha=0.7, markersize=3)
            j += 1
        axs[i1, i2].tick_params(axis='both', labelsize=9)
        axs[i1, i2].set_title(model_names[i])
        axs[i1, i2].grid(axis='y', linestyle='--', alpha=0.3)
        axs[i1, i2].set_xlabel('time step', fontsize=9)
        if i2 == 0: axs[i1, i2].set_ylabel('PCK', fontsize=9)
        else: axs[i1, i2].set_yticklabels([])

    # normalize y limits across all subplots
    y_max = max(ax.get_ylim()[1] for ax in axs.flat)
    for ax in axs.flat:
        ax.set_ylim(0, y_max)

    # Create legend using block_names and colors
    axs[-1, -1].axis('off')
    axs[-1, -1].legend(handles=[plt.Line2D([0], [0], color=colors[i], label=name, marker='o', alpha=0.7, markersize=3) for i, name in enumerate(block_names)], title="block", bbox_to_anchor=(0, 1.05), loc='upper left', fontsize=8)
    plt.tight_layout()
```

```{python}
#| label: fig-sc-pck-over-blocks-resolution
#| fig-cap: Percentage correct keypoints (PCK@0.1$_{\text{bbox}}$) for semantic correspondence on \nameref{sec-datasets-spair} for different blocks and resolutions.

# relevant notebook: semantic_correspondence/analyze_sc_hyper_results_step50_vary_all_else.ipynb

@cache_mpl_plot
def plot_sc_pck_over_blocks_resolution():

    # load data
    pck = np.load('assets/data/sc_pck_spair_step50_vary_all_else.npy')
    model_names = ['SD-1.5', 'SD-2.1', 'SD-Turbo', 'SDXL', 'SDXL-Turbo']
    resolutions = [256, 512, 768, 1024]
    blocks = ['conv_in','down_blocks[0]','down_blocks[1]','down_blocks[2]','down_blocks[3]','mid_block','up_blocks[0]','up_blocks[1]','up_blocks[2]','up_blocks[3]','conv_out']
    block_names = np.array([x.replace('_blocks', '').replace('_block', '').replace('_', '-') for x in blocks])
    sdxl_block_indices = np.array([i for i, b in enumerate(blocks) if '3' not in b])

    # plot
    fig, axs = plt.subplots(2, 3, figsize=(9,5))
    for i, model in enumerate(model_names):
        colors = plt.cm.viridis(np.linspace(0.0, 0.9, len(resolutions)))
        i1, i2 = divmod(i, 3)
        for j in range(len(resolutions)):
            x = block_names[sdxl_block_indices] if 'SDXL' in model else block_names
            y_idx = sdxl_block_indices if 'SDXL' in model else slice(None)
            axs[i1, i2].plot(x, pck[i,j,y_idx]*100, color=colors[j], label=resolutions[j], marker='o', markersize=3, alpha=0.7)
        axs[i1, i2].tick_params(labelsize=9, labelrotation=90)
        axs[i1, i2].set_ylim(0)
        axs[i1, i2].set_title(model_names[i])
        axs[i1, i2].grid(axis='y', linestyle='--', alpha=0.3)
        if i2 == 0: axs[i1, i2].set_ylabel('PCK', fontsize=9)
        else: axs[i1, i2].set_yticklabels([])

    # Get and set global min/max y values across all subplots
    y_max = max(ax.get_ylim()[1] for ax in axs.flat)
    for ax in axs.flat:
        ax.set_ylim(0, y_max)

    axs[-1, -1].axis('off')
    axs[-1, -1].legend(handles=[plt.Line2D([0], [0], color=colors[i], label=f'{name}$\\times${name} px', marker='o', alpha=0.7, markersize=3) for i, name in enumerate(resolutions)], title="resolution", bbox_to_anchor=(0, 1), loc='upper left', fontsize=9)
    plt.tight_layout()
```

```{python}
# load results so they can be used in the text

# sc_results = [np.load(f'assets/data/sc_pck_spair_over_blocks_noise_{model}.npy')*100 for model in ['SD15', 'SD21', 'SD-Turbo', 'SDXL', 'SDXL-Turbo']]
sc_results = np.load('assets/data/sc_pck_spair_step50_vary_all_else.npy') * 100
```

We evaluate the performance of the representations over different blocks and time steps, as shown in @fig-sc-pck-over-blocks-noise. SD-1.5, SD-2.1, and SD-Turbo achieve their best performance with the `up[1]` block, while SDXL and SDXL-Turbo achieve their best performance with the `up[0]` block, which are therefore the blocks we will focus on. Due to the differences in model architecture, these blocks have actually the same depth in the U-Net and the same spatial resolution for images of the same size. The optimal time step (noise level) differs between models, but values between 25 and 100 achieve near optimal performance for all models. This observation motivates our choice of time step 50 as default for our experiments.
Higher input image resolutions can lead to higher PCK values, as shown in @fig-sc-pck-over-blocks-resolution. For SDXL, higher resolutions visibly increase performance at least up to 1024\times1024 (in pixels) for the best block (`up[0]`). Among all other tested models, the optimal resolution is 768\times768, however, for SD1.5, SD-Turbo and SDXL-Turbo, the difference to 512\times512 is only around 2 PCK. A resolution of 1024\times1024 even leads to a decrease in performance compared to 768\times768. As computational requirements increase quadratically with the resolution, we mostly use 512\times512 for our experiments.

The different models achieve PCK@0.1$_\text{bbox}$ values of up to `{python} f"{sc_results[0,2].max():.2f}"` for SD-1.5, `{python} f"{sc_results[1,2].max():.2f}"` for SD-2.1, `{python} f"{sc_results[2,2].max():.2f}"` for SD-Turbo, `{python} f"{sc_results[3,2].max():.2f}"` for SDXL, and `{python} f"{sc_results[4,2].max():.2f}"` for SDXL-Turbo, at our default time step 50 and using an image resolution of 768\times768. These values are similar to the results of @tang2023emergent, who use a similar method and report 52.8 PCK@0.1$_\text{bbox}$ for SD-2.1.

```{python}
#| label: fig-spair-71k-baseline
#| fig-cap: Baseline percentage of correct keypoints (PCK) for semantic correspondence on the \nameref{sec-datasets-spair} dataset using random and same-position estimators, i.e. where the target keypoint is predicted to be random or equal to the source keypoint, respectively.
#| wrapfigure: R 0.4

# relevant notebook: semantic_correspondence/baseline.ipynb

@cache_mpl_plot
def plot_spair_71k_baseline():
    import numpy as np
    import datasets

    # load dataset
    pairs = datasets.load_dataset('0jl/SPair-71k', 'pairs', split='test', trust_remote_code=True).to_list()  # to_list is much faster
    rng = np.random.default_rng(seed=42)

    # compute pck
    bbox_sizes = np.linspace(0, 0.15, 16)
    correct_random = [[] for _ in range(len(bbox_sizes))]
    correct_same_pos = [[] for _ in range(len(bbox_sizes))]
    for pair in pairs:
        trg_max_x, trg_max_y, _ = pair['trg_imsize']
        tbb_max = max(pair['trg_bndbox'][2] - pair['trg_bndbox'][0], pair['trg_bndbox'][3] - pair['trg_bndbox'][1])
        for (sx, sy), (tx, ty) in zip(pair['src_kps'], pair['trg_kps']):
            px, py = rng.random((2,1000)) * [[trg_max_x], [trg_max_y]]
            for i, bbox_size in enumerate(bbox_sizes):
                correct_random[i].append((np.sqrt((px - tx)**2 + (py - ty)**2) < tbb_max * bbox_size).mean())
                correct_same_pos[i].append(np.sqrt((sx - tx)**2 + (sy - ty)**2) < tbb_max * bbox_size)

    # plot
    plt.figure(figsize=(3,2))
    l1, = plt.plot(bbox_sizes, np.mean(correct_random, axis=1)*100)
    l2, = plt.plot(bbox_sizes, np.mean(correct_same_pos, axis=1)*100)
    plt.scatter(0.1, np.mean(correct_random[10])*100)
    plt.scatter(0.1, np.mean(correct_same_pos[10])*100)
    plt.text(0.105, np.mean(correct_random[10])*100, f'{np.mean(correct_random[10])*100:.2f}%', color=l1.get_color(), verticalalignment='top', fontsize=8)
    plt.text(0.105, np.mean(correct_same_pos[10])*100, f'{np.mean(correct_same_pos[10])*100:.2f}%', color=l2.get_color(), verticalalignment='top', fontsize=8)
    plt.xlim(0)
    plt.xlabel('$\\alpha_\\text{bbox}$', fontsize=9)
    plt.ylabel('PCK@$\\alpha_\\text{bbox}$', fontsize=9)
    plt.legend([l2, l1], ['Same position', 'Random'], fontsize=9)
    plt.tight_layout()
```

To be able to estimate if representations are semantically meaningful at all, we compute the PCK for a random and a same-position estimator, see @fig-spair-71k-baseline. If representations yield semantic correspondence results at a similar level as these trivial baselines, they are not semantically meaningful in this context. The random estimator randomly guesses a predicted keypoint with uniform distribution over the input image size. It correctly predicts 2.02% of the keypoints correctly at $\alpha_\text{bbox}=0.1$, which can be considered the lowest baseline above which a semantic correspondence algorithm should perform. The same-position estimator always assumes that the target keypoint is the same as the source keypoint and correctly predicts 5.49% of the keypoints at $\alpha_\text{bbox}=0.1$. This means that estimators should reach a PCK@$0.1_\text{bbox}$ of more than 5.49% to be considered potentially meaningful for semantic correspondence on the \nameref{sec-datasets-spair} dataset. This value is what we find most blocks to converge to for very high noise levels (high time steps), see @fig-sc-pck-over-blocks-noise. Especially some upper blocks (`conv-in`, `down[0]`, `up[3]`, and `conv-out`) are so close to these baselines, that they can be considered unsuitable for semantic correspondence.

Interestingly, we find that the maximum semantic correspondence performance of the different models does not directly correspond to the image generation quality of the models. For example, images generated by SDXL are usually of higher quality than images generated by SD-1.5 or SD-2.1 [@podell2023sdxl], but the semantic correspondence performance of SDXL is significantly lower. Similarly, the performances of the distilled models, SD-Turbo and SDXL-Turbo, are higher than that of their non-Turbo counterparts, SD-2.1 and SDXL, respectively, even though their image quality is lower in general [@sauer2023adversarial]. This aligns with the equivalent findings for linear probe classification in @sec-linear-probe-classification. These observations seem to contradict the intuition that models with higher image quality might also have better representation quality. This might be due to architectural differences between the models, or due to improvements of non-semantic properties, such as texture and style. We leave a more detailed analysis of the relationship between image quality, model architecture, and representation quality for future work.

Overall, our results show that SD representations can be usable for semantic correspondence, which aligns with the results of related work [@tang2023emergent;@zhang2023tale]. The performance depends on the model, block, resolution, and time step. Using suboptimal settings can easily degrade the performance to the level of a trivial baseline (see @fig-sc-pck-over-blocks-noise and @fig-spair-71k-baseline).


## Improvements for Semantic Correspondence {#sec-sc-improvements}

To investigate what augmentations and improvements can be applied to the representations or workflows using them, we perform a series of experiments aiming to improve the semantic correspondence performance on the \nameref{sec-datasets-spair} dataset. However, we find that most of the tested approaches result in only small improvements of 1-2 PCK@0.1$_\text{bbox}$, no significant change, or even a decrease in performance. As this is not the main focus of this thesis, we do not run extensive hyperparameter searches or ablation studies for all tested approaches, but rather report trends and observations based on a few selected experiments.

* **Class prompt**: Using the prompt "a photo of a {class}" during representation extraction might help the model to extract more suitable representations [@xu2023open;@li2023sd4match;@tang2023emergent;@zhang2023tale;@zhang2025three]. We find improvements of 1-2 PCK, compared to an empty prompt.
* **Concatenation of representations**: As described in @sec-representation-similarities, representations of different blocks can be concatenated along the channel dimension to combine information from different blocks [@zhang2023tale;@luo2023dhf;@ji2024diffusion;@baranchuk2022labelefficient]. Only specific combinations improve performance, for example, for SD-1.5, concatenating `up[0]` and `up[1]` seems to perform best and is about 1 PCK better than `up[1]` alone.
* **Subtracting the representations of an empty image**: As further explored in @sec-position-bias, we observe a position bias in the representations. To counteract this bias, we subtract the representations of an empty image, which thus cannot encode much semantic information, from the representations. This could remove the positional information, if it is encoded additively. However, it seems to disrupt the integrity of the representations and leads to a decrease in performance.
* **Image shifting for sub-token-size accuracy**: The observation of lower performance of `down[2]` to `up[0]` compared to `up[1]` in the semantic correspondence task, while the respective performance in the linear probe classification task is better, could be due to the low spatial resolution of these blocks. We shift the input images by fractions of the token size and extract representations for all shifted image variants to gain sub-token-size spatial accuracy. However, we observe artifacts introduced by the combination of different representations, and do not see any significant change in performance.
* **Representation averaging**: As the random noise introduced during the representation extraction might lead to degradation of representations, we hypothesize that averaging the representations over multiple representations of the same image with different noise seeds might improve representation quality. We observe improvements of around 1 PCK for when averaging over 10 representations, which is similar to the observations reported by @tang2023emergent.
* **Cosine similarity over multiple representations**: Similar to *representation averaging*, but instead of averaging multiple representations, we compute the cosine similarity between multiple representations of the same image with different noise seeds and use the maximum similarity to determine the predicted keypoint. We observe similar improvements of around 1 PCK.
* **Cosine similarity over multiple image scales**: Similar to *cosine similarity over multiple representations*, but we compute the cosine similarity between representations of the same image at different scales. This is motivated by the observation that the objects in different images of \nameref{sec-datasets-spair} are often at different scales. The results vary heavily depending on the image scale used and can significantly degrade performance when including e.g. very small scales. In some cases the performance improves slightly, but a more extensive analysis would be recommended.
* **Cosine similarity over image flips**: We extract the representations of the same image of both the original and horizontally flipped version, compute the cosine similarities separately, and then average the cosine similarity maps before computing the argmax. We observe improvements of around 2 PCK.
* **Image padding**: As further explored in @sec-anomalies-corner, the corners and borders of the representations are sometimes less semantically meaningful. We also find that the semantic correspondence performance is around 10 PCK lower for keypoints near the borders of the image. We hypothesize that padding the images with zeros might improve the performance, as then no keypoints are near the borders anymore. However, we find mostly similar performance, but very slight improvements of less than 1 PCK in some cases.
* **PCA dimensionality reduction**: To increase quality of the representations, we perform principal component analysis (PCA) dimensionality reduction along the channel dimension, computed over all tokens and images. We try different numbers of components, but observe at most very small improvements of less than 1 PCK. We refer to @zhang2023tale for more information.

Overall, we find that some of the evaluated approaches lead to small improvements of a few PCK maximum on the semantic correspondence task, while others do not increase, or even decrease, performance. Several approaches that improve performance, also reduce the impact of noise, mostly though averaging multiple representations or their similarities. This suggests that when using multiple approaches at once, the improvements may not be additive. For many, a disadvantage are the additional computational requirements, mostly due to additional representations that need to be computed. Among these results, the most promising approaches are the usage of a *class prompt*, *concatenation of representations*, *representation averaging*, and *cosine similarity over image flips*. *Cosine similarity over multiple image scales* and *PCA dimensionality reduction* also show potential, but require further investigation.

For more complex workflows with good performance, we refer to @zhang2023tale, with the improvements introduced by @zhang2024telling and @stracke2024clean. To the best of our knowledge, this combination sets the current state-of-the-art performance of 69.99 PCK@0.1$_\text{bbox}$ for unsupervised zero-shot semantic correspondence on \nameref{sec-datasets-spair}, as of December 2024.

<!-- Could do: maybe add results when applying all these improvements -->


# Biases in the Representations {#sec-biases}

During our experiments and exploration of the representation similarities, we observe several unexpected patterns, or biases, in the representations. These biases in the representations include an encoding of position, different levels of abstraction over the different blocks, and groups of tokens with unusually high norm and non-semantic similarities. These biases and their implications are explored and discussed in the following sections.


## Position Bias {#sec-position-bias}

```{python}
#| label: fig-position-similarities-empty-image
#| fig-cap: Observed positional embeddings in SD-1.5 `mid` representations, which have a spatial shape of $8\times8$. Each tile shows the cosine similarity between the corresponding reference location (x, y) and the rest of the representation, averaged over 100 empty images to reduce noise impact.
#| wrapfigure: R 0.5

@cache_mpl_plot()
def plot_position_similarities_empty_image():
    from sdhelper import SD
    import torch

    # calculate similarities
    sd = SD(disable_progress_bar=True)
    representations = sd.img2repr([Image.new('RGB', (512, 512), (0, 0, 0))]*100, extract_positions=['mid_block'], step=50, seed=42)
    avg_similarities = torch.stack([r.cosine_similarity(r) for r in representations]).mean(dim=0)

    # setup matrix for plotting
    n = avg_similarities.shape[0]
    result_matrix = torch.full((n**2+n+1,n**2+n+1), torch.nan)
    for i in range(n):
        for j in range(n):
            result_matrix[i*(n+1)+1:(i+1)*(n+1),j*(n+1)+1:(j+1)*(n+1)] = avg_similarities[i,j]

    # plot
    fig, ax = plt.subplots(figsize=(3.5,3))
    ax.imshow(result_matrix, cmap='viridis', interpolation='nearest')
    for spine in ax.spines.values(): spine.set_visible(False)
    ax.set_xticks(torch.arange(0,(n+1)*n,(n+1)) + n/2 + .5, range(n), fontsize=9)
    ax.set_yticks(torch.arange(0,(n+1)*n,(n+1)) + n/2 + .5, range(n), fontsize=9)
    ax.set_xlabel('x position', fontsize=9)
    ax.set_ylabel('y position', fontsize=9)
    cbar = plt.colorbar(ax.images[0], ax=ax, fraction=0.043, pad=0.06)
    cbar.ax.tick_params(labelsize=7)
    cbar.ax.set_ylabel('cosine similarity', fontsize=8)
    plt.tight_layout()
```

Position bias refers to the tendency of representation tokens to encode information about their spatial position in the image. It is one of the first properties we observed in the representations, where closeby tokens share an increased cosine similarity, even when no clear semantic connection can be observed. @fig-position-similarities-empty-image visualizes this bias in the `mid` representations of empty images (RGB values of 0). It is clearly visible that each position has increased similarity to nearby positions. An exception are border and corner positions, where the similarity does not expand into the center region of the image. This behavior is further explored in @sec-anomalies-corner. Notably, position bias also extends across images, with representations at the same positions in different images showing higher cosine similarity between each other.


### Linear Position Estimation {.unnumbered .unlisted}

```{python}
#| label: fig-position-classifier-accuracy
#| fig-cap: Position estimation test accuracies on the output of the different blocks and layers of SD-1.5 for different resolutions on which the accuracy is evaluated. Two linear probe position estimators are trained on the representations of the \nameref{sec-datasets-imagenet-subset}. The classification estimator has outputs for each row and column of the representation, while the regression estimator has only the x and y coordinates as outputs.

# relevant notebook: representation_exploration/position_classifier.ipynb

@cache_mpl_plot()
def plot_position_classifier_accuracy():
    from sdhelper import SD
    import torch
    import numpy as np
    import datasets
    from matplotlib.legend_handler import HandlerTuple
    from matplotlib.lines import Line2D
    import torch.nn as nn

    # load model and dataset
    blocks = [x for y in sd15_all_blocks.values() for x in y]
    data = datasets.load_dataset('JonasLoos/imagenet_subset', split='train')
    images = [x['image'] for x in data]
    sd = SD('SD1.5', disable_progress_bar=True)
    representations = sd.img2repr(images, blocks, 50, seed=42)
    sizes_to_compare = [8, 16, 32, 64]

    # free (v)ram
    del sd
    if torch.cuda.empty_cache():
        torch.cuda.empty_cache()

    # train config
    batch_size = 512
    num_epochs = 5
    num_train = int(len(representations) * 0.8)

    # set seed for reproducibility
    torch.manual_seed(42)
    torch.cuda.manual_seed(42)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    # setup logging variables
    regressors = []
    classifiers = []
    accuracies_reg = torch.full((len(blocks), num_epochs, len(sizes_to_compare)), torch.nan)
    accuracies_cls = torch.full((len(blocks), num_epochs, len(sizes_to_compare)), torch.nan)

    # train models
    for block_idx, block in enumerate(blocks):

        # setup models
        _, features, w, h = representations[0][block].shape
        regressor = nn.Linear(features, 2).cuda()
        opt1 = torch.optim.Adam(regressor.parameters(), lr=1e-3)
        classifier = nn.Linear(features, w+h).cuda()
        opt2 = torch.optim.Adam(classifier.parameters(), lr=1e-3)
        regressors.append(regressor)
        classifiers.append(classifier)

        # get representations
        reprs = torch.stack([r[block].squeeze(0) for r in representations]).permute(0, 2, 3, 1).flatten(0, 2).cuda()
        labels = torch.stack(torch.meshgrid(torch.arange(w), torch.arange(h), indexing='ij'), dim=-1).expand(len(representations), -1, -1, -1).flatten(0, 2).cuda()
        reprs_train = reprs[:num_train*w*h]
        labels_train = labels[:num_train*w*h]
        reprs_test = reprs[num_train*w*h:]
        labels_test = labels[num_train*w*h:]

        # epoch loop
        for epoch in range(num_epochs):

            # train
            regressor.train()
            classifier.train()
            indices = torch.randperm(len(reprs_train))
            for i in range(0, len(reprs_train), batch_size):
                reprs_batch = reprs_train[indices[i:i+batch_size]].float()
                labels_batch = labels_train[indices[i:i+batch_size]]

                regressor.zero_grad()
                preds = regressor(reprs_batch)
                loss = nn.functional.mse_loss(preds, labels_batch.float())
                loss.backward()
                opt1.step()

                classifier.zero_grad()
                preds = classifier(reprs_batch).view(batch_size, w, 2)
                loss = nn.functional.cross_entropy(preds, labels_batch)
                loss.backward()
                opt2.step()

            # test at epoch end
            with torch.no_grad():
                regressor.eval()
                classifier.eval()
                preds_reg = regressor(reprs_test.float())
                preds_cls = classifier(reprs_test.float()).view(len(reprs_test), w, 2)
                loss_reg = nn.functional.mse_loss(preds_reg, labels_test.float())
                loss_cls = nn.functional.cross_entropy(preds_cls, labels_test)
                for i, comparison_size in enumerate(sizes_to_compare):
                    accuracies_reg[block_idx, epoch, i] = (preds_reg.round()*comparison_size // w == labels_test*comparison_size // w).float().mean().cpu()
                    accuracies_cls[block_idx, epoch, i] = (preds_cls.argmax(dim=1)*comparison_size // w == labels_test*comparison_size // w).float().mean().cpu()

    # setup figure
    fig, ax = plt.subplots(figsize=(8, 3))

    # plot x ticks
    x = np.arange(len(blocks))
    ticks = ['attn' if 'attentions' in block else 'res' if 'resnets' in block else 'down' if 'downsamplers' in block else 'up' if 'upsamplers' in block else 'conv' if 'conv' in block else '?' for block in blocks]
    ax.set_xticks(x)
    ax.set_xticklabels(ticks, rotation=90)

    # compute main blocks names and positions
    main_blocks = []
    main_block_positions = []
    layer_counter = 0
    for block_name, block_list in sd15_all_blocks.items():
        if 'mid' in block_name:
            name = 'mid'
        elif 'conv' in block_name:
            name = block_name[5:]
        else:
            name = block_name.replace('_blocks','').replace('[','').replace(']','').replace('down', 'dn')
        main_blocks.append(name)
        main_block_positions.append(layer_counter)
        layer_counter += len(block_list)

    # plot main blocks names
    ax_x2 = ax.secondary_xaxis(location=0)
    ax_x2.set_xticks([p+len(bl)/2-0.5 for p, bl in zip(main_block_positions, sd15_all_blocks.values())], labels=[f'\n\n\n{b}' for b in main_blocks], ha='center')
    ax_x2.tick_params(length=0)
    ax.text(-0.01, -0.12, 'layers:', ha='right', va='center', transform=ax.transAxes)
    ax.text(-0.01, -0.30, 'blocks:', ha='right', va='center', transform=ax.transAxes)

    # lines between main blocks
    for p in main_block_positions[1:]:
        ax.axvline(x=p-0.5, color='black', linestyle='--', c='lightgray')
    ax_x3 = ax.secondary_xaxis(location=0)
    ax_x3.set_xticks([p-0.5 for p in main_block_positions[1:]], labels=[])
    ax_x3.tick_params(axis='x', length=34, width=1.5, color='lightgray')

    # plot accuracies
    colors_reg = plt.cm.Blues(np.linspace(0.1, 0.9, len(sizes_to_compare)))
    colors_cls = plt.cm.Reds(np.linspace(0.1, 0.9, len(sizes_to_compare)))
    for i in range(accuracies_reg.shape[2]):
        l1 = ax.plot(accuracies_cls[:, -1, i]*100, label=f'classification', color=colors_cls[i])
        l2 = ax.plot(accuracies_reg[:, -1, i]*100, label=f'regression', color=colors_reg[i])

    # legends
    legend1 = ax.legend(
        [tuple(Line2D([0], [0], color=c[i], marker='o', markersize=10, linestyle='None') for c in [colors_reg, colors_cls]) for i in range(accuracies_reg.shape[2])], 
        [f'{x}{x}' for x in sizes_to_compare], 
        handler_map={tuple: HandlerTuple(ndivide=None)},
        loc='center left',
        bbox_to_anchor=(1, 0.7),
        title="resolution"
    )
    ax.add_artist(legend1)

    legend2 = ax.legend(
        [tuple(Line2D([0], [0], color=c[i], marker='o', markersize=10, linestyle='None') for i in range(accuracies_reg.shape[2])) for c in [colors_cls, colors_reg]], 
        [l1[0].get_label(), l2[0].get_label()], 
        handler_map={tuple: HandlerTuple(ndivide=None)},
        loc='center left',
        bbox_to_anchor=(1, 0.2),
        title="estimator type"
    )

    # finish plot
    ax.set_ylabel('test accuracy')
    plt.tight_layout()
```

The observation of similarities based on position means that spatial position is saliently encoded in the representation tokens. To quantify this observation, we explore how well this positional embedding can be linearly extracted. @fig-position-classifier-accuracy shows the test accuracy of two different linear probe position estimators that take one representation token as input and predict the position of the token in the image. We train the linear models on 80% of the \nameref{sec-datasets-imagenet-subset} using the Adam optimizer [@kingma2017adam] and evaluate them on the remaining 20%. The classification estimator treats the rows and columns of the representation as each as a separate set of classes from which it tries to choose the correct row and column. The regression estimator has two continuous outputs for the x and y coordinates of the token, which are then discretized to the nearest integer, i.e. position. It is visible that the classification estimator performs significantly better than the regression estimator, and reaches an accuracy of over 90% on the output of some layers. Both models have better performance in the representations of the lower blocks of the U-Net, i.e. `down[2]` to `up[1]`. There, the problem is also easier due to the smaller spatial resolution of the representations. But even when evaluating the upper blocks using lower resolutions, which is visualized by the shaded lines, the lower blocks still perform better. Therefore, we conclude both from this experiment and the qualitative observations, that the lower layers in the U-Net seem to have a more pronounced positional embedding.

For a qualitative visualization of the position estimation results of the models used for @fig-position-classifier-accuracy, see @sec-appendix-position-bias (@fig-appendix-position-classifier-example).


### Different Image Sizes {.unnumbered .unlisted}

```{python}
#| label: fig-pos-embedding-aspect-ratios
#| fig-cap: Behavior of position embedding over different aspect ratios and image sizes for SD-1.5. The first row shows a color gradient for the default image size of 512512, which is then transferred according to the highest cosine similarity between the representations of the respective block to the 3 different target resolutions.

# Thoughts
# - Maybe say that we use 1 empty image
# - it's fairly consistent over different numbers of samples
# - how was the positional embedding in SD again? I checked it at some point - either none or learned embedding in the attention layers.
# - maybe we should further investigate the improved performance near the borders
# - what might this insight be relevant for?

@cache_mpl_plot
def plot_position_embedding_over_different_aspect_ratios():
    from sdhelper import SD
    import numpy as np
    import torch
    import matplotlib.gridspec as gridspec

    pos = [f'down_blocks[{i}]' for i in range(4)] + ['mid_block'] + [f'up_blocks[{i}]' for i in range(4)]
    noise_step = 50
    res1 = (512, 512)
    res_others = [
        (768,768),
        (256, 512),
        (512, 256),
        (256, 256),
    ]
    seed = 42
    num_samples = 1

    # Create two empty images
    img1 = Image.new('RGB', res1, (0, 0, 0))
    img_others = [Image.new('RGB', res, (0, 0, 0)) for res in res_others]

    # Compute cosine similarity
    sd = SD('SD1.5', disable_progress_bar=True)
    cossims = [{p: [] for p in pos} for _ in range(len(res_others))]
    for _ in range(num_samples):
        r1 = sd.img2repr(img1, pos, noise_step, seed=seed, output_device='cuda')
        r_others = sd.img2repr(img_others, pos, noise_step, seed=seed, output_device='cuda')
        for p in pos:
            for i, r2 in enumerate(r_others):
                cossims[i][p].append(r1.at(p).cosine_similarity(r2.at(p)).cpu())
    cossims = [{p: torch.stack(v).mean(0) for p, v in x.items()} for x in cossims]

    # Set DPI and spacing
    dpi = 500
    h_spacing = 0.1  # Horizontal spacing between plots in inches
    v_spacing = 0.1  # Vertical spacing between plots in inches
    text_width = 0.9

    # Calculate figure size
    all_res = [res1] + res_others
    max_width = max(res[0] for res in all_res) / dpi
    max_height = max(res[1] for res in all_res) / dpi
    fig_width = len(pos) * (max_width + v_spacing) + text_width
    fig_height = sum(res[1] for res in all_res) / dpi + h_spacing * len(all_res)


    # Create figure
    fig = plt.figure(figsize=(fig_width, fig_height), dpi=dpi)

    # Create GridSpec
    gs = gridspec.GridSpec(len(all_res), len(pos) + 1, 
                        height_ratios=[res[1]/dpi for res in all_res],
                        width_ratios=[text_width] + [max_width] * len(pos),
                        hspace=h_spacing, wspace=v_spacing)

    # Add column labels
    for j, res in enumerate(all_res):
        label = f'Source\n{res1}' if j == 0 else f'Target\n{res}'
        ax = fig.add_subplot(gs[j, 0])
        ax.text(0.5, 0.5, label, ha='center', va='center', fontsize=12)
        ax.axis('off')

    for i, p in enumerate(pos):
        # Add row labels
        ax_row = fig.add_subplot(gs[:, i+1])
        block_name = p.replace('_blocks', '').replace('_block', '').replace('_', '-')
        ax_row.text(0.5, -0.05, block_name, va='top', ha='center', fontsize=12, transform=ax_row.transAxes)
        ax_row.axis('off')

        for j, res in enumerate(all_res):
            ax = fig.add_subplot(gs[j, i+1])

            if j == 0:  # Source image
                source_shape = cossims[0][p].shape[:2]
                source_color = np.zeros((*source_shape, 3))
                offsets = np.array(source_shape)/2 - .5
                for k in range(source_shape[0]):
                    for l in range(source_shape[1]):
                        angle = np.arctan2(k-offsets[0], l-offsets[1])
                        dist = 1 - np.sqrt((k-offsets[0])**2 + (l-offsets[1])**2) / (offsets[0]**2+offsets[1]**2)**.5
                        source_color[k, l, :] = np.array([.5+.5*np.sin(angle), .5+.5*np.sin(angle+np.pi/2), dist])
                ax.imshow(source_color, aspect='equal', interpolation='nearest')
            else:  # Target images
                rows, cols = np.unravel_index(cossims[j-1][p].flatten(end_dim=1).argmax(axis=0), source_shape)
                target_color = source_color[rows, cols]
                ax.imshow(target_color, aspect='equal', interpolation='nearest')

            ax.axis('off')

    plt.subplots_adjust(top=1, bottom=0.1, left=0, right=1)
```

Furthermore, we investigate the behavior of the position bias over different aspect ratios and image sizes in @fig-pos-embedding-aspect-ratios. This is visualized on a colorwheel, which is an image containing a color gradient. It is used to show which regions of an image are mapped where. The shown mapping is created by searching the token in the source representation with the highest cosine similarity for each token in the target representation and transferring the pixels of the colorwheel accordingly. The ideal mapping would preserve the relative positions in the colorwheel as much as possible. 
As visible in the figure, for the higher blocks in the U-Net, especially `down[0]`, `up[2]`, and `up[3]`, the mapping is mostly random, i.e. the positional correspondences are not preserved. The lower blocks better preserve the positional information.
This observation aligns well with the previous observations using the position estimation task in @fig-position-classifier-accuracy. Interestingly, the mapping does not seem to just be linearly scaled depending on the resolution, but rather depends on the relative distances to the nearest border. Additionally, in all mappings, the borders tend to better preserve the color gradient, i.e. seem to have more reliable positional information.


### Dense Correspondence {.unnumbered .unlisted}

```{python}
#| label: fig-dense-correspondence-flip
#| fig-cap: Dense Correspondence when flipping images. The representations are extracted for an image in both original and flipped/mirrored form and then each token of the flipped representation is matched to the token with the highest cosine similarity in the original representation. The emerging mapping is visualized on the original image pixels (column 1, 4), on a colorwheel (column 2, 5), and with a map of the error between the predicted mapping and the target mapping of flipping horizontally (column 3, 6). For more info and a figure containing all blocks, see @sec-appendix-position-bias (@fig-appendix-dense-correspondence-flip-all-blocks).

# relevant notebook: semantic-correspondence/artifical_dataset/show_flip_sc_fails.ipynb

# Thoughts
# - calling the flipped image "target" might be confusing, as it would contain the source keypoints when interpreting the mapping as a semantic correspondence task.


@cache_mpl_plot()
def plot_dense_correspondence_flip(n=512, seed=42, step=50):
    from sdhelper import SD
    from PIL import ImageOps
    import numpy as np
    import torch

    sd = SD(disable_progress_bar=True)
    img_paths = [
        'assets/images/cat_next_to_house.png',
        'assets/images/cat_going_right.png',
    ]
    imgs = [Image.open(img_path).resize((n, n)) for img_path in img_paths]
    poss = [
        ['mid_block'],
        ['up_blocks[0]'],
        ['up_blocks[1]'],
        ['up_blocks[2]'],
    ]
    reprs = [[sd.img2repr(img, pos, step=step, seed=seed) for pos in poss] for img in imgs]
    reprs_flipped = [[sd.img2repr(ImageOps.mirror(img), pos, step=step, seed=seed) for pos in poss] for img in imgs]

    # setup colorwheel
    colorwheel = np.zeros((n, n, 3), dtype=np.uint8)
    offset = n/2 + .5
    for i in range(n):
        for j in range(n):
            angle = np.arctan2(i-offset, j-offset)
            dist = 1 - np.sqrt((i-offset)**2 + (j-offset)**2) / offset / np.sqrt(2)
            colorwheel[i, j, :] = np.array([.5+.5*np.sin(angle), .5+.5*np.sin(angle+np.pi/2), dist]) * 255

    # setup figure
    fig = plt.figure(figsize=((len(imgs)*3)*2+1, (len(poss)+2)*2))
    outer_gs = fig.add_gridspec(1, 2, wspace=0.2)
    gss = [outer_gs[i].subgridspec(len(poss)+2, 3) for i in range(len(imgs))]
    axs = [[fig.add_subplot(gss[i][j, k]) for k in range(3)] for j in range(len(poss)+2) for i in range(len(imgs))]
    axs = np.array(axs).reshape(len(poss)+2, len(imgs)*3)

    # plot original images
    for i, img in enumerate(imgs):
        axs[0, 3*i].imshow(img)
        axs[0, 3*i].axis('off')
        axs[0, 3*i].set_title('(mapped)\nimage')
        axs[0, 3*i+1].imshow(colorwheel)
        axs[0, 3*i+1].axis('off')
        axs[0, 3*i+1].set_title('(mapped)\ncolorwheel')
        axs[0, 3*i+2].imshow(np.zeros(colorwheel.shape[:2]), cmap='YlOrRd')
        axs[0, 3*i+2].axis('off')
        axs[0, 3*i+2].set_title('error') 

    # plot flipped images
    for i, img in enumerate(imgs):
        axs[1, 3*i].imshow(ImageOps.mirror(img))
        axs[1, 3*i].axis('off')
        axs[1, 3*i+1].imshow(colorwheel[:, ::-1])
        axs[1, 3*i+1].axis('off')
        axs[1, 3*i+2].imshow(np.zeros(colorwheel.shape[:2]), cmap='YlOrRd')
        axs[1, 3*i+2].axis('off')

    # add horizontal line between rows 2 and 3
    fig.patches.extend([plt.Rectangle((0.08, 0.625), 0.82, 0.002, facecolor='gray', alpha=0.4, transform=fig.transFigure)])

    # plot transferred images
    for i, img in enumerate(imgs):
        for j, pos in enumerate(poss):
            img = np.array(img)
            similarities = reprs[i][j].cosine_similarity(reprs_flipped[i][j])
            transferred_img = np.zeros_like(img)
            transferred_colorwheel = np.zeros_like(img)
            m = similarities.shape[0]
            s = n // m
            for k in range(m):
                for l in range(m):
                    argmax = similarities[:,:,k, l].flatten().argmax()
                    k_, l_ = argmax // m, argmax % m
                    transferred_img[k*s:(k+1)*s, l*s:(l+1)*s] = img[k_*s:(k_+1)*s, l_*s:(l_+1)*s][:, ::-1]  # use ::-1 to flip, and ::1 to not flip
                    transferred_colorwheel[k*s:(k+1)*s, l*s:(l+1)*s] = colorwheel[k_*s:(k_+1)*s, l_*s:(l_+1)*s][:, ::-1]

            indices = similarities.view(-1, m, m).argmax(dim=0)
            k_, l_ = indices // m, indices % m
            l_ = m - 1 - l_  # flip (mirror)
            k, l = torch.meshgrid(torch.arange(m), torch.arange(m), indexing='ij')
            errors = ((k - k_)**2 + (l - l_)**2)**.5
            all_distances = torch.cdist(*[torch.stack([k.flatten(), l.flatten()], dim=1).float()]*2)
            percentiles = (all_distances < errors.flatten().unsqueeze(1)).float().mean(dim=1).reshape(errors.shape)

            axs[j+2, 3*i].imshow(transferred_img)
            axs[j+2, 3*i].axis('off')
            axs[j+2, 3*i+1].imshow(transferred_colorwheel)
            axs[j+2, 3*i+1].axis('off')
            axs[j+2, 3*i+2].imshow(percentiles, cmap='YlOrRd', interpolation='nearest')
            axs[j+2, 3*i+2].axis('off')

    # add titles
    position_names = [pos.replace('_blocks', '').replace('_block', '').replace('_', '-') for pos, in poss]
    for i, pos in enumerate(['original', 'flipped\n(target)'] + position_names):
        axs[i, 0].text(-0.1, 0.5, pos, va='center', ha='right', transform=axs[i, 0].transAxes)
    axs[3, 0].text(-0.6, -0.05, 'blocks', va='center', ha='right', transform=axs[3, 0].transAxes, rotation=90, fontsize=14)
```

One downstream task, where the positional embedding is visible, is dense correspondence, where each point in one image should be mapped to another image. A simple variant of this task is matching images with their flipped version, as visualized in @fig-dense-correspondence-flip, where we show the results for two example images. Especially in the `up[0]` row, we see regions of high errors, where the colorwheel mapping is exactly the same as the original, instead of being similar to the flipped version. This indicates that the mapping is dominated by the position embedding. Interestingly, this primarily occurs in parts of the image where the semantic content of the image in the error regions does not differ much between the original and flipped images. This is especially visible in the left example for `up[0]` in @fig-dense-correspondence-flip, where the error mainly occurs in the region of the grass, which goes over the full width of the image and is thus semantically invariant to flipping. In the right example, most of the background is semantically invariant to flipping, and thus the regions with high error is much larger.

This effect also appears at the spatial locations showing a cat in the left example at all shown blocks. However, it is less visible in the error map due to the small distance between the left and right side of the cat. Here, the "erroneous" mapping does not only make sense from a positional embedding perspective, but also from a semantic one. The shown mapping preserves the cat's orientation, i.e. the left side of the cat is mapped to the left side of the new flipped cat. This effect highlights the limitations of the dense correspondence task over flipped images. In this task, but also in other cases, what is semantically meaningful and desired can differ depending on the task and user.

In several places, there are single tokens with high error at the borders or corners, i.e. corner or border anomalies, an effect that we further investigate in @sec-anomalies-corner.


### Semantic Correspondence {.unnumbered .unlisted}

```{python}
#| label: fig-sc-errors-by-relative-position
#| fig-cap: Mean semantic correspondence error rate over the predicted position/distance relative to the source keypoint. A prediction is considered an error, if the distance between predicted and target keypoint is larger than 16 pixels, i.e. larger than the size of one representation token. We use the `up[1]` representations, except for the SDXL and SDXL-Turbo, where `up[0]` is used. We exclude very high distances from the plots due to low sample size. Extended plots and more context can be found in @sec-appendix-position-bias (@fig-appendix-sc-errors-by-relative-position-full).
#| fig-subcap:
#|   - "Sample of the synthetic dataset. The target similarities are relative to the source keypoint and used for predicting the target keypoint."
#|   - "Error rate over the relative position to the source keypoint for SD-1.5. The center (0, 0) is the location of the source keypoint."
#|   - "Error rate over the distance relative to the source keypoint for different models."
#| layout: "[[-15,70,-15],[48, -4, 48]]"

# Thoughts
# - is the example good? Should it rather show a clear failure case instead of a an unclear one?
# - is the shared y-axis/colorbar a good idea?
# - cropping the outer parts might be a bit misleading, as there are even more extreme values in the outer regions. But not cropping would make the main point much less clear.

# relevant notebook: semantic_correspondence/artifical_dataset/sc_errors_over_position.ipynb

@cache_mpl_plot
def plot_example():
    import torch.nn.functional as F
    import pickle

    plot_data = pickle.load(open('assets/data/sc_errors_over_position_example_plot_data.pkl', 'rb'))
    r1, r2, img1, img2, kp1, kp2 = plot_data.values()
    background_size = 512
    n = r1.shape[1]
    kp1_x = int(kp1[0]) * n // background_size
    kp1_y = int(kp1[1]) * n // background_size
    best_idx = F.cosine_similarity(r1[:,kp1_y,kp1_x,None,None], r2[:,:,:], dim=0).flatten().argmax()
    y_trg_base, x_trg_base = np.unravel_index(best_idx, (n,n))
    x_trg = (x_trg_base + 0.5) * background_size // n
    y_trg = (y_trg_base + 0.5) * background_size // n
    fig, axs = plt.subplots(1,3, figsize=(3*1.4, 1.4+0.3))
    axs[0].imshow(img1)
    axs[0].set_title('source image', fontsize=8)
    axs[0].scatter(*kp1, c='red', marker='x', alpha=0.8, label='source keypoint')
    axs[0].set_xticks([])
    axs[0].set_yticks([])
    axs[0].legend(fontsize=6)
    axs[1].imshow(img2)
    axs[1].set_title('target image', fontsize=8)
    axs[1].scatter(*kp2, c='red', marker='x', alpha=0.8, label='target keypoint')
    axs[1].scatter(x_trg, y_trg, c='green', marker='+', alpha=0.8, label='predicted target')
    axs[1].legend(fontsize=6)
    axs[1].set_xticks([])
    axs[1].set_yticks([])
    axs[2].imshow(F.cosine_similarity(r1[:,kp1_y,kp1_x,None,None], r2[:,:,:], dim=0).cpu().view(n,n))
    axs[2].set_title('target similiarities', fontsize=8)
    axs[2].axis('off')
    plt.tight_layout()

@cache_mpl_plot
def plot_sc_errors_by_relative_position():
    import torch

    # load data
    errors = torch.load('assets/data/sc_errors_by_position_error_counts_SD1.5.pt', weights_only=True)
    counts = torch.load('assets/data/sc_errors_by_position_counts_SD1.5.pt', weights_only=True)

    # plot
    fig, ax = plt.subplots(figsize=(3,2))
    tmp = (errors / counts).reshape(len(errors)//4,4,len(errors)//4,4).mean(dim=(1,3))  # downscale
    ax.imshow(tmp[5:-5,5:-5])  # remove outer stuff with low counts
    ax.set_aspect('equal')
    ax.set_xlabel('relative x [px]', fontsize=8)
    ax.set_ylabel('relative y [px]', fontsize=8)
    ticks = np.linspace(-512, 512, 9)
    tick_positions = np.linspace(-5, len(tmp)-5, 9) - 0.5
    ax.set_xticks(tick_positions[2:-2])
    ax.set_xticklabels([str(int(x)) for x in ticks[2:-2]], fontsize=7)
    ax.set_yticks(tick_positions[2:-2])
    ax.set_yticklabels([str(int(y)) for y in ticks[2:-2]], fontsize=7)
    cbar = plt.colorbar(ax.images[0])
    cbar.ax.tick_params(labelsize=7)
    cbar.ax.set_ylabel('error rate', fontsize=7)
    plt.tight_layout()


@cache_mpl_plot
def plot_sc_errors_over_relative_diatance():
    import torch
    import matplotlib.cm as cm
    import matplotlib.colors as mcolors

    # setup distance normalization
    n = torch.load('assets/data/sc_errors_by_position_error_counts_SD1.5.pt', weights_only=True).shape[0]
    distance_map = (((torch.arange(n).unsqueeze(1) - n//2)**2 + (torch.arange(n).unsqueeze(0) - n//2)**2).float())**.5
    distance_sorting = distance_map.flatten().argsort()
    x = np.array([i**2 for i in np.linspace(1, distance_map.max()**.5, 20)])
    x_counts = np.array([0] + [(distance_map.flatten() <= x[i]).to(torch.int).sum() for i in range(len(x))])

    # plot error rate over distance
    fig, ax = plt.subplots(figsize=(3,2))
    for model in ['SD1.5', 'SD2.1', 'SD-Turbo', 'SDXL', 'SDXL-Turbo']:
        errors = torch.load(f'assets/data/sc_errors_by_position_error_counts_{model}.pt', weights_only=True)
        counts = torch.load(f'assets/data/sc_errors_by_position_counts_{model}.pt', weights_only=True)
        rate = (errors / counts).flatten()[distance_sorting]
        y_rate = np.array([rate[x_counts[i]:x_counts[i+1]].nanmean() for i in range(len(x))])
        model_name = model.replace('SD', 'SD-') if '.' in model else model
        ax.plot(2*x*512/n, y_rate, label=model_name, alpha=0.7)

    # finish plot
    ax.legend(fontsize=7)
    ax.set_ylabel('error rate', fontsize=8)
    ax.set_xlabel('distance [px]', fontsize=8)
    ax.tick_params(labelsize=7)
    ax.set_ylim(0.38, 0.6)
    ax.set_xlim(0, 300)
    plt.tight_layout()
```

We also investigate the impact of the positional embedding on the semantic correspondence task. For this, we create a synthetic dataset, where different foreground objects, specifically animal images with annotated keypoints, e.g. eyes or paws, are placed at different locations on a white background image. For an example, see @fig-sc-errors-by-relative-position-1. The dataset consists of 11,623,140 semantic correspondence tasks on 2048 unique images (512\times512 px), which include horizontally flipped versions of all images, to reduce position biases from the dataset side. Using cosine similarity on the representations of all synthetic images, we predict the target keypoint locations and compare them to the annotated locations. From our previous experiments, we have the hypothesis that the positional embedding might sometimes overrule the semantic content in the representations and therefore negatively influence the semantic correspondence task. @fig-sc-errors-by-relative-position supports this hypothesis, because the error rate is visibly increased, when the predicted keypoint is close to the position of the source keypoint. This effect occurs for all models we tested, indicating that the positional embedding might be an inherent property of the representations of U-Net based diffusion models in general. More details for this experiment can be found in @sec-appendix-position-bias (@fig-appendix-sc-errors-by-relative-position-full).

Furthermore, we investigate the impact of this error case when using non-synthetic data, i.e. the \nameref{sec-datasets-spair} dataset. We find significantly increased error rates for very short distances between the predicted and target keypoint, similar to @fig-sc-errors-by-relative-position-3, for details see @fig-appendix-sc-errors-by-relative-position-spair-lines. While this occurs for most blocks and time steps, the effect is not as clearly visible as in the synthetic data. Particularly, while the error rate map in @fig-sc-errors-by-relative-position-2 shows a clear increase near (0,0), this is mostly not really visible in the error rate maps for \nameref{sec-datasets-spair}, for details see @fig-appendix-sc-errors-by-relative-position-spair-maps. We see two factors that might explain this: First, the spatial averaging used to create the error rate maps can obscure localized effects. Second, and more importantly, there appears to be a counteracting effect in real-world images - the error rates are actually lower in a broader region around the source keypoint. This lower error rate in the vicinity likely stems from the non-uniform distribution of keypoints and semantic content in natural images like those in \nameref{sec-datasets-spair}. There, related features tend to appear in similar regions, for example near the image center, rather than being randomly distributed as in our synthetic dataset.

This phenomenon hampers accurate quantification of the impact of the increased error rates due to the positional embedding for real world data. Furthermore, the behavior heavily depends on the block and on the dataset. For example, for semantic correspondence on \nameref{sec-datasets-spair}, using the `up[1]` representations of SD-1.5, we estimate an additional increase in total errors of about 0.027%, while we get an increase of 5.480% when using the `up[0]` representations.

<!-- percentage of increased errors are calculated with `sum((error_rate[i]-error_rate[lowest_error_idx])*total_counts[i] for i in range(lowest_error_idx)) / total_counts.sum()` -->
<!-- I.e. it is the increase at low distances (before minimum error rate) in @fig-appendix-sc-errors-by-relative-position-spair-lines for time step 50 -->
<!-- see `semantic_correspondence/analyze_sc_hyper_results_step_over_blocks.ipynb` -->


### Concluding Remarks {.unnumbered .unlisted}

The existence of position bias is to some degree surprising, as SD does not use spatial positional embeddings as part of the architectural design [@rombach2022highresolution;@CompVis2022StableDiffusionRepo]. The observation that the positional information is relative to the nearest border (see @fig-pos-embedding-aspect-ratios) gives us a clue about its origin. It is an indication that the position information may arise in the convolutional layers, which use 0-padding, and therefore can easily detect the image boundaries.

In conclusion, our experiments establish the existence of positional bias in the representations of SD models. The positional information is linearly extractable and primarily present in the lower blocks of the U-Net (see @fig-position-classifier-accuracy). It affects both dense correspondence (see @fig-dense-correspondence-flip) and semantic correspondence tasks (see @fig-sc-errors-by-relative-position), however, the impact is hard to quantify for real world data and is limited to specific blocks. Therefore, whether this positional bias is relevant or negligible depends on the specific setup, block, and dataset.


## Texture and Color Bias {#sec-texture-color-bias}

```{python}
#| label: fig-texture-color-bias-examples
#| fig-cap: Examples of texture and color bias in the dense correspondence task in selected representations (for all see @fig-appendix-texture-color-bias-examples-1). For each representation token from the respective block for the target image, the most similar representation token from the source is selected using cosine similarity. This mapping is visualized by transferring the image pixels of the source image (columns 1, 3, 5) and pixels of the colorwheel (columns 2, 4, 6) from their source location to the location defined by the mapping (just as in @fig-dense-correspondence-flip).

# Thoughts
# - this is only qualitative, not quantitative
# - are the selected examples representative, or are they too cherry-picked? They are more or less the first ones I tried.
# - are this too many examples?

# relevant notebook: semantic_correspondence/artifical_dataset/show_texture_sc_fails.ipynb

def plot_texture_color_bias_examples_general(poss: list[str]):
    from sdhelper import SD
    import numpy as np
    from pathlib import Path
    import matplotlib.lines as mlines

    sd = SD(disable_progress_bar=True)

    n=512
    seed=42
    step=50
    base_img_path = Path('assets/images')
    img_paths = [
        ('cat_next_to_house.png', 'cat_going_left.png'),
        ('cat_next_to_house.png', 'dog_in_snow.jpg'),
        ('cat_on_roof.jpg', 'dog_in_snow.jpg'),
    ]

    num_imgs = len(img_paths)
    imgs1, imgs2 = zip(*[[Image.open(base_img_path / img_path).resize((n, n)) for img_path in img_paths] for img_paths in img_paths])
    reprs1 = [[sd.img2repr(img1, [pos], step=step, seed=seed) for pos in poss] for img1 in imgs1]
    reprs2 = [[sd.img2repr(img2, [pos], step=step, seed=seed) for pos in poss] for img2 in imgs2]

    # setup colorwheel
    colorwheel = np.zeros((n, n, 3), dtype=np.uint8)
    offset = n/2 + .5
    for i in range(n):
        for j in range(n):
            angle = np.arctan2(i-offset, j-offset)
            dist = 1 - np.sqrt((i-offset)**2 + (j-offset)**2) / offset / np.sqrt(2)
            colorwheel[i, j, :] = np.array([.5+.5*np.sin(angle), .5+.5*np.sin(angle+np.pi/2), dist]) * 255

    # setup figure
    fig = plt.figure(figsize=((num_imgs*2)*2+2, (len(poss)+2)*2+1))
    outer_gs = fig.add_gridspec(1, num_imgs, wspace=0.15)
    gss = [outer_gs[i].subgridspec(len(poss)+2, 2, wspace=0.05, hspace=0.05) for i in range(num_imgs)]
    axs = np.array([[[fig.add_subplot(gss[i][j, k]) for k in range(2)] for j in range(len(poss)+2)] for i in range(num_imgs)])

    # plot source images
    for i, img in enumerate(imgs1):
        axs[i, 0, 0].text(1.15, 1.3, f'example {i+1}', va='bottom', ha='center', transform=axs[i, 0, 0].transAxes, fontsize=14)
        axs[i, 0, 0].imshow(img)
        axs[i, 0, 0].axis('off')
        axs[i, 0, 0].text(0.5, 1.05, 'image', va='bottom', ha='center', transform=axs[i, 0, 0].transAxes, fontsize=12)
        axs[i, 0, 1].imshow(colorwheel)
        axs[i, 0, 1].axis('off')
        axs[i, 0, 1].text(0.5, 1.05, 'colorwheel', va='bottom', ha='center', transform=axs[i, 0, 1].transAxes, fontsize=12)

    # plot target images
    for i, img in enumerate(imgs2):
        axs[i, 1, 0].imshow(img)
        axs[i, 1, 0].axis('off')
        # axs[i, 1, 1].imshow(colorwheel)
        axs[i, 1, 1].axis('off')

    # plot transferred images
    for i, (img1, img2) in enumerate(zip(imgs1, imgs2)):
        for j, pos in enumerate(poss):
            img = np.array(img1)
            similarities = reprs1[i][j].cosine_similarity(reprs2[i][j])
            transferred_img = np.zeros_like(img)
            transferred_colorwheel = np.zeros_like(img)
            m = similarities.shape[0]
            s = n // m
            for k in range(m):
                for l in range(m):
                    argmax = similarities[:,:,k, l].flatten().argmax()
                    k_, l_ = argmax // m, argmax % m
                    transferred_img[k*s:(k+1)*s, l*s:(l+1)*s] = img[k_*s:(k_+1)*s, l_*s:(l_+1)*s][:, ::-1]  # use ::-1 to flip, and ::1 to not flip
                    transferred_colorwheel[k*s:(k+1)*s, l*s:(l+1)*s] = colorwheel[k_*s:(k_+1)*s, l_*s:(l_+1)*s][:, ::-1]

            axs[i, j+2, 0].imshow(transferred_img)
            axs[i, j+2, 0].axis('off')
            axs[i, j+2, 1].imshow(transferred_colorwheel)
            axs[i, j+2, 1].axis('off')

    # add titles
    position_names = [pos.replace('_blocks', '').replace('_block', '').replace('_', '-') for pos in poss]
    for i, pos in enumerate(['image 1\n(source)', 'image 2\n(target)'] + position_names):
        axs[0, i, 0].text(-0.1, 0.5, pos, va='center', ha='right', transform=axs[0, i, 0].transAxes, fontsize=12)
    axs[0, 2+len(poss)//2, 0].text(-0.6, 0.5 if len(poss)%2==1 else 1.1, 'blocks', va='center', ha='right', transform=axs[0, 2+len(poss)//2, 0].transAxes, rotation=90, fontsize=14)

    plt.tight_layout()

    # add horizontal line below the 2nd row
    fig.add_artist(mlines.Line2D([0.07,0.9], [axs[0, 1, 0].get_position().y0-0.025/(len(poss)+2.5)]*2, color='gray', linestyle='--', linewidth=1, transform=fig.transFigure))


@cache_mpl_plot(watch=[plot_texture_color_bias_examples_general])
def plot_texture_color_bias_examples_1():
    plot_texture_color_bias_examples_general(['conv_in', 'up_blocks[0]', 'up_blocks[1]', 'up_blocks[2]'])
```

Texture and color bias refers to the tendency of prioritizing texture and color over semantic information. As described before, the extracted representations of the SD U-Net carry such semantic information, however, this does not apply to all blocks in the same way. The upper blocks (`conv-in`, `down[0]`, `down[1]`, `up[2]`, `up[3]`) are biased towards low-level surface properties in the image, such as texture and color, while the lower blocks (`down[2]`, `down[3]`, `mid`, `up[0]`, `up[1]`) focus more on the abstract or semantic meaning [@zhang2023tale;@tang2023emergent]. The transition between the abstraction levels is gradual, i.e. there is no sharp separation between upper and lower blocks in terms of texture and color bias.


### Dense Correspondence {.unnumbered .unlisted}

@fig-texture-color-bias-examples shows some examples of this effect using the dense correspondence task. It visualizes the mapping defined by matching each representation token in the target image to the representation token with the highest cosine similarity in the source image. The pixels from the source image and colorwheel are then transferred accordingly to the target image. Among the blocks visualized in @fig-texture-color-bias-examples, the `conv-in` block shows the strongest texture or color bias, as can be seen by the similar looks of the mapped image to the target image, while the colorwheel shows that the mapping does not well correspond to any semantic meaning. An example for this are the wall being mapped to represent the light regions of the cat (example 1), and the sky regions being mapped to represent the snow (example 2). For `up[0]`, in contrast, the mapping corresponds more to the semantic meaning. For example, the head regions of the source image are used to represent the head regions of the target image (all examples), and similar for e.g. feet and ground, even though color and texture might be different. Going further along the up blocks, the mapping of `up[1]` introduces some texture and color bias again, while mostly preserving the semantics. The mapping of `up[2]` is even more texture and color biased, but still more semantically meaningful as e.g. the `conv-in` mapping.

```{python}
#| label: fig-color-bias-rgb-bgr-dc
#| fig-cap: Dense correspondence results on the \nameref{sec-datasets-imagenet-subset}, see (b), when permuting the RGB channels of the image from RGB to BGR, as visualized in (a). The ideal mapping (100% accuracy) would be if for each token in the target image, the token with the highest cosine similarity in the source image is at the same spatial location.
#| fig-subcap:
#| - Example of the gradual color channel permutation
#| - Accuracy, and change relative to the initial accuracy, for dense correspondence

# Thoughts
# - maybe this not enough change to be interesting
# - should the color channel permutation be different e.g. RGB -> GBR?

def plot_color_texture_bias_dc_example_general(degradation_fn):
    image = Image.open('assets/images/cat_next_to_house.png')
    fig, axs = plt.subplots(1, 5, figsize=(5*1.0, 1.0))
    for i, x in enumerate(np.linspace(0, 1, 5)):
        arr = np.array(image)
        axs[i].imshow(degradation_fn(arr, x).astype(np.uint8))
        axs[i].set_title(f'{x:.2f}', fontsize=7)
        axs[i].axis('off')


def plot_color_texture_bias_dc_accuracy_general(path: str):
    accuracies = np.load(path)
    block_names = [b.replace('_blocks', '').replace('_block', '').replace('_', '-') for b in sd15_all_blocks.keys()][:-1]
    n, m, _ = accuracies.shape

    fig, axs = plt.subplots(1, 2, figsize=(8, 2.3))
    colors = plt.cm.rainbow(np.linspace(0, 1, n))
    for block_idx, (block, color) in enumerate(zip(block_names, colors)):
        axs[0].plot(np.linspace(0, 1, m), accuracies[block_idx].mean(axis=1)*100, label=block, color=color)
    axs[0].set_xlabel('interpolation step', fontsize=9)
    axs[0].set_ylabel('accuracy [%]', fontsize=9)
    axs[0].set_xticks(np.linspace(0, 1, 5))
    axs[0].tick_params(labelsize=8)
    axs[0].grid(alpha=0.3)

    for block_idx, (block, color) in enumerate(zip(block_names, colors)):
        init_acc = accuracies[block_idx, 0].mean()
        acc_change = (accuracies[block_idx].mean(axis=1) - init_acc) / init_acc
        axs[1].plot(np.linspace(0, 1, m), acc_change*100, label=block, color=color)
    axs[1].set_xlabel('interpolation step', fontsize=9)
    axs[1].set_ylabel('rel. change in accuracy [%]', fontsize=9)
    axs[1].plot([], [], color='white', alpha=0, label=''.join(char + '\u0336' for char in 'conv-out'))  # conv-out legend entry with strikethrough
    legend = axs[1].legend(bbox_to_anchor=(1.01, 1.05), loc='upper left', fontsize=9)
    legend.get_texts()[-1].set_color('gray')  # gray out conv-out legend entry
    axs[1].set_xticks(np.linspace(0, 1, 5))
    axs[1].tick_params(labelsize=8)
    axs[1].grid(alpha=0.3)

    plt.tight_layout()


@cache_mpl_plot(watch=[plot_color_texture_bias_dc_example_general])
def plot_color_bias_rgb_bgr_example():
    plot_color_texture_bias_dc_example_general(lambda arr, x: x * arr[:,:,::-1] + (1-x) * arr)


@cache_mpl_plot(watch=[plot_color_texture_bias_dc_accuracy_general])
def plot_color_bias_rgb_bgr_dense_correspondence():
    plot_color_texture_bias_dc_accuracy_general('assets/cached_data/color_bias_dense_correspondence_rgb_bgr_accuracies.npy')
```

We hypothesize that the texture and color bias might negatively impact downstream tasks like semantic and dense correspondence, as the model might incorrectly prioritize low-level perceptual similarities over semantic ones. 

To quantify the color bias, we investigate the impact of degradations in color space by evaluating the dense correspondence accuracy of the task of matching the tokens between an image and the same image with permuted color channels, see @fig-color-bias-rgb-bgr-dc. We measure the accuracy $\text{acc}_i$ for 10 interpolation steps $i\in[0,1]$, and the relative change in accuracy $\Delta\text{acc}_{i,\text{rel}}=\frac{\text{acc}_i-\text{acc}_0}{\text{acc}_0}$, when permuting the RGB channels of the image from RGB to BGR. The results are averaged over the 500 images of the \nameref{sec-datasets-imagenet-subset}. When looking at the relative change in accuracy, we find, as expected, that the performance of the upper blocks drops with increased color degradation, especially `conv-in`, and `up[3]`, but also `down[0]`, and `up[2]`. At the same time, the lower blocks do not show any significant change in accuracy. This shows that the upper blocks are easily confused by color changes. At interpolation step 0, one might expect perfect accuracy for all blocks, as the source and target images are exactly the same. However, as we use different noise seeds for source and target image, the dense correspondence accuracy might not be perfect. This setup is more relevant, as, in general, one can not expect the noise pattern to be related between the matching regions in real world dense correspondence tasks.

```{python}
#| label: fig-texture-bias-texture-overlay-dc
#| fig-cap: Dense correspondence results on the \nameref{sec-datasets-imagenet-subset}, see (b), when overlaying a texture on the image, e.g. grass, as visualized in (a). The ideal mapping (100% accuracy) would be if for each token in the target image, the token with the highest cosine similarity in the source image is at the same spatial location.
#| fig-subcap:
#| - Example of the gradual addition of the texture overlay
#| - Accuracy, and change relative to the initial accuracy, for dense correspondence

# Thoughts
# - is this texture representative?
# - is the method of overlaying the texture a good way to evaluate texture bias?


@cache_mpl_plot(watch=[plot_color_texture_bias_dc_example_general])
def plot_texture_bias_texture_overlay_example():
    import numpy as np
    grass_arr = np.array(Image.open('assets/images/grass.png').convert('RGB'))
    plot_color_texture_bias_dc_example_general(lambda arr, x: x/2 * grass_arr + (1-x/2) * arr)


@cache_mpl_plot(watch=[plot_color_texture_bias_dc_accuracy_general])
def plot_texture_bias_texture_overlay_dense_correspondence():
    plot_color_texture_bias_dc_accuracy_general('assets/cached_data/texture_bias_dense_correspondence_texture_overlay_accuracies.npy')
```

In @fig-texture-bias-texture-overlay-dc, we evaluate the same setup as in @fig-color-bias-rgb-bgr-dc, but instead of permuting the color channels, we overlay a texture on the image. This aims to change the texture perceived by the model, thus allowing to evaluate texture bias. We find that in this case, all blocks are somewhat impacted, but that the upper blocks are again more affected. We also evaluate this for other texture overlays, blurring, and noise, and find similar results for all these cases. More details can be found in @sec-appendix-texture-color-bias (@fig-appendix-texture-bias-dense-correspondence-blur and @fig-appendix-texture-bias-dense-correspondence-noise).

However, while these results show the impact of color and texture on the upper blocks, they alone do not show whether the lower blocks are actually more resistant to such changes. The reason is that in this dense correspondence task, the relatively good performance of the lower blocks could also be due to increased reliance on positional embedding, which we showed to be more pronounced in these blocks, see @sec-position-bias.


### Semantic Correspondence {.unnumbered .unlisted}

```{python}
#| label: fig-texture-color-bias-sc
#| fig-cap: Semantic correspondence percentage of correct keypoints (PCK) results on the \nameref{sec-datasets-spair} dataset, when permuting the RGB channels of the image (upper plots) and when overlaying a texture on the image (lower plots). The permutation of the RGB channels from RGB to BGR is the same as in @fig-color-bias-rgb-bgr-dc, and the overlay of a grass texture is the same as in @fig-texture-bias-texture-overlay-dc.

@cache_mpl_plot
def plot_color_texture_bias_sc_pck_general():
    import numpy as np

    pcks_color_permutation = np.load('assets/data/sc_pck_spair_color_bias_rgb_to_gbr_colors_on_trg_SD1.5.npy')
    pcks_texture_overlay = np.load('assets/data/sc_pck_spair_texture_bias_0_to_.5_grass_on_trg_SD1.5.npy')
    # pcks_texture_blur = np.load('assets/data/sc_pck_spair_texture_blur_on_trg_SD1.5.npy')
    assert pcks_color_permutation.shape == pcks_texture_overlay.shape
    block_names = [b.replace('_blocks', '').replace('_block', '').replace('_', '-') for b in sd15_all_blocks.keys()][:-1]
    n, m = pcks_color_permutation.shape

    fig, axs = plt.subplots(2, 2, figsize=(9, 3.8))
    for i, pcks in enumerate([pcks_color_permutation, pcks_texture_overlay]):
        colors = plt.cm.rainbow(np.linspace(0, 1, n))
        for block_idx, (block, color) in enumerate(zip(block_names, colors)):
            axs[i, 0].plot(np.linspace(0, 1, m), pcks[block_idx]*100, label=block, color=color)
        if i == 1: axs[i, 0].set_xlabel('interpolation step')
        else: axs[i, 0].set_xticklabels([])
        axs[i, 0].set_xticks(np.linspace(0, 1, 5))
        axs[i, 0].grid(alpha=0.3)
        axs[i, 0].set_ylim(0)

        for block_idx, (block, color) in enumerate(zip(block_names, colors)):
            init_acc = pcks[block_idx, 0]
            acc_change = (pcks[block_idx] - init_acc) / init_acc
            axs[i, 1].plot(np.linspace(0, 1, m), acc_change*100, label=block, color=color)
        if i == 1: axs[i, 1].set_xlabel('interpolation step')
        else: axs[i, 1].set_xticklabels([])
        axs[i, 1].set_xticks(np.linspace(0, 1, 5))
        axs[i, 1].grid(alpha=0.3)

    axs[0, 1].plot([], [], color='white', alpha=0, label=''.join(char + '\u0336' for char in 'conv-out'))  # conv-out legend entry with strikethrough
    legend = axs[0, 1].legend(bbox_to_anchor=(1.01, -0.025), loc='center left', fontsize=10)
    legend.get_texts()[-1].set_color('gray')  # gray out conv-out legend entry
    axs[0, 0].text(-0.15, 0.5, 'permutation of\nRGB channels', va='center', ha='right', transform=axs[0, 0].transAxes, fontsize=10)
    axs[1, 0].text(-0.15, 0.5, 'overlay of\ngrass texture', va='center', ha='right', transform=axs[1, 0].transAxes, fontsize=10)
    plt.tight_layout()
    axs[0, 0].text(-0.15, -0.025, 'PCK@0.1$_\\text{bbox}$', va='center', ha='right', transform=axs[0, 0].transAxes, fontsize=10, rotation=90)
    axs[0, 1].text(-0.15, -0.025, 'rel. change in PCK@0.1$_\\text{bbox}$', va='center', ha='right', transform=axs[0, 1].transAxes, fontsize=10, rotation=90)
    plt.subplots_adjust(wspace=0.28, hspace=0.05)
```

To further investigate the texture and color bias and answer the question about the reason for the better performance of the lower blocks, we evaluate the semantic correspondence performance on \nameref{sec-datasets-spair} while applying the same color and texture changes as above. The results are shown in @fig-texture-color-bias-sc. As discussed in @sec-semantic-correspondence, the semantic correspondence performance of many of the blocks is not much better than the trivial baseline for \nameref{sec-datasets-spair} (which is at around 5.5 PCK, see @sec-datasets). This means interpretation of these results is difficult for all blocks, except for `up[0]`, `up[1]`, and `up[2]`, where the initial performance is relatively high. For the permutation of the RGB channels, all blocks are only slightly impacted. While this does not mean much for the bad performing blocks, it shows that the three better performing blocks are quite robust against color channel permutation. The same goes for the texture overlay results, where the performance of the three blocks significantly decreases, but still remains significantly above the trivial baseline. With this, we can rule out the position bias as the reason for the previous good performance of the lower blocks in the dense correspondence task, because the positional embedding is not enough for good performance in the semantic correspondence task. Additionally, we observe that among the three better performing blocks, the relative decrease in performance increases with being higher in the U-Net. This aligns well the general observation of color and texture bias increasing towards the upper blocks.


### Concluding Remarks {.unnumbered .unlisted}

Neural style transfer might seem like another good path forward for investigating texture bias. However, we find that popular style transfer methods [@fofr2024styletransfer] also sometimes impact the semantic meaning of image regions or objects. Therefore, it is unsuitable for the semantic correspondence task, especially on the \nameref{sec-datasets-spair} dataset, where keypoints often depend on fine details. However, using other style transfer methods or with extensive manual hyperparameter tuning, neural style transfer might offer an interesting challenge for further investigating texture and color bias of SD representations.

In conclusion, we qualitatively find texture and color bias in the upper blocks of the SD U-Net and quantitatively show that it impacts the dense correspondence task. The evaluation on the semantic correspondence task is restricted by the unsuitability of many representations for semantic correspondence on the \nameref{sec-datasets-spair} dataset, but we show that lower blocks tend to be more resistant to texture and especially color changes.


## Anomalies {#sec-anomalies}

When investigating the similarities of representations in the representation similarity explorer (see @sec-repr-sim-explorer), we observe different unexpected patterns. Some of these patterns are localized to only one or a few tokens and only appear in the representations of certain blocks or at certain spatial positions. They do not occur in all images, and their spatial position differs depending on the image. These anomalies can be separated into different categories:

* **corner and border anomalies**: border and corner tokens with non-semantic similarities
* **high-norm anomalies**: localized groups of tokens with high norm and non-semantic similarities


### Corner and Border Anomalies {#sec-anomalies-corner}

```{python}
#| label: fig-cosine-similarity-corner
#| fig-cap: Cosine similarities between `mid` block representations of three images and the upper left corner token in the left example.
#| wrapfigure: R 0.5


@cache_mpl_plot()
def plot_cosine_similarity_corner():
    from sdhelper import SD

    blocks = ['mid_block']
    img_paths = [
        'assets/images/dog_in_snow.jpg',
        'assets/images/cat_next_to_house.png',
        'assets/images/whale.jpg',
    ]

    sd = SD(disable_progress_bar=True)
    representations = sd.img2repr(img_paths, extract_positions=blocks, step=50, seed=42, batch_size=1, resize=512)

    fig, axs = plt.subplots(2, len(img_paths), figsize=(len(img_paths)*1.0+0.7, 2*1.0))
    for i, img_path in enumerate(img_paths):
        img = plt.imread(img_path)
        axs[0, i].imshow(img)
        axs[0, i].axis('off')

        # axs[1, i].imshow(representations[i].norm(dim=0))
        axs[1, i].imshow(representations[0].cosine_similarity(representations[i])[0,0,:,:])
        axs[1, i].axis('off')

    axs[0, 0].text(-0.15, 0.5, 'Images', va='center', ha='right', transform=axs[0, 0].transAxes, fontsize=8)
    axs[1, 0].text(-0.15, 0.5, 'Cosine\nSimilarities', va='center', ha='right', transform=axs[1, 0].transAxes, fontsize=8)
    plt.tight_layout()
```

One anomaly that we observe in the similarity maps is that the borders, and especially the corners, in most of the representations of the different blocks behave differently and show similarities that do not correspond to the semantic content of the image.
@fig-cosine-similarity-corner shows an example of such increased cosine similarities between corners and borders. Here, many of the corners and borders have an increased similarity towards the upper left corner of the first representation. These similarities between corners and borders sometimes correspond to semantic similarities in the images, but often do not.

::: {#fig-border-corner-similarity layout="[60,-5,40]"}

```{python}
#| label: fig-border-corner-similarity-over-blocks
#| fig-cap: Average relative cosine similarities in the token groups containing the corners, the borders, or the other tokens. A value above 1 indicates increased similarities among a token group, when the corners/borders are already in the corners/borders during representation extraction.

def plot_border_corner_similarity_general(model_name: str, limit_images: int = 500):
    from datasets import load_dataset
    from sdhelper import SD
    import torch
    import torch.nn.functional as F

    # load data and model
    data = load_dataset("jonasloos/imagenet_subset", split="train")
    images = [d['image'] for d in data][:limit_images]
    sd = SD(model_name, disable_progress_bar=True)

    # setup slices to select groups of tokens
    corner_slices = [(a,b) for a in [0,-1] for b in [0,-1]]
    border_slices = [(slice(1,-1), 0), (slice(1,-1), -1), (0, slice(1,-1)), (-1, slice(1,-1))]
    other_slices  = [(slice(1,-1), slice(1,-1))]
    slices = [
        ('corner', corner_slices),
        ('border', border_slices),
        ('other', other_slices),
    ]

    # helper function to calculate similarities between the tokens in a group
    def calc_group_similarities(block, images):
        representations = sd.img2repr(images, extract_positions=[block], step=50)
        w, h = images[0].size
        token_size = w // representations[0][block].shape[-1]
        representations_cropped = sd.img2repr([img.crop((token_size, token_size, w-token_size, h-token_size)) for img in images], extract_positions=[block], step=50)
        similarity_maps = torch.stack([x.cosine_similarity(x) for x in representations])
        similarity_maps_cropped = torch.stack([x.cosine_similarity(x) for x in representations_cropped])
        similarity_maps_repr_cropped = F.pad(similarity_maps, [-1]*8, value=torch.nan)

        results = []
        for sim_maps in [similarity_maps_cropped, similarity_maps_repr_cropped]:
            n = similarity_maps.shape[1]
            m = len(slices)
            result = torch.zeros((m, m))
            for i, (_, slices1) in enumerate(slices):
                for j, (_, slices2) in enumerate(slices):
                    count = torch.stack([torch.ones((n,n,n,n))[s11,s12,s21,s22].sum() for s11, s12 in slices1 for s21, s22 in slices2]).sum() * len(sim_maps)
                    self_similarities = torch.stack([torch.ones((n,n))[s11,s12].sum() for s11, s12 in slices1]).sum() * len(sim_maps) if i == j else 0
                    result[i,j] = (torch.stack([sim_maps[:,s11,s12,s21,s22].sum() for s11, s12 in slices1 for s21, s22 in slices2]).sum() - self_similarities) / (count - self_similarities)
            results.append(result)
        return results

    # calculate similarities
    results = {}
    for block in sd.available_extract_positions[:-1]:
        results[block] = calc_group_similarities(block, images)

    # plot results
    xs = range(len(results))
    for i, group_name in enumerate(['corner', 'border', 'other']):
        plt.plot(xs, [x[0][i,i]/x[1][i,i] for x in results.values()], label=group_name)

    # finish up plot
    plt.plot(xs, [1]*len(xs), linestyle='--', color='gray')
    plt.ylabel('Relative Similarity')
    plt.yscale('log')
    plt.yticks([0.5, 1, 2], ['0.5', '1', '2'])
    plt.gca().yaxis.set_minor_formatter(plt.NullFormatter())
    block_names = [x.replace('_blocks', '').replace('_block', '').replace('_', '-') for x in results.keys()]
    plt.xticks(ticks=xs, labels=block_names, rotation=90)
    plt.title('Similarity between groups of tokens')
    plt.legend()
    plt.tight_layout()

@cache_mpl_plot(watch=[plot_border_corner_similarity_general])
def plot_border_corner_similarity_general_sd15():
    plot_border_corner_similarity_general('sd15', 50)
```

![Schematic visualization of the experiment.](assets/figures/corner_similarities_method.jpg){#fig-corner-similarities-method}

Visualization of increased corner similarities. (a) shows the similarities using cropped images (see (b), left path) relative to those where the representations are cropped instead (see (b), right path).

:::

A naive way of quantifying these observations would be to compare the mean similarity among the groups of tokens in the corners or border, and compare them to the mean similarity among the other tokens. This, however, does not take into account the non-independent nature of representation tokens. Natural images tend to have certain characteristics that influence the similarity values, such as foreground objects often being located in the center of an image. To remedy this, we compare the similarities of the representations at the same position in the image, but once where the image is cropped so that the token is the corner/border during representation extraction and once where it is not. The amount of pixels cropped is based on the corresponding size of one token, which differs depending on the block used for representation extraction. The similarities are calculated per image and averaged over the 500 samples in the \nameref{sec-datasets-imagenet-subset}. 
The results are shown in @fig-border-corner-similarity show that borders have an relatively average cosine similarity between each other. The corners, however, show a significant increase in similarity between each other for the lower blocks (`down[2]`, `down[3]`, `mid`, `up[0]`). Interestingly, for `conv-in`, `up[2]`, and `up[3]` the similarity among the corners in decreased instead. In the representation similarity explorer (see @sec-repr-sim-explorer), we observe that the corner similarities are more spread out in the `up[1]`, `up[2]`, and `up[3]` representations, which might be an explanation for why the similarity is not increased there.

We also check the similarity of corners and borders for SD-2.1, SD-Turbo, SDXL, and SDXL-Turbo (see @sec-appendix-corner-anomalies). For SD-2.1 and SD-Turbo, the corner similarities drop sharply in the `up[0]` representations. For SDXL and SDXL-Turbo, the corner similarity increases only after the `down[1]` block. But despite these slight differences, the overall pattern is consistent across all tested models. This indicates that corner anomalies might be inherent to the used U-Net architecture, and not tied to a specific layer count, model size, training data, and training procedure.

Two hypotheses for the cause of these corner anomalies are that they are caused by the padding in the convolutions of the ResNet-layers, or that the models might consider them as less important regions, where non-semantic information can be "stored", similar to the findings of @darcet2024vision, where such behavior is observed in vision transformers. The impact of border anomalies on downstream tasks is further discussed in @sec-anomalies-impact.


### High-norm Anomalies {#sec-anomalies-high-norm}

<!-- Could do: add paragraph headings -->

While investigating the norms of the representation tokens, we notice two kinds of anomalies in the representations of different blocks. The two distinct types of anomalies are localized groups of neighboring tokens with highly increased norms. One type occurs in the `conv-in`, `down[0]`, and `up[3]` blocks, while the other type occurs in the `up[1]` and `up[2]` blocks.

```{python}
def plot_high_norm_anomalies(img_path, reprs, steps):
    import torch

    rows = sum(len(r[0].data) for r in reprs.values())
    cols = len(steps) + 1
    row_labels = []

    # collect norms
    norms = [[None]*len(steps) for _ in range(rows)]
    for model_idx, (model, reprs_model) in enumerate(reprs.items()):
        for col, (step, reprs_step) in enumerate(zip(steps, reprs_model)):
            row = sum([len(r[0].data) for r in reprs.values()][:model_idx])
            for block, repr in reprs_step.data.items():
                block = block.replace('_blocks', '')
                if col == 0: row_labels.append(f'{model}\n{block}')
                norms[row][col] = repr.squeeze(0).norm(dim=0).cpu().numpy()
                row += 1
    min_norms = [min(norms[i][j].min() for j in range(len(steps))) for i in range(rows)]
    max_norms = [max(norms[i][j].max() for j in range(len(steps))) for i in range(rows)]

    # plot images
    fig, axs = plt.subplots(rows, cols+1, figsize=(cols*0.7+1.0, rows*0.7+0.2), width_ratios=[1]*cols + [0.05])
    for row, row_label in enumerate(row_labels):
        axs[row, 0].imshow(Image.open(img_path))
        axs[row, 0].axis('off')
        axs[row, 0].text(-0.1, 0.5, row_label, verticalalignment='center', horizontalalignment='right', transform=axs[row, 0].transAxes, fontsize=8)
        if row == 0: axs[row, 0].set_title('image', fontsize=8)
        for col, step in enumerate(steps, 1):
            im = axs[row, col].imshow(norms[row][col-1], vmin=min_norms[row], vmax=max_norms[row], aspect=1)
            axs[row, col].axis('off')
            if row == 0: axs[row, col].set_title(f'$t = {step}$', fontsize=8)

        # colorbar
        cbar = fig.colorbar(im, cax=axs[row, -1], orientation='vertical')
        cbar.set_label('Norm', labelpad=9, fontsize=7)
        cbar.ax.tick_params(labelsize=7)
        axs[row, -1].yaxis.set_ticks_position('right')
        axs[row, -1].yaxis.set_label_position('right')

    plt.tight_layout()
    fig.subplots_adjust(wspace=0.05, hspace=0.05)  # reduce space between subplots
```

```{python}
#| label: fig-high-norm-anomalies-1
#| fig-cap: Example of the L2 norms of representation tokens for different time steps $t$ of SD-1.5 and SD-2.1 where a high-norm anomaly occurs in the `conv-in`, `down[0]`, and `up[3]` blocks.
#| wrapfigure: R 0.6

@cache_mpl_plot(watch=[plot_high_norm_anomalies])
def plot_high_norm_anomalies1():
    from sdhelper import SD
    img_path = 'assets/images/whale.jpg'
    blocks = ['conv_in', 'down_blocks[0]', 'up_blocks[3]']
    steps = [0, 50, 200]
    reprs = {}
    sd = SD('sd15', disable_progress_bar=True)
    reprs['SD-1.5'] = [sd.img2repr(img_path, blocks, step=step, seed=42) for step in steps]
    sd = SD('sd21', disable_progress_bar=True)
    reprs['SD-2.1'] = [sd.img2repr(img_path, ['down_blocks[0]'], step=step, seed=42) for step in steps]
    plot_high_norm_anomalies(img_path, reprs, steps)
```

The first kind of anomalies with a high norm occurs primarily in the representations of the `conv-in`, `down[0]`, and `up[3]` blocks of SD-1.5. For SD-2.1, we additionally observe them in `down[1]` and `conv-out`. As they are most pronounced in the `conv-in` representations, we call them `conv-in` anomalies. They consist of a group of roughly 1-9 neighboring tokens with considerably higher L2 norm than the rest of the tokens. The spatial position of these anomalies is robust over the different blocks and over a wide range of noise levels. Interestingly, these anomalies sometimes even occur at exactly the same spatial position in SD-1.5 and SD-2.1 for the same image. This is particularly surprising, as SD-1.5 and SD-2.1, while they share the same U-Net architecture, do not share any pretraining history. In @fig-high-norm-anomalies-1, one such anomaly is visualized for different noise levels, blocks, and models.

```{python}
#| label: fig-high-norm-anomalies-2
#| fig-cap: Example of the L2 norms of representation tokens for different time steps $t$ of SD-1.5 and SD-2.1 where high-norm anomalies occur in the `up[1]` and `up[2]` blocks.

@cache_mpl_plot(watch=[plot_high_norm_anomalies])
def plot_high_norm_anomalies2():
    from sdhelper import SD

    img = 'assets/images/dark_sky.jpg'
    blocks = ['up_blocks[1]', 'up_blocks[2]']
    steps = [0, 20, 80, 250, 600]
    reprs = {}
    sd = SD('sd15', disable_progress_bar=True)
    reprs['SD-1.5'] = [sd.img2repr(img, blocks, step=step, seed=42) for step in steps]
    sd = SD('sd21', disable_progress_bar=True)
    reprs['SD-2.1'] = [sd.img2repr(img, blocks, step=step, seed=42) for step in steps]
    plot_high_norm_anomalies(img, reprs, steps)
```

The second kind of high-norm anomalies occur in the `up[1]` and `up[2]` representations of SD-1.5 and SD-2.1. Their occurence and spatial position does not seem to be connected to the `conv-in` anomalies and is also not consistent over different models. They tend to disappear for very low noise levels and are not always stable over different noise levels and noise seeds. See @fig-high-norm-anomalies-2 for an example over different noise levels, blocks, and models. These `up[1]` anomalies typically occur in the shape of 2\times2 patches in the `up[1]` representations and only in every second row and column. These two properties are likely related to the spatial upscaling of the representations at the end of the up-blocks.

```{python}
#| label: fig-high-norm-anomalies-searching
#| fig-cap: True positives, false positives, and false negatives over different thresholds when searching for high-norm anomalies in the \nameref{sec-datasets-nyu-v2} dataset using the cosine similarity to the mean `up[1]` anomaly token from the \nameref{sec-datasets-imagenet-subset}.
#| wrapfigure: R 0.5

# relevant notebooks: data_labeler/analyze_imagenet_subset_high_norms.ipynb and representation_exploration/anomaly_similarities.ipynb

@cache_mpl_plot
def plot_high_norm_anomalies_searching():
    import numpy as np
    from sdhelper import SD
    import torch
    from datasets import load_dataset
    import torch
    from torch.nn import functional as F

    # load data
    imagenet_subset = load_dataset("JonasLoos/imagenet_subset", split="train")
    is_up1_anomalies = np.load("assets/data/high_norm_anomalies_imagenet_subset_step50_seed42_up1.npy")
    nyuv2 = load_dataset("0jl/NYUv2", split="train")
    nyu_up1_anomalies = np.load("assets/data/high_norm_anomalies_nyuv2_step50_seed42_up1.npy")

    # load model
    sd = SD(disable_progress_bar=True)

    # extract representations
    is_reprs = sd.img2repr([x['image'] for x in imagenet_subset], extract_positions=['up_blocks[1]'], step=50, seed=42)
    nyu_reprs = sd.img2repr([x['image'] for x in nyuv2], extract_positions=['up_blocks[1]'], step=50, seed=42)

    # save (v)ram
    del imagenet_subset, nyuv2, sd
    torch.cuda.empty_cache()

    # imagenet subset - train/test split
    is_reprs_torch = torch.stack([x.concat().permute(1,2,0) for x in is_reprs])
    is_anomalies_all = np.concatenate([is_up1_anomalies + [[0,i,j]] for i in [0,1] for j in [0,1]])[:,[0,2,1]]  # use the full 2x2 anomaly patches
    is_reprs_anomalies = torch.stack([is_reprs_torch[i,j,k,:] for i,j,k in is_anomalies_all])
    is_prototype_anomaly = is_reprs_anomalies.mean(dim=0)

    # nyu - train/test split
    nyu_reprs_torch = torch.stack([x.concat().permute(1,2,0) for x in nyu_reprs])
    nyu_anomalies_all = np.concatenate([nyu_up1_anomalies + [[0,i,j]] for i in [0,1] for j in [0,1]])[:,[0,2,1]]
    nyu_reprs_anomalies = torch.stack([nyu_reprs_torch[i,j,k,:] for i,j,k in nyu_anomalies_all])
    nyu_prototype_anomaly = nyu_reprs_anomalies.mean(dim=0)

    # plot IS (all) -> NYU (all)
    fig, ax = plt.subplots(1, 1, figsize=(3.4, 2.3))
    x = np.linspace(0.70, 0.98, 15)
    nyu_reprs_torch_cuda = nyu_reprs_torch.to('cuda')
    def find_anomalies2(min_sim):
        nyu_sims = F.cosine_similarity(is_prototype_anomaly[None,:].to('cuda'), nyu_reprs_torch_cuda.flatten(0,2), dim=-1).cpu()
        nyu_found_anomalies = np.array(np.unravel_index(torch.arange(len(nyu_sims))[nyu_sims > min_sim], nyu_reprs_torch.shape[:3])).T
        nyu_intersection_size = len(set(tuple(x) for x in nyu_anomalies_all) & set(tuple(x) for x in nyu_found_anomalies))
        return len(nyu_found_anomalies)-nyu_intersection_size, nyu_intersection_size, len(nyu_anomalies_all)-nyu_intersection_size
    tmp = np.array([find_anomalies2(i) for i in x])
    ax.bar(x, tmp[:,0]+tmp[:,1], label='false positives', color='orange', width=0.015)
    ax.bar(x, tmp[:,1], label='true positives', color='green', width=0.015)
    ax.bar(x, -tmp[:,2], label='false negatives', color='purple', width=0.015)
    ax.tick_params(axis='x', labelsize=8)
    ax.tick_params(axis='y', labelsize=8)
    ax.set_xlabel('min. cosine similarity', fontsize=9)
    ax.set_ylabel('count', fontsize=9)
    ax.legend(fontsize=7)
    plt.tight_layout()
```

We labelled the anomaly patches in the norm plots for `up[1]` in the \nameref{sec-datasets-imagenet-subset} using a custom build annotation tool and find that about 25% of all images have at least one such high-norm anomaly. Anomaly tokens have a very high cosine similarity of ~0.80 between each other, compared to an average cosine similarity of about 0.055 between all representation tokens in `up[1]`. Especially high is the cosine similarity between tokens at same position in different anomaly patches with a value of ~0.92 (e.g. between the upper left tokens in the 2\times2 patches). This high similarity suggests that the anomalies could be systematically found using cosine similarity. We test this hypothesis by computing the cosine similarity between the mean `up[1]` anomaly token of the manually labelled anomalies in the \nameref{sec-datasets-imagenet-subset} and all `up[1]` representation tokens in the \nameref{sec-datasets-nyu-v2} dataset. All tokens with a cosine similarity higher than a threshold (e.g. 0.8) are considered to be anomalies. As can be seen in @fig-high-norm-anomalies-searching, for the right threshold, this simple search method achieves a high true positive rate and a low false negative rate, while the false positive rate is moderate. Notably, the false positives here do not necessarily indicate a failure of the method, but could also be due to missing labels caused by the manual labelling process. Lower thresholds lead to more false positives, while higher thresholds lead to more false negatives.

<!-- Note: the 0.055 were taken over 20 images from the imagenet subset -->

```{python}
#| label: fig-high-norm-anomalies-over-layers
#| fig-cap: Mean L2 norms of high-norm anomaly tokens relative to the mean of all token norms per representation over the different layers of SD-1.5. We manually labeled these anomalies in the representation norm plots of the `conv-in` and `up[1]` blocks for the \nameref{sec-datasets-imagenet-subset}. The shaded areas show the standard deviation.

@cache_mpl_plot()
def plot_high_norm_anomalies_over_layers():
    import numpy as np
    from sdhelper import SD
    import torch
    from datasets import load_dataset
    import torch

    # load data
    data = load_dataset("JonasLoos/imagenet_subset", split="train")
    up1_anomalies = np.load("assets/data/high_norm_anomalies_imagenet_subset_step50_seed42_up1.npy")
    convin_anomalies = np.load("assets/data/high_norm_anomalies_imagenet_subset_step50_seed42_conv_in.npy")
    blocks = [b for blocks_list in sd15_all_blocks.values() for b in blocks_list]

    # get representations
    sd = SD(disable_progress_bar=True)
    representations_raw = sd.img2repr([x['image'] for x in data], extract_positions=blocks, step=50, seed=42)

    # init result matrices
    up1_anomaly_norms = np.zeros((len(blocks),len(up1_anomalies)))
    convin_anomaly_norms = np.zeros((len(blocks),len(convin_anomalies)))

    # up1 anomaly
    for j, tmp in enumerate(up1_anomalies):
        img_idx, w_idx, h_idx = tmp.tolist()
        for i, block in enumerate(blocks):
            repr = representations_raw[img_idx][block].squeeze(0).to(dtype=torch.float32)
            features, h, w = repr.shape
            norms = repr.norm(dim=0)
            h_up1 = representations_raw[0]['up_blocks[1].upsamplers[0]'].shape[2]
            scale = h / h_up1
            h_idx_scaled = int(h_idx*scale)
            w_idx_scaled = int(w_idx*scale)
            offset = int(2*scale)
            if offset < 1: offset = 1
            reprs_anomaly = repr[:,h_idx_scaled:h_idx_scaled+offset, w_idx_scaled:w_idx_scaled+offset]
            rel_norm = reprs_anomaly.norm(dim=0).mean() / norms.mean()
            up1_anomaly_norms[i,j] = rel_norm.item()

    # convin anomaly
    for j, tmp in enumerate(convin_anomalies):
        img_idx, w_idx, h_idx = tmp.tolist()
        for i, block in enumerate(blocks):
            repr = representations_raw[img_idx][block].squeeze(0).to(dtype=torch.float32)
            features, h, w = repr.shape
            norms = repr.norm(dim=0)
            h_convin = representations_raw[0]['conv_in'].shape[2]
            scale = h / h_convin
            h_idx_scaled = int(h_idx*scale)
            w_idx_scaled = int(w_idx*scale)
            reprs_anomaly = repr[:,h_idx_scaled, w_idx_scaled]
            rel_norm = reprs_anomaly.norm(dim=0) / norms.mean()
            convin_anomaly_norms[i,j] = rel_norm.item()

    # setup plot
    fig, ax1 = plt.subplots(figsize=(8,3))

    # up[1] anomaly
    plt.fill_between(range(len(blocks)), 
                    up1_anomaly_norms.mean(axis=1) - up1_anomaly_norms.std(axis=1),
                    up1_anomaly_norms.mean(axis=1) + up1_anomaly_norms.std(axis=1), 
                    alpha=0.2)
    plt.plot(range(len(blocks)), up1_anomaly_norms.mean(axis=1), label='up[1] anomalies')

    # conv-in anomaly
    plt.fill_between(range(len(blocks)),
                    convin_anomaly_norms.mean(axis=1) - convin_anomaly_norms.std(axis=1),
                    convin_anomaly_norms.mean(axis=1) + convin_anomaly_norms.std(axis=1),
                    alpha=0.2) 
    plt.plot(range(len(blocks)), convin_anomaly_norms.mean(axis=1), label='conv-in anomalies')

    # mean norm
    plt.plot(range(len(blocks)), np.ones(len(blocks)), color='black', linestyle='--', label='all')

    # plot x ticks
    x = np.arange(len(blocks))
    ticks = ['attn' if 'attentions' in block else 'res' if 'resnets' in block else 'down' if 'downsamplers' in block else 'up' if 'upsamplers' in block else 'conv' if 'conv' in block else '?' for block in blocks]
    ax1.set_xticks(x)
    ax1.set_xticklabels(ticks, rotation=90)
    ax1.text(-0.01, -0.12, 'layers:', ha='right', va='center', transform=ax1.transAxes)
    ax1.text(-0.01, -0.30, 'blocks:', ha='right', va='center', transform=ax1.transAxes)

    # compute main blocks names and positions
    main_blocks = []
    main_block_positions = []
    layer_counter = 0
    for block_name, block_list in sd15_all_blocks.items():
        if 'mid' in block_name:
            name = 'mid'
        elif 'conv' in block_name:
            name = block_name[5:]
        else:
            name = block_name.replace('_blocks','').replace('[','').replace(']','').replace('down', 'dn')
        main_blocks.append(name)
        main_block_positions.append(layer_counter)
        layer_counter += len(block_list)

    # plot main blocks names
    ax_x2 = ax1.secondary_xaxis(location=0)
    ax_x2.set_xticks([p+len(bl)/2-0.5 for p, bl in zip(main_block_positions, sd15_all_blocks.values())], labels=[f'\n\n\n{b}' for b in main_blocks], ha='center')
    ax_x2.tick_params(length=0)

    # lines between main blocks
    for p in main_block_positions[1:]:
        ax1.axvline(x=p-0.5, color='black', linestyle='--', c='lightgray')
    ax_x3 = ax1.secondary_xaxis(location=0)
    ax_x3.set_xticks([p-0.5 for p in main_block_positions[1:]], labels=[])
    ax_x3.tick_params(axis='x', length=34, width=1.5, color='lightgray')

    # configure plot
    plt.ylabel("mean norm relative to all")
    ax1.legend()
    ax1.set_yscale('log')
    ax1.set_yticks([0.5, 1.0, 2.0, 4.0])
    ax1.set_yticklabels([f'{x:.1f}' for x in ax1.get_yticks()])
    plt.tight_layout()
```

As described above, both types of the high-norm anomalies occur in the same regions of nearby blocks and layers. To investigate this observation, we manually label the anomalies in the `conv-in` and `up[1]` representations of SD-1.5 and compare the norms at their positions to the mean representation norm in the respective block or layer. As can be seen in @fig-high-norm-anomalies-over-layers, `conv-in` anomalies tend to have the highest relative norm in the `conv-in` representations, and continue to have a high but decreasing relative norm in the `down[0]` layers and the first layers of the `down[1]` block. In the lower blocks of the U-Net, i.e. `down[2]` to `up[2]`, the tokens at these positions tend to have average norms, but in the layers of the `up[3]` block, the relative norm is again increased. This pattern is plausible due to the skip connections in the U-Net architecture between the down- and up-blocks. The `up[1]` anomalies begin to have a high relative norm in the layers of the `up[1]` block, which holds until the end of the `up[2]` block. These findings confirm and quantify the observed presence of two distinct types of high-norm anomalies that each persist over multiple layers.

While the described high-norm anomalies occur regularly and are clearly distinguishable in the SD-1.5 and SD-2.1 representations, we do not find similar anomalies when inspecting SDXL and SDXL-Turbo.

The observed `up[1]` high-norm anomalies seem similar to the artifacts found by @darcet2024vision in the attention maps of vision transformers such as DINOv2 [@oquab2024dinov]. An additional indication, that these anomalies might be of the same nature is that they seem to materialize in the output of the attention layers of the `up[1]` block, as can be seen in @fig-high-norm-anomalies-over-layers. @darcet2024vision propose to train vision transformers with additional register tokens to remedy the artifacts they found, which might also be possible for the attention layers of the SD U-Net. Due to the high computational cost of training near state-of-the-art diffusion models, we leave this approach for future work. Another potential approach that might be able to reduce these anomalies is *CleanDIFT* proposed by @stracke2024clean, who suggest finetuning SD models to reduce noise in the representations and to allow extraction of high quality representations at time step 0. The observations visualized in @fig-high-norm-anomalies-2 suggest that this approach might remove the `up[1]` high-norm anomalies, as they are not present at time step 0.

Further information and findings on the statistics of the representation norms can be found in @sec-appendix-repr-norms.


### Impact of Anomalies {#sec-anomalies-impact}

Both border and high-norm anomalies introduce non-semantic similarities between tokens, which might impact the performance of downstream tasks. However, border anomalies are by definition at the borders of the image, which usually is part of the background. We also found qualitatively that the `up[1]` high-norm anomalies tend to be in background areas. The question is, whether these anomalies still have a measurable impact on the performance of downstream tasks. To investigate this, we evaluate the performance on a depth estimation task, which is a dense prediction task, where each token in the representation is relevant to the prediction. We presume that by using such a dense prediction task the impact of the anomalies can be observed more clearly, compared to a task there only selected tokens are relevant to the prediction, such as semantic correspondence on \nameref{sec-datasets-spair}. As the `conv-in` anomalies are only present in the upper blocks that are usually not used for representation extraction, we leave their evaluation for future work, and only focus on the corner/border and `up[1]` anomalies.

```{python}
#| label: fig-depth-estimation-performance-over-layers
#| fig-cap: Root mean squared error (rmse) of linear probes on the representations of the test set of the \nameref{sec-datasets-nyu-v2} dataset (lower is better). The border, corner, and anomaly results use the same linear probes, but are only evaluated on the tokens that are at the spatial position of a border, corner, or `up[1]` anomaly, respectively.

# relevant notebook: depth_estimation/depth_estimation_nyu_like_beyond_surface_statistics.ipynb

@cache_mpl_plot
def plot_depth_estimation_performance_over_layers():

    # load data
    y_train_rmse, y_train_huber, y_test_rmse, y_test_huber, y_train_rmse_anomaly, y_train_huber_anomaly, y_test_rmse_anomaly, y_test_huber_anomaly, y_train_rmse_corner, y_train_huber_corner, y_test_rmse_corner, y_test_huber_corner, y_train_rmse_border, y_train_huber_border, y_test_rmse_border, y_test_huber_border = np.load('assets/data/depth_estimation_nyu_like_beyond_surface_statistics_losses_10k_steps_2.npy').T
    x = np.arange(len(y_train_huber))
    blocks = [b for blocks_list in sd15_all_blocks.values() for b in blocks_list]

    fig, ax1 = plt.subplots(figsize=(8, 3))

    # plot x ticks
    ticks = ['attn' if 'attentions' in block else 'res' if 'resnets' in block else 'down' if 'downsamplers' in block else 'up' if 'upsamplers' in block else 'conv' if 'conv' in block else '?' for block in blocks]
    ax1.set_xticks(x)
    ax1.set_xticklabels(ticks, rotation=90)
    ax1.set_ylabel('rmse')
    ax1.text(-0.01, -0.12, 'layers:', ha='right', va='center', transform=ax1.transAxes)
    ax1.text(-0.01, -0.30, 'blocks:', ha='right', va='center', transform=ax1.transAxes)

    # compute main blocks names and positions
    main_blocks = []
    main_block_positions = []
    layer_counter = 0
    for block_name, block_list in sd15_all_blocks.items():
        if 'mid' in block_name:
            name = 'mid'
        elif 'conv' in block_name:
            name = block_name[5:]
        else:
            name = block_name.replace('_blocks','').replace('[','').replace(']','').replace('down', 'dn')
        main_blocks.append(name)
        main_block_positions.append(layer_counter)
        layer_counter += len(block_list)

    # plot main blocks names
    ax_x2 = ax1.secondary_xaxis(location=0)
    ax_x2.set_xticks([p+len(bl)/2-0.5 for p, bl in zip(main_block_positions, sd15_all_blocks.values())], labels=[f'\n\n\n{b}' for b in main_blocks], ha='center')
    ax_x2.tick_params(length=0)

    # lines between main blocks
    for p in main_block_positions[1:]:
        ax1.axvline(x=p-0.5, color='black', linestyle='--', c='lightgray')
    ax_x3 = ax1.secondary_xaxis(location=0)
    ax_x3.set_xticks([p-0.5 for p in main_block_positions[1:]], labels=[])
    ax_x3.tick_params(axis='x', length=34, width=1.5, color='lightgray')

    # plot train and test huber loss
    ax1.plot(x, y_test_rmse, label='all', color='tab:green', linestyle='-')
    ax1.plot(x, y_test_rmse_border, label='border', color='tab:red', linestyle='-')
    ax1.plot(x, y_test_rmse_corner, label='corner', color='tab:orange', linestyle='-')
    ax1.plot(x, y_test_rmse_anomaly, label='anomaly', color='tab:purple', linestyle='-')

    ax1.legend()
    plt.tight_layout()
```

We evaluate the performance of the SD-1.5 representations for depth estimation on the \nameref{sec-datasets-nyu-v2} dataset [@Silberman2012Indoor] using a simple linear probe, as described in @sec-methods-depth-estimation. This linear probe was trained on 1159 and tested on 290 pairs of RGB and depth images, where the RGB images are the input and the depth images are the target. The results over the different blocks and layers of SD-1.5 are shown in @fig-depth-estimation-performance-over-layers. The performance follows an overall similar trend as what was found by @Chen2023BeyondSS, where the depth estimation performance tends to improve for the lower layers, peaks at the beginning of the `up`-blocks and then decreases again.

For the border tokens, the depth estimation results are mostly similar to the average performance of all tokens, except for `conv-in` the first `down`-blocks, where the performance is better. The performance for corner tokens follows a similar trend, except for heavily varying performance in the layers of the `down[0]` block and decreased performance in and around the `up[1]` block, where the performance over all tokens actually peaks. We find `up[1]` to be also the most useful block for representation extraction for other tasks, such as semantic correspondence (see @sec-semantic-correspondence). This decrease in performance at the `up[1]` block suggests that downstream tasks might have decreased performance in corner regions. Fortunately, image corners often contain less critical content for downstream tasks^[For example, for semantic correspondence on \nameref{sec-datasets-spair}, the number of keypoints per corner token is, depending on the block, 3-6 times below the average number of keypoints per token.].

Interestingly, the performance of tokens at the position of `up[1]` high-norm anomalies is better across the board. This result is unexpected, as previous observations showed less semantic behavior for these anomaly tokens. However, the somewhat uniform improve in performance over all layers, while the high-norm anomalies only impact certain layers (see @fig-high-norm-anomalies-over-layers), suggests that the performance increase is unrelated to the high-norm anomalies. Potentially, the anomalies tend to be located in areas where depth estimation is easier.

Overall, these results suggest that `up[1]` high-norm anomalies likely do not significantly impact depth estimation negatively. As the depth estimation was performed using a linear probe, which might be able to ignore the dimensions that cause the high norms, this result might not transfer to tasks without a linear probe. Investigating the impact of high-norm anomalies on other types of downstream tasks is therefore a potential direction for further research.


# Discussion {#sec-discussion}

In this thesis, we set out to investigate representation similarity in latent diffusion models, to evaluate their performance on downstream tasks, and to identify relevant properties of the representations. We confirm that SD representations are useful for downstream tasks, and identify a position bias, a texture and color bias, and high-norm anomalies. To the best of our knowledge, our findings for the position bias and the high-norm anomalies are novel and not reported in the literature. In the following, we summarize and discuss our results, point out limitations, and suggest directions for future work.


## Summary of Results {#sec-discussion-results}

**Position Bias.** We find a position bias (see @sec-position-bias) in the lower U-Net blocks, which is linearly extractable. The origin of the position bias in the SD representations is unclear, as SD does not use spatial positional embeddings as part of the architecture [@rombach2022highresolution;@CompVis2022StableDiffusionRepo]. However, we observe that the position information seems to be relative to the nearest border. This suggests that it might arise in the convolutional layers, which use 0-padding and therefore can detect the image boundaries. While some position-related issues of SD representations are described and addressed in the literature [@zhang2024telling], to the best of our knowledge, we are the first to describe and analyze this bias more broadly. Even though the position bias may be beneficial in tasks where the absolute position of objects in the image is relevant, it can lead to errors in other tasks by outweighing semantic similarity. We show that it has negative impact on the semantic correspondence performance on the \nameref{sec-datasets-spair} dataset, which however heavily differs depending on the block. For `up[1]`, which is the most relevant block for semantic correspondence, the performance degradation is negligible, but for other tasks, where other blocks are preferred, the impact of the position bias might be significant.

**Texture and Color Bias.** While the lower blocks encode more abstract meaning, the upper blocks encode more low-level features, such as texture and color (see @sec-texture-color-bias). We specifically investigate the biases towards changes in texture and color of the different blocks and find that, especially for color, there is a strong difference between the blocks. While the lower blocks seem very robust against color changes, the upper blocks, especially `conv-in` and `up[3]` (in the case of SD-1.5), are easily disturbed. The results of our texture-related experiments tend to paint a similar picture, but are less conclusive. We primarily use a simple texture overlay, while more sophisticated texture degradation methods could likely yield more conclusive results. Overall, our results confirm the observations of previous works that different blocks of the U-Net encode different levels of abstraction [@zhang2023tale;@tang2023emergent].

**Anomalies.** We find both corner and high-norm anomalies in the representation norms and cosine similarity maps (see @sec-anomalies). The corner tokens in the representations tend to have an increased similarity between each other, which often is not related to any semantic similarity. We find that the performance of corner tokens for depth estimation is decreased, at least for the `up[1]` block. However, their impact on downstream tasks is likely limited, as corners are often less critical to solving a task. The origin of the corner anomalies is unclear, but could be related to usage of padded convolutions in the U-Net architecture, and/or to the position bias.
Additionally to the anomalous behavior of corner tokens, we find two types of anomalies with high norms in different blocks. Both types mostly consist of multiple neighboring tokens with an anomalously high norm, and both have a high cosine similarity among each other, similar to the corner anomalies.
The first type is found primarily in the `conv-in` block, but also appears in the layers of the `down[0]` and `up[3]` blocks, and is robust over different time steps and surprisingly sometimes even over different models. Its relevance for downstream tasks is limited, as these blocks are usually not used. Similar to the corner anomalies, the origin of this anomaly type is unclear. One hypothesis is that they may be connected to some unintuitive behavior of the SD VAE.
The second type of high-norm anomalies is found primarily in the `up[1]` block, but also appears in the layers of the `up[2]` block. The L2 norm of these anomalies tends to increase in the attention layers of the `up[1]` block, which is an indication that they might be related to the artifact tokens found by @darcet2024vision in the representations of vision transformers. Interestingly, the cosine similarity between these anomalies is so high, that it can be used to identify new `up[1]` anomalies with a high accuracy. When evaluating their impact on depth estimation on the \nameref{sec-datasets-nyu-v2} dataset, we do not find any degradation in performance. While we chose depth estimation due to its dense prediction nature that might make the impact of anomalies more visible, the usage of a linear probe for this task might nullify any negative impact of these anomalies. While this suggests that tasks using trained networks on top of the representations may not be significantly impacted by these anomalies, their effect on tasks that directly use the representations, such as dense correspondence through similarity matching, remains a possibility.

**Performance Evaluation.** While we focussed on the properties and biases of the representations, we also evaluated their general performance for linear probe classification and in more detail for semantic correspondence, see @sec-downstream-tasks. For linear probe classification, we find results of 76 - 83% maximum accuracy for the different models on \nameref{sec-datasets-cifar}-10, 50 - 57% on \nameref{sec-datasets-cifar}-100, and 32 - 47% on Tiny-\nameref{sec-datasets-imagenet}. We find the best performance at or near the `mid` block, depending on the model. For semantic correspondence on the \nameref{sec-datasets-spair} dataset, we find maximum PCK@0.1$_\text{bbox}$ values of 39.85 - 54.80 for the different models. The best blocks here are the `up[1]` block for SD-1.5, SD-2.1, and SD-Turbo, and `up[0]` for SDXL and SDXL-Turbo. Surprisingly, for semantic correspondence, SDXL and SDXL-Turbo perform significantly worse than the others, even though they are newer models with higher image generation quality [@podell2023sdxl;@sauer2023adversarial]. Interestingly, the semantic correspondence performance of the Turbo variants (SD-Turbo, SDXL-Turbo) [@sauer2023adversarial] is better than the respective base models (SD-2.1, SDXL). We find time step (noise level) 50 out of 1000 to be a good choice for representation extraction across all evaluated models. The choice of the time step is highly debated topic in the literature, with both higher and lower values being suggested (see @sec-related-work). While our finding is not unusual, a higher time step seems more common [@zhang2023tale;@tang2023emergent].

**Performance Improvements.** We also investigated methods of improving the performance using the representations for semantic correspondence on the \nameref{sec-datasets-spair} dataset, see @sec-sc-improvements. Some of these ideas are inspired by the found biases, while others are more general approaches or inspired by the literature. We find several methods that slightly improve the performance by up to 1-2 PCK. Especially methods that reduce noise in the representations, such as representation averaging, tend to help. In general, the more complex approaches suggested in the literature remain more promising [@zhang2023tale;@zhang2024telling;@luo2023dhf;@stracke2024clean]. Our results of only relatively small improvements indicate that before implementing further optimization methods on top of the representations, it is important to first determine the most performant choice of block(s) and time step for representation extraction.

<!-- Could do: join the literature comparison with the performance evaluation -->

**Comparison with Literature.** Our results for semantic correspondence on \nameref{sec-datasets-spair} without further enhancements tend to fall in line with existing literature using SD representations [@tang2023emergent]. Other works vary in how they extract representations and many additionally employ further optimizations and methods to improve the performance. @hedlin2023unsupervised do semantic correspondence using the attention maps in the U-Net of SD-1.4 reporting a PCK@0.1$_\text{bbox}$ of 45.4, which is lower than our results. @tang2023emergent use a relatively simple and mostly similar approach to ours, but include e.g. representation averaging (as discussed in @sec-sc-improvements), and report 52.8 PCK@0.1$_\text{bbox}$ - very similar to our results. Improvements to this are reported, for example, by @zhang2023tale, who fuse SD and DINO representations and achieve up to 63.73 PCK@0.1$_\text{bbox}$. @zhang2024telling further improve on this by introducing a test-time adaptive pose alignment strategy to achieve 68.64 PCK@0.1$_\text{bbox}$. @stracke2024clean propose a fine-tuning method for SD to improve the quality of the representations resulting in further improvements by 1-2 PCK.

When comparing our results with non-diffusion foundation models for representation learning, SD performs well, but not strictly better than other approaches. For DINO (v1, ViT-S/8) [@tang2023emergent], the performance on semantic correspondence on \nameref{sec-datasets-spair} is reported to be relatively low at 33.3 PCK@0.1$_\text{bbox}$ [@zhang2023tale]. For DINOv2 (ViT-B/14) [@oquab2024dinov], it is at 55.6 PCK@0.1$_\text{bbox}$ [@zhang2023tale], i.e. slighly higher than our results for SD.

**Tool Development.** The tools developed in the course of this thesis have been instrumental in identifying and quantifying the biases and anomalies (see @sec-representation-extraction-exploration). Both the `sdhelper` package allowing for faster implementation of experiments and the *Representation Similarity Explorer* for interactive exploration of representation similarities proved to be invaluable tools.

<!-- 
Could add:
* maybe shortly mention SD2.1 `up[0]` behaving weirdly
* maybe mention advantage in terms of interpretability when using representation similarities, because one can look at the similarity maps to understand what the model is doing
 -->


## Limitations {#sec-limitations}

While we believe our findings contribute meaningfully to understanding representation extraction from latent diffusion models, there are several important limitations to consider. In general, the field of using diffusion models for representation extraction is very young, and about half of our references were published in 2024 or are only available as preprints, and thus are potentially not as rigorously evaluated.

**Representation Extraction.** We use a default time step (50) for representation extraction, which could cause us to miss time step dependent properties or improvements. For example, @tang2023emergent find that higher time steps relate to more semantic representations, while lower time steps relate to more low-level bias. Additionally, a non-zero time step inherently adds noise to the latent image, which can reduce the information available for representation extraction, especially fine-grained details [@stracke2024clean]. Approaches against this include averaging over multiple representations [@tang2023emergent] or using specific finetuning methods [@stracke2024clean]. An additional limitation that might prevent us from fully utilizing the representations is that we use an empty prompt. As indicated in @sec-related-work and @sec-sc-improvements, using relevant text prompt conditioning during representation extraction can improve the performance on downstream tasks [@xu2023open;@li2023sd4match;@tang2023emergent;@zhang2023tale;@zhang2025three], however, a relevant text prompt is not always available. Furthermore, the spatial resolution of the representations is relatively low, especially for the lower blocks, as described in @sec-methods-representation-extraction. This limits the performance on downstream tasks that rely on spatially accurate predictions, such as semantic correspondence.

**Downstream Tasks.** To utilize the representations for downstream tasks, we either use cosine similarity or linear probes. While cosine similarity is commonly used [@tang2023emergent;@zhang2023tale;@luo2023dhf], e.g. @steck2024cosinesimilarity caution against "blindly using cosine similarity" for embeddings, as it can yield meaningless similarities in certain cases. Other similarity measures might be more suitable for certain tasks or models. For example, for SD-3, a diffusion transformer discussed in @sec-future-work, our preliminary observations indicate that plain cosine similarity does not work well. While the usage of linear probes might be a good starting point for evaluating the representations, more sophisticated methods can further improve the performance, as described in @sec-related-work [@zhao2023unleashing;@Patni2024ECoDepth;@baranchuk2022labelefficient]. A general difficulty of evaluating the impact of biases in the representations is the existence of image composition biases, such as foreground objects typically being located in the center of the image. This relationship between image content and spatial positions adds an additional potential origin for observed spatial differences in the representations, and therefore complicates the evaluation of representation biases.

**Computational Constraints.** Due to time and resource constraints, we were not able to perform all experiments on all SD models and over all potentially interesting hyperparameter settings. This might have prevented us from finding more conclusive results and additional insights. Especially the focus on only a few U-Net based SD models limits the generalizability of our findings.


## Future Work {#sec-future-work}

There are several directions for future work. First of all, based on the intuition of the image generation capabilities of diffusion models hinting at semantic representations, the relationship between image and representation quality could be further investigated.


### Other Tasks {.unnumbered .unlisted}

In this thesis, we evaluate the representations and their biases on semantic correspondence, dense correspondence, depth estimation, and linear probe classification. As described in @sec-related-work, existing works also explore the suitability of diffusion model representations for other tasks, such as semantic segmentation [@baranchuk2022labelefficient;@ji2024diffusion;@couairon2024zeroshot;@zhao2023unleashing;@tian2024diffuse;@zhang2025three;@couairon2024diffcutcatalyzingzeroshotsemantic;@Yang2023Diffusion], robot control [@gupta2024pretrained;@shridhar2024generativeimageactionmodels;@tsagkas2024clickgraspzeroshotprecise], modification of the image generation process [@park2023unsupervised;@jeong2024trainingfree;@haas2023discovering;@gambashidze2024aligningdiffusionmodelsnoiseconditioned;@hudson2023soda;@park2023understanding], 3D scene understanding [@Man2024Lexicon3DPV], and surface normal estimation [@Ke2024RepurposingDI;@Lee2024ExploitingDI;@xu2024diffusionmodelstrainedlarge;@ye2024stablenormalreducingdiffusionvariance]. Creating a comprehensive overview of the suitability of diffusion model representations for all these tasks and a comparison between different models and alternatives such as DINOv2 [@oquab2024dinov] is a promising direction for future work, and may give insights into which architectural decisions and training methods lead to useful representations.

Compared to this thesis, many of these existing works already employ more complex methods to extract and use the representations for the tasks. However, comparing and improving these methods and developing new ones can lead to further improvements and new insights about the representations. Particularly interesting seem the directions of @zhang2024telling, who add test-time adaptive pose alignment to a combination of SD and DINO representations, and @stracke2024clean, who remove the requirement of adding noise during representation extraction.


### Biases {.unnumbered .unlisted}

Our findings about the biases and anomalies in the representations are based on both qualitative and quantitative analysis. However, especially further quantitative experiments could help to better understand the biases and anomalies, and their implications for representation extraction and downstream tasks. For the position bias, we found correspondences between absolute positions, but the analysis could be extended to relative positional information. For the texture and color bias, our analyses could be extended by evaluating the robustness to style transfer over different blocks. For the high-norm anomalies, their origin, relevance for the model, and impact on downstream tasks are interesting directions for future research. Especially noteworthy is the potential connection to the artifacts found by @darcet2024vision in vision transformers.

In general, our experiments could benefit from being extended to the output of the attention and ResNet layers, and potentially also to the attention scores inside the attention layers [@zhao2023unleashing;@hedlin2023unsupervised]. Moreover, extending the experiments and the analysis of the impact of the biases and anomalies on downstream tasks to additional models, tasks, and datasets would strengthen the generalizability of our findings. Especially for the position bias and the anomalies, an exploration into their origins may be insightful. As a further step, mitigating negative effects of the biases and anomalies on downstream tasks could be an important direction for future investigation.


### Diffusion Transformer {.unnumbered .unlisted}

```{python}
#| label: fig-sd3-example
#| fig-cap: Representation L2 norms and cosine similarity for SD-3-medium at transformer block 12. Cosine similarity is relative to the token at position (24,34) in image 1 (cat's eye).
#| wrapfigure: R 0.55

@cache_mpl_plot
def plot_sd3_example():
    from sdhelper import SD

    blocks = ['transformer_blocks[12]']
    img_paths = [
        'assets/images/cat_on_roof.jpg',
        'assets/images/dog_in_snow.jpg',
    ]

    sd = SD('SD3', disable_progress_bar=True)
    reprs = sd.img2repr(img_paths, extract_positions=blocks, step=50, resize=1024, batch_size=2)

    fig, axs = plt.subplots(len(img_paths), 3, figsize=(3*1.2+0.5, len(img_paths)*1.2+0.3))
    for i, img_path in enumerate(img_paths):
        axs[i, 0].imshow(Image.open(img_path))
        axs[i, 1].imshow(reprs[i][blocks[0]][0].norm(dim=0).cpu().numpy())
        axs[i, 2].imshow(reprs[i].cosine_similarity(reprs[0])[:,:,24,34])
        axs[i, 0].axis('off')
        axs[i, 1].axis('off')
        axs[i, 2].axis('off')
    axs[0,0].set_title('Image', fontsize=8)
    axs[0,1].set_title('Norm', fontsize=8)
    axs[0,2].set_title('Cosine\nSimilarity', fontsize=8)
    axs[0,0].text(-0.1, 0.5, 'Image 1', ha='right', va='center', fontsize=8, transform=axs[0,0].transAxes)
    axs[1,0].text(-0.1, 0.5, 'Image 2', ha='right', va='center', fontsize=8, transform=axs[1,0].transAxes)
    plt.tight_layout()
    plt.subplots_adjust(wspace=0.05, hspace=0.05)
```

During the course of this thesis, the state-of-the-art in open-source image generation shifted from the U-Net based models to diffusion transformers [@peebles2023scalable]. Stability AI released SD-3 in February 2024 [@esser2024scaling] and later SD-3.5 [@stabilityai2024sd35] and a Turbo variant [@sauer2024fasthighresolutionimagesynthesis]. The SD-3 models primarily improve prompt adherence and text support, which is achieved using a transformer-only denoising model that more deeply integrates the text embedding into the model architecture [@esser2024scaling].

Additionally, Black Forest Labs released the Flux.1 family of models in August 2024, which not only set a new state-of-the-art in open-source image generation, but also rival the best closed-source models such as Midjourney v6.0 or Ideogram v2.0 [@blackforestlabs2024fluxRepo;@blackforestlabs2024fluxAnnouncement]. This indicates that now art generated even by open-source diffusion models is not distinguishable as AI-generated anymore for general users [@ha2024organicdiffuseddistinguishhuman].

We analyze SD-3-medium and Flux.1-schnell to get a first glimpse into their potential for representation extraction. Both models have a similar general architecture and are latent diffusion models just as the previous SD models, but switch for the denoising process from a U-Net to a transformer-based model. SD-3-medium has 24 transformer blocks with 1536 channels and a spatial resolution of 64\times64 at the default image size of 1024\times1024. Flux.1-schnell has 19 transformer blocks with 3072 channels and the same spatial resolution of 64\times64 at an image size of 1024\times1024. The representations can be similarly extracted and analyzed as for the U-Net based SD models. One difference is that for the diffusion transformer models, the representations for all blocks have the same spatial and channel dimensions.

Preliminary results for downstream tasks, such as semantic correspondence, show relatively low performance when using the same settings as for the U-Net based models. In the *Representation Similarity Explorer* (see @sec-repr-sim-explorer) other similarity measures than cosine similarity, for example centered cosine similarity, or an L2 based similarity, seem to result in more semantically meaningful similarity maps. But as visible in @fig-sd3-example, even the cosine similarity maps can be semantically meaningful. Therefore, investigating the suitability of diffusion transformers for representation extraction seems like a promising direction for future research.


# Conclusion {#sec-conclusion}

This thesis investigated the representation similarity in latent diffusion models, specifically SD, aiming to assess the suitability for downstream tasks and to uncover inherent properties and biases. Our findings confirm the potential of SD models as a source of useful visual representations, but simultaneously reveal limitations arising from a position bias, a texture and color bias, and high-norm anomalies. To the best of our knowledge, our investigation of the position bias and the high-norm anomalies constitute the main novelties of our work, contributing new insights on the usage of SD models for downstream tasks.

We found that the representations of SD are not solely driven by semantic content. 
First, we found a **positional bias**, primarily in the lower blocks of the SD U-Net. They encode linearly extractable positional information that is relative to the image borders. It can negatively impact tasks like semantic correspondence by overshadowing semantic similarities, but in other cases might also be helpful when the absolute spatial position of objects is relevant.
Second, we investigated a **texture and color bias**, and found that the upper blocks are more sensitive to low level visual features, such as texture and color. In contrast, the lower blocks capture more semantically meaningful information.
Third, we uncovered several types of spatially-localized **anomalies**, which appear in the representations of some images. Similarities for corner tokens, and to some degree border tokens, are often less semantically meaningful than for other positions. Furthermore, we observe groups of tokens with a very high L2 norm in the upper blocks, primarily `conv-in`, with unclear origins. Similarly, in the lower blocks (especially `up[1]`), which are more relevant for representation extraction, we observe a second type of high-norm anomalies. There, the cosine similarity between the anomalous tokens is so high, that new anomalous tokens can be detected with it. These `up[1]` anomalies might be related to artifacts found in vision transformers [@darcet2024vision]. 
We showed that these biases and anomalies affect downstream tasks in some cases, but the impact varies depending on the model, task, data, and hyperparameters.

We demonstrated the utility of SD representations in linear probe classification, and primarily semantic correspondence, while also emphasizing the need for careful selection of the U-Net block and time step. Notably, the distilled Turbo models [@sauer2023adversarial] seem to provide slightly better representations, indicating that training methods can further improve representation quality. However, the more recent SDXL models surprisingly show a significant decrease in semantic correspondence performance compared to SD-1.5 and SD-2.1, which challenges the idea that better image generation capabilities indicate more useful representations.

In summary, our results demonstrate that SD models learn representations that contain more than just semantic information. Future research should focus on how the identified biases and anomalies can be mitigated and if they generalize to other models, including new architectures like diffusion transformers. This will not only advance the field of representation learning but also promote more reliable and robust use of diffusion model representations in downstream tasks.


# Acknowledgements {#sec-acknowledgements}

Many thanks to Lorenz for the great supervision, countless meetings, and helpful feedback.

Thank you to Prof. Dr. Klaus-Robert Mller and Prof. Dr. Grgoire Montavon for the opportunity to work on this thesis. Also thanks to the Machine Learning Group at TU Berlin in general for providing a great environment for research. Thanks to friends, family, and especially Jule, for all the support, motivation, and helpful discussions along the way.


# References {-}

::: {#refs}
:::



\appendix \renewcommand{\thefigure}{A\arabic{figure}} \renewcommand{\thetable}{A\arabic{table}} \setcounter{figure}{0} \setcounter{table}{0}

# Appendix {#sec-appendix}

\clearpage
## Experimental Setup {#sec-appendix-experimental-setup}

For all experiments, we use SD-1.5 as default model, except when stated otherwise. The default time step for representation extraction is `{python} DEFAULT_NOISE` out of 1000 steps (see @sec-methods-representation-extraction). For semantic and dense correspondence, we use cosine similarity as default measure for determining the best match. We always use our `sdhelper` library (see @sec-sdhelper) for extracting the representations, which uses the Hugging Face `diffusers` [@vonplaten2022diffusers] library and the model weights available on the Hugging Face model hub. We conducted most experiments on a single Nvidia RTX 3090, however, more demanding experiments were run on the Hydra cluster of the TU Berlin^[More information about the Hydra cluster can be found at [git.tu-berlin.de/ml-group/hydra/documentation](https://git.tu-berlin.de/ml-group/hydra/documentation)].


### Public Code

We publish the code for some of our experiments, tools, and scripts as open-source software:

* Thesis document and experiments: [github.com/JonasLoos/thesis](https://github.com/JonasLoos/thesis)
* Representation Similarity Explorer: [github.com/JonasLoos/sd_representation_similarity\hspace{0pt}_explorer](https://github.com/JonasLoos/sd_representation_similarity_explorer)
* `sdhelper` library: [github.com/JonasLoos/sdhelper](https://github.com/JonasLoos/sdhelper)
* \nameref{sec-datasets-spair} dataset loading script: [huggingface.co/datasets/0jl/SPair-71k](https://huggingface.co/datasets/0jl/SPair-71k)
* \nameref{sec-datasets-nyu-v2} dataset loading script: [huggingface.co/datasets/0jl/NYUv2](https://huggingface.co/datasets/0jl/NYUv2)


### Experiment Parameters for Reproducibility

To facilitate reproducibility of our results, we provide additional details on the experimental setup.

For representation extraction in our experiments, we used SD-1.5 and time step `{python} DEFAULT_NOISE`, except if stated otherwise.

* @tbl-repr-shapes: data: images of the specified default sizes;
* @fig-noise-levels: data: one manually chosen image of size 512\times512 that is generated with Flux.1-schnell; the VAE of SD-1.5 is used; linear interpolation between the original image and standard Gaussian noise;
* @fig-similarities-example: data: three manually chosen images of size 512\times512 generated with Flux.1-schnell;
* @fig-kmeans-norms-pca: data: one manually chosen image of size 512\times512 generated with Flux.1-schnell;
* @tbl-lpc: data: \nameref{sec-datasets-cifar}-10, \nameref{sec-datasets-cifar}-100, and Tiny-\nameref{sec-datasets-imagenet}; linear probe training: 80 epochs, batch size 128, data shuffle, optimizer Adam [@kingma2017adam], learning rate $10^{-3}$;
* @fig-sc-pck-over-blocks-noise: data: \nameref{sec-datasets-spair} (test split - 88328 correspondences); input images are scaled so that the larger side is 512px; time steps: $[0, 10, 25, 50, 75, 100, 150, 200, 300, 500, 800]$;
* @fig-position-similarities-empty-image: data: 100 empty (black) images of size 512\times512; block: `mid`;
* @fig-position-classifier-accuracy: data: \nameref{sec-datasets-imagenet-subset} (500 images, 400 train, 100 test); Batch size: 512; 5 epochs; optimizer Adam[@kingma2017adam]; learning rate $10^{-3}$; loss: cross-entropy for classification, MSE for regression; samples were randomly permutated each epoch;
* @fig-pos-embedding-aspect-ratios: data: One empty (black) image per resolution and block;
* @fig-dense-correspondence-flip: data: two manually chosen images of size 512\times512 that are generated with Flux.1-schnell; The transferred image tiles in the "(mapped) image" columns are individually flipped, so that a perfect correspondence results in the flipped image;
* @fig-sc-errors-by-relative-position: data: custom synthetic dataset with 11623140 correspondences over 2048 images of size 512\times512 (see @sec-position-bias);
* @fig-texture-color-bias-examples: data: four manually chosen images of size 512\times512 that are generated with Flux.1-schnell;
* @fig-color-bias-rgb-bgr-dc: data: \nameref{sec-datasets-imagenet-subset}; results are averaged over the 500 images; for representation extraction, we used different seeds for the source and target images; number of interpolation steps: 10;
* @fig-texture-bias-texture-overlay-dc: same as @fig-texture-bias-texture-overlay-dc, except for the following; overlay texture is generated with SD-1.5 and the same for all images;
* @fig-texture-color-bias-sc: data: \nameref{sec-datasets-spair} (test split - 88328 correspondences); input images are scaled so that the larger side is 512px;
* @fig-cosine-similarity-corner: data: three manually chosen images of size 512\times512 that are generated with Flux.1-schnell; block: `mid`;
* @fig-border-corner-similarity: data: \nameref{sec-datasets-imagenet-subset}; results are averaged over the 500 images;
* @fig-high-norm-anomalies-1: data: one manually chosen image of size 512\times512 that is generated with Flux.1-schnell; L2 norm along the channel dimension;
* @fig-high-norm-anomalies-2: data: one manually chosen image of size 512\times512 that is generated with Flux.1-schnell; L2 norm along the channel dimension;
* @fig-high-norm-anomalies-searching: data: \nameref{sec-datasets-imagenet-subset} and \nameref{sec-datasets-nyu-v2}; anomalies were labeled manually by a human annotator, always with size 2\times2 tokens; We used the same noise seed during data labeling and anomaly searching;
* @fig-high-norm-anomalies-over-layers: data: \nameref{sec-datasets-imagenet-subset}; anomaly labeling as in @fig-high-norm-anomalies-searching; L2 norm along the channel dimension; results are averaged over the 500 images;
* @fig-depth-estimation-performance-over-layers: data: \nameref{sec-datasets-nyu-v2} (training on train split, evaluation on test split); linear probe training: 10000 steps with random batches, optimizer Adam [@kingma2017adam], learning rate $10^{-3}$, batch size 64, Huber loss [@pytorch2023huberloss] (following [@Chen2023BeyondSS], with $\delta = 1$) (i.e. different loss functions for training and evaluation);
* @fig-sd3-example: data: two manually chosen images of size 1024\times1024 that are generated with Flux.1-schnell; L2 norm along the channel dimension;
* @fig-appendix-position-classifier-example: data: \nameref{sec-datasets-imagenet-subset} (500 images, 400 train, 100 test); linear probe training: 5 epochs, batch size 512, optimizer Adam [@kingma2017adam], learning rate $10^{-3}$; samples were randomly permutated each epoch;
* @fig-appendix-dense-correspondence-flip-all-blocks: data: two manually chosen images of size 512\times512 that are generated with Flux.1-schnell; The transferred image tiles in the "(mapped) image" columns are individually flipped, so that a perfect correspondence results in the flipped image; the error is calculated as the percentile of possible error distances, so that the error distribution for a random estimator is independent of position;
* @fig-appendix-sc-errors-by-relative-position-full: same as @fig-sc-errors-by-relative-position;
* @fig-appendix-sc-errors-by-relative-position-spair-maps: data: \nameref{sec-datasets-spair} (test split - 88328 correspondences); input images are scaled so that the larger side is 512px; error rate is averaged over 32\times32 pixel tiles; Only the relative positions $[-255, ..., 256]^2 \subset [-511, ..., 512]^2$ are shown;
* @fig-appendix-sc-errors-by-relative-position-spair-lines: data: \nameref{sec-datasets-spair} (test split - 88328 correspondences); input images are scaled so that the larger side is 512px; error rate is averaged over (smoothed) bins with quadratically increasing width (to counteract the decreased sample density for larger distances);
* @fig-appendix-texture-color-bias-examples-1: same as @fig-texture-color-bias-examples
* @fig-appendix-texture-color-bias-examples-2: same as @fig-texture-color-bias-examples
* @fig-appendix-texture-bias-dense-correspondence-blur: same as @fig-texture-bias-texture-overlay-dc, except for the following; the interpolation steps correspond to Gaussian blurring with a kernel size between 0 and 16px;
* @fig-appendix-texture-bias-dense-correspondence-noise: same as @fig-texture-bias-texture-overlay-dc, except for the following; the interpolation steps are between the original image and uniformly random noise, where interpolation step 1 corresponds to half image and half noise;
* @fig-appendix-texture-color-bias-dense-correspondence-with-offset: same as @fig-color-bias-rgb-bgr-dc and @fig-texture-bias-texture-overlay-dc, except for the following; The input images are upscaled from 512\times512 to 768\times768, and the source image is cropped from (0, 0) to (512, 512), while the target image is cropped from (256, 256) to (768, 768); The dense correspondence is calculated only for the overlapping region;
* @fig-appendix-border-corner-similarity: same as @fig-border-corner-similarity;
* @fig-appendix-spatial-norm: data: \nameref{sec-datasets-imagenet-subset};
* @fig-appendix-histogram-norm-similarity: data: \nameref{sec-datasets-imagenet-subset}; Due to the large number of tokens per representation for `conv-in`, `down[0]`, `up[1]`, `up[2]`, `up[3]`, and `conv-out`, only a random subset of the pairs is shown for these blocks. Each histogram still shows at least $10^9$ pairs;
* @fig-appendix-norm-norm-scatter: data: \nameref{sec-datasets-imagenet-subset};


## Stable Diffusion Models {#sec-appendix-sd-models}

The SD series has attracted interest among researchers, artists, hobbyists, and commercial users, which lead to a large variety of different third-party finetunes and variants. This section lists and shortly describes primarily official SD variants and checkpoints. The following information is taken from @CompVis2022StableDiffusionRepo, @StabilityAI2022StableDiffusionRepo, and the respective referenced model cards on Hugging Face.

- **SD-1.1**: 237k steps at resolution 256\times256 on laion2B-en. 194k steps at resolution 512\times512 on laion-high-resolution, released by CompVis.^[SD-1.1 Hugging Face repository: [huggingface.co/CompVis/stable-diffusion-v1-1](https://huggingface.co/CompVis/stable-diffusion-v1-1)] Initial release of SD as proposed by @rombach2022highresolution.
- **SD-1.2**: Resumed from SD-1.1. 515k steps at resolution 512\times512 on laion-aesthetics v2 5+ with additional filtering. Released by CompVis.^[SD-1.2 Hugging Face repository: [huggingface.co/CompVis/stable-diffusion-v1-2](https://huggingface.co/CompVis/stable-diffusion-v1-2)]
- **SD-1.3**: Resumed from SD-1.2. 195k steps at resolution 512\times512 on "laion-aesthetics v2 5+" and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. Released by CompVis.^[SD-1.3 Hugging Face repository: [huggingface.co/CompVis/stable-diffusion-v1-3](https://huggingface.co/CompVis/stable-diffusion-v1-3)]
- **SD-1.4**: Resumed from SD-1.2. 225k steps at resolution 512\times512 on "laion-aesthetics v2 5+" and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. Released by CompVis.^[SD-1.4 Hugging Face repository: [huggingface.co/CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4)]
- **SD-1.5**: Resumed from SD-1.2. 595k steps at resolution 512\times512 on "laion-aesthetics v2 5+" and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. Released by Runway.^[SD-1.5: The original Hugging Face repository by Runway was deleted. An alternative repository can be found at [huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5](https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5)] This checkpoint is the latest of the original SD series and received the most attention from the community.
- **SD-1.6**: Announced by Stability AI, but not available as open-source.^[SD-1.6 Announcement: [platform.stability.ai/docs/release-notes#stable-image-v1-release](https://platform.stability.ai/docs/release-notes#stable-image-v1-release)]
- **SD-2.0-base**: Same number of parameters in the U-Net as SD-1.5, but uses OpenCLIP-ViT/H as the text encoder and is trained from scratch 550k steps at resolution 256\times256 on a subset of LAION-5B. Then it is further trained for 850k steps at resolution 512\times512. Released by Stability AI.^[SD-2.0 Hugging Face repository: [huggingface.co/stabilityai/stable-diffusion-2-base](https://huggingface.co/stabilityai/stable-diffusion-2-base)]
- **SD-2.0**: Resumed from SD-2.0-base. 150k steps using a v-objective [@salimans2022progressivedistillationfastsampling] on the same dataset. Resumed for another 140k steps on 768\times768 images. Released by Stability AI.^[SD-2.0 Hugging Face repository: [huggingface.co/stabilityai/stable-diffusion-2](https://huggingface.co/stabilityai/stable-diffusion-2)]
- **SD-2.1**: Resumed from SD-2.0. 55k steps of the same dataset (with `punsafe=0.1`), and then fine-tuned for another 155k extra steps (`with punsafe=0.98`). Released by Stability AI.^[SD-2.1 Hugging Face repository: [huggingface.co/stabilityai/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1)]
- **SD-Turbo**: Distilled version of SD-2.1 using adversarial diffusion distillation [@sauer2023adversarial] to generate images with only 1-4 steps required. Released by Stability AI.^[SD-Turbo Hugging Face repository: [huggingface.co/stabilityai/sd-turbo](https://huggingface.co/stabilityai/sd-turbo)]
- **SDXL**: Updated architecture with more parameters and a second text-encoder [@podell2023sdxl]. The base model can be used standalone or in an ensemble of experts, where an additional refinement model is used to improve image quality. Released by Stability AI.^[SDXL Hugging Face repository: [huggingface.co/stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)]
- **SDXL-Turbo**: Distilled version of SDXL using adversarial diffusion distillation [@sauer2023adversarial] to generate images with only 1-4 steps required. Released by Stability AI.^[SDXL-Turbo Hugging Face repository: [huggingface.co/stabilityai/sdxl-turbo](https://huggingface.co/stabilityai/sdxl-turbo)]
- **SDXL-Lightning**: Alternative distilled version of SDXL based on @lin2024sdxllightning to generate images with only 1-8 steps required. Compared with SDXL-Turbo, it has improved image size and quality. Released by ByteDance.^[SDXL-Lightning Hugging Face repository: [huggingface.co/bytedance/sdxl-lightning](https://huggingface.co/bytedance/sdxl-lightning)] The finetuning and release of this model is independent of Stability AI, so it is not part of the original SD series, but has nonetheless received a lot of attention.


\clearpage
## Position Bias {#sec-appendix-position-bias}

This sections provides additional visualizations and results for the position bias (see @sec-position-bias).

```{python}
#| label: fig-appendix-position-classifier-example
#| fig-cap: Position estimation examples for two linear probe position estimators.

# relevant notebook: representation_exploration/position_classifier.ipynb

@cache_mpl_plot()
def plot_position_classifier_accuracy():
    from sdhelper import SD
    import torch
    import numpy as np
    import datasets
    import torch.nn as nn

    # load model and dataset
    blocks = list(sd15_all_blocks.keys())
    data = datasets.load_dataset('JonasLoos/imagenet_subset', split='train')
    images = [x['image'] for x in data]
    sd = SD('SD1.5', disable_progress_bar=True)
    representations = sd.img2repr(images, blocks, 50, seed=42)

    # free (v)ram
    del sd
    if torch.cuda.empty_cache():
        torch.cuda.empty_cache()

    # train config
    batch_size = 512
    num_epochs = 5
    num_train = int(len(representations) * 0.8)

    # set seed for reproducibility
    torch.manual_seed(42)
    torch.cuda.manual_seed(42)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    # setup logging variables
    regressors = []
    classifiers = []
    accuracies_reg = torch.full((len(blocks), num_epochs), torch.nan)
    accuracies_cls = torch.full((len(blocks), num_epochs), torch.nan)

    # train models
    for block_idx, block in enumerate(blocks):

        # setup models
        _, features, w, h = representations[0][block].shape
        regressor = nn.Linear(features, 2).cuda()
        opt1 = torch.optim.Adam(regressor.parameters(), lr=1e-3)
        classifier = nn.Linear(features, w+h).cuda()
        opt2 = torch.optim.Adam(classifier.parameters(), lr=1e-3)
        regressors.append(regressor)
        classifiers.append(classifier)

        # get representations
        reprs = torch.stack([r[block].squeeze(0) for r in representations]).permute(0, 2, 3, 1).flatten(0, 2).cuda()
        labels = torch.stack(torch.meshgrid(torch.arange(w), torch.arange(h), indexing='ij'), dim=-1).expand(len(representations), -1, -1, -1).flatten(0, 2).cuda()
        reprs_train = reprs[:num_train*w*h]
        labels_train = labels[:num_train*w*h]
        reprs_test = reprs[num_train*w*h:]
        labels_test = labels[num_train*w*h:]

        # epoch loop
        for epoch in range(num_epochs):

            # train
            regressor.train()
            classifier.train()
            indices = torch.randperm(len(reprs_train))
            for i in range(0, len(reprs_train), batch_size):
                reprs_batch = reprs_train[indices[i:i+batch_size]].float()
                labels_batch = labels_train[indices[i:i+batch_size]]

                regressor.zero_grad()
                preds = regressor(reprs_batch)
                loss = nn.functional.mse_loss(preds, labels_batch.float())
                loss.backward()
                opt1.step()

                classifier.zero_grad()
                preds = classifier(reprs_batch).view(batch_size, w, 2)
                loss = nn.functional.cross_entropy(preds, labels_batch)
                loss.backward()
                opt2.step()

            # test at epoch end
            with torch.no_grad():
                regressor.eval()
                preds = regressor(reprs_test.float())
                loss_reg = nn.functional.mse_loss(preds, labels_test.float())
                acc_reg = (preds.round() == labels_test).float().mean()

                classifier.eval()
                preds = classifier(reprs_test.float()).view(len(reprs_test), w, 2)
                loss_cls = nn.functional.cross_entropy(preds, labels_test)
                acc_cls = (preds.argmax(dim=1) == labels_test).float().mean()

            # log test results
            accuracies_reg[block_idx, epoch] = acc_reg.cpu()
            accuracies_cls[block_idx, epoch] = acc_cls.cpu()

    # setup figure
    fig, axs = plt.subplots(len(sd15_all_blocks), 3, figsize=(8, 2*len(sd15_all_blocks)))

    # plot results
    for block_idx, (block, regressor, classifier) in enumerate(zip(blocks, regressors, classifiers)):
        with torch.no_grad():
            regressor.eval()
            classifier.eval()

            # Get predictions on test set
            r = representations[num_train][block].squeeze(0).permute(1,2,0)
            w, h, features = r.shape
            r = r.flatten(0, 1).float().cuda()
            preds_reg = regressor(r)
            preds_cls = classifier(r).view(len(r), w, 2)
            preds_cls = preds_cls.argmax(dim=1)

            colorwheel = torch.zeros((w, h, 3), dtype=torch.float32)
            offset = w/2 + .5
            for i in range(w):
                for j in range(h):
                    angle = torch.atan2(torch.tensor(i)-offset, torch.tensor(j)-offset)
                    dist = 1 - torch.sqrt((torch.tensor(i)-offset)**2 + (torch.tensor(j)-offset)**2) / offset / torch.sqrt(torch.tensor(2))
                    colorwheel[i, j, :] = torch.tensor([.5+.5*torch.sin(angle), .5+.5*torch.sin(angle+torch.pi/2), dist]).clamp(0, 1)

            # Plot actual vs predicted positions
            if block_idx == 0:
                axs[0, 0].set_title('True Positions')
                axs[0, 1].set_title('Classification Results')
                axs[0, 2].set_title('Regression Results')
            block_name = block.split('.')[0].replace('_blocks', '').replace('_block', '').replace('_', '-')
            axs[block_idx, 0].text(-0.1, 0.5, block_name, ha='right', va='center', transform=axs[block_idx, 0].transAxes)

            # colorwheel for reference
            # axs[0].imshow(colorwheel.permute(1, 0, 2))
            axs[block_idx, 0].scatter(np.arange(w).repeat(h), np.tile(np.arange(h), w), c=colorwheel.flatten(0, 1), s=5000/w/h, alpha=0.5, rasterized=True)
            axs[block_idx, 0].set_ylim(-0.1*w, 1.1*w)
            axs[block_idx, 0].set_xlim(-0.1*h, 1.1*h)
            axs[block_idx, 0].set_aspect('equal')
            axs[block_idx, 0].axis('off')

            # classification results
            axs[block_idx, 1].scatter(*preds_cls.T.cpu(), c=colorwheel.flatten(0, 1), s=5000/w/h, alpha=0.5, rasterized=True)
            axs[block_idx, 1].set_ylim(-0.1*w, 1.1*w)
            axs[block_idx, 1].set_xlim(-0.1*h, 1.1*h)
            axs[block_idx, 1].set_aspect('equal')
            axs[block_idx, 1].axis('off')

            # Regression results
            axs[block_idx, 2].scatter(*preds_reg.T.cpu(), c=colorwheel.flatten(0, 1), s=5000/w/h, alpha=0.5, rasterized=True)
            axs[block_idx, 2].set_ylim(-0.1*w, 1.1*w)
            axs[block_idx, 2].set_xlim(-0.1*h, 1.1*h)
            axs[block_idx, 2].set_aspect('equal')
            axs[block_idx, 2].axis('off')

    plt.tight_layout()
```

**Position Estimators Example.** @fig-appendix-position-classifier-example visualizes exemplarily the position estimation results for two linear probe position estimators are trained for 5 epochs on the SD-1.5 representations of 80% of the \nameref{sec-datasets-imagenet-subset} using Adam [@kingma2017adam]. The classification estimator has outputs for each row and column of the representation and is trained using cross-entropy loss. The regression estimator has x and y coordinates as outputs and is trained using MSE loss. The figure shows the position estimation results for one image of the test set. For this example, we use the first image of the test set and estimate the position of each token using the two estimators. Depending on the block, the estimation result is more or less accurate. The ideal position estimation would be equal to the displayed true positions. It is visible that the position estimates for the lower blocks of the U-Net are closer to the true position, while the position estimates for the higher blocks are of worse quality. Additionally, the classification estimator is able to estimate the position more accurately than the regression estimator.

```{python}
#| label: fig-appendix-dense-correspondence-flip-all-blocks
#| fig-cap: Visualization of examples for dense correspondence and the occurring error when images are flipped.

# relevant notebook: semantic-correspondence/artifical_dataset/show_flip_sc_fails.ipynb

@cache_mpl_plot()
def plot_dense_correspondence_flip(n=512, seed=42, step=50):
    from sdhelper import SD
    from PIL import ImageOps
    import numpy as np
    import torch

    sd = SD(disable_progress_bar=True)
    img_paths = [
        'assets/images/cat_next_to_house.png',
        'assets/images/cat_going_right.png',
    ]
    imgs = [Image.open(img_path).resize((n, n)) for img_path in img_paths]
    poss = [
        ['conv_in'],
        ['down_blocks[0]'],
        ['down_blocks[1]'],
        ['down_blocks[2]'],
        ['down_blocks[3]'],
        ['mid_block'],
        ['up_blocks[0]'],
        ['up_blocks[1]'],
        ['up_blocks[2]'],
        ['up_blocks[3]'],
        ['conv_out'],
    ]
    reprs = [[sd.img2repr(img, pos, step=step, seed=seed) for pos in poss] for img in imgs]
    reprs_flipped = [[sd.img2repr(ImageOps.mirror(img), pos, step=step, seed=seed) for pos in poss] for img in imgs]

    # setup colorwheel
    colorwheel = np.zeros((n, n, 3), dtype=np.uint8)
    offset = n/2 + .5
    for i in range(n):
        for j in range(n):
            angle = np.arctan2(i-offset, j-offset)
            dist = 1 - np.sqrt((i-offset)**2 + (j-offset)**2) / offset / np.sqrt(2)
            colorwheel[i, j, :] = np.array([.5+.5*np.sin(angle), .5+.5*np.sin(angle+np.pi/2), dist]) * 255

    # setup figure
    fig = plt.figure(figsize=((len(imgs)*3)*2+1, (len(poss)+2)*2))
    outer_gs = fig.add_gridspec(1, 2, wspace=0.2)
    gss = [outer_gs[i].subgridspec(len(poss)+2, 3) for i in range(len(imgs))]
    axs = [[fig.add_subplot(gss[i][j, k]) for k in range(3)] for j in range(len(poss)+2) for i in range(len(imgs))]
    axs = np.array(axs).reshape(len(poss)+2, len(imgs)*3)

    # plot original images
    for i, img in enumerate(imgs):
        axs[0, 3*i].imshow(img)
        axs[0, 3*i].axis('off')
        axs[0, 3*i].set_title('(mapped)\nimage')
        axs[0, 3*i+1].imshow(colorwheel)
        axs[0, 3*i+1].axis('off')
        axs[0, 3*i+1].set_title('(mapped)\ncolorwheel')
        axs[0, 3*i+2].imshow(np.zeros(colorwheel.shape[:2]), cmap='YlOrRd')
        axs[0, 3*i+2].axis('off')
        axs[0, 3*i+2].set_title('error')

    # plot flipped images
    for i, img in enumerate(imgs):
        axs[1, 3*i].imshow(ImageOps.mirror(img))
        axs[1, 3*i].axis('off')
        axs[1, 3*i+1].imshow(colorwheel[:, ::-1])
        axs[1, 3*i+1].axis('off')
        axs[1, 3*i+2].imshow(np.zeros(colorwheel.shape[:2]), cmap='YlOrRd')
        axs[1, 3*i+2].axis('off')

    # add horizontal line between rows 2 and 3
    fig.patches.extend([plt.Rectangle((0.08, 0.763), 0.82, 0.0015, facecolor='gray', alpha=0.4, transform=fig.transFigure)])

    # plot transferred images
    for i, img in enumerate(imgs):
        for j, pos in enumerate(poss):
            img = np.array(img)
            similarities = reprs[i][j].cosine_similarity(reprs_flipped[i][j])
            transferred_img = np.zeros_like(img)
            transferred_colorwheel = np.zeros_like(img)
            m = similarities.shape[0]
            s = n // m
            for k in range(m):
                for l in range(m):
                    argmax = similarities[:,:,k, l].flatten().argmax()
                    k_, l_ = argmax // m, argmax % m
                    transferred_img[k*s:(k+1)*s, l*s:(l+1)*s] = img[k_*s:(k_+1)*s, l_*s:(l_+1)*s][:, ::-1]  # use ::-1 to flip, and ::1 to not flip
                    transferred_colorwheel[k*s:(k+1)*s, l*s:(l+1)*s] = colorwheel[k_*s:(k_+1)*s, l_*s:(l_+1)*s][:, ::-1]

            indices = similarities.view(-1, m, m).argmax(dim=0)
            k_, l_ = indices // m, indices % m
            l_ = m - 1 - l_  # flip (mirror)
            k, l = torch.meshgrid(torch.arange(m), torch.arange(m), indexing='ij')
            errors = ((k - k_)**2 + (l - l_)**2)**.5
            all_distances = torch.cdist(*[torch.stack([k.flatten(), l.flatten()], dim=1).float()]*2)
            percentiles = (all_distances < errors.flatten().unsqueeze(1)).float().mean(dim=1).reshape(errors.shape)

            axs[j+2, 3*i].imshow(transferred_img)
            axs[j+2, 3*i].axis('off')
            axs[j+2, 3*i+1].imshow(transferred_colorwheel)
            axs[j+2, 3*i+1].axis('off')
            axs[j+2, 3*i+2].imshow(percentiles, cmap='YlOrRd', interpolation='nearest')
            axs[j+2, 3*i+2].axis('off')

    # add titles
    position_names = [pos.replace('_blocks', '').replace('_block', '').replace('_', '-') for pos, in poss]
    for i, pos in enumerate(['original', 'flipped\n(target)'] + position_names):
        axs[i, 0].text(-0.1, 0.5, pos, va='center', ha='right', transform=axs[i, 0].transAxes)
    axs[7, 0].text(-0.6, 0.5, 'blocks', va='center', ha='right', transform=axs[7, 0].transAxes, rotation=90, fontsize=14)
```

**Dense Correspondence Examples over all Blocks.** @fig-appendix-dense-correspondence-flip-all-blocks visualizes dense correspondence results for SD-1.5 over all block outputs.
The representations are extracted for an image in both original and flipped/mirrored form and then each token of the flipped representation is matched to the token with the highest cosine similarity in the original representation. The first column visualizes the mapping using the pixels of the original image, the second column shows the mapping on an image with color gradients, and the third column shows the error of the mapping. Given the space of all spatial positions $P = \{1,...,w\}\times\{1,...,h\}$, the error is computed by comparing the Euclidean distance between the original position $p_\text{src}\in P$ of the mapped tokens to the position in the original image that corresponds to the predicted position $p_\text{pred}\in P$ in the flipped image. This error is then compared to all possible errors at the current position to compute the error percentile $E$:

$$E = \frac{|\{p' \mid ||\text{flip}(p_\text{pred})-p_\text{src}|| \geq ||p'-p_\text{src}||,\ p'\in P\}|}{w\cdot h}$$

Using the error percentile as measure takes into account that the distribution of what errors are possible depends on the position in the image. This error calculation also applies to @fig-dense-correspondence-flip. As can be seen by looking at the error maps and colorwheel of @fig-appendix-dense-correspondence-flip-all-blocks, many errors are due to the representation tokens being mapped to their original absolute position, instead of to the desired flipped position. For example, this is well visible in the right image for `up[0]`, where many tokens at the left and right sides of the image are mapped to the original position. In the left image, this only really occurs in the region of the grass, where the semantics between left and right are ambiguous. For semantically different regions, the semantics seem to dominate over the positional embedding for determining the mapping. Important to note is that while the usage of absolute positional information is not desired in this task, the preservation of the position in the case of semantic ambiguity might be useful in other tasks.

```{python}
#| label: fig-appendix-sc-errors-by-relative-position-full
#| fig-cap: "Extended plots for the semantic correspondence error rate on our synthetic dataset (see @sec-position-bias) over the predicted position/distance relative to the source keypoint and the respective number of samples per pixel. The `up[1]` representations are used, except for the SDXL and SDXL-Turbo, where `up[0]` is used."
#| fig-subcap:
#| - "Error rate over the relative position to the source keypoint for SD-1.5. The center (0,0) is the location of the source keypoint."
#| - "Error rate over the distance relative to the source keypoint for different models. The size of the encoded images is 512512 px."
#| - "Number of samples per pixel over the relative position to the source keypoint for SD-1.5."
#| - "Number of samples per pixel over the distance relative to the source keypoint for different models."
#| layout: "[[40,-5,40], [40,-5,40]]"

# relevant notebook: semantic_correspondence/artifical_dataset/sc_errors_over_position.ipynb

@cache_mpl_plot
def plot_sc_errors_by_relative_position_all():
    import torch

    # load data
    errors = torch.load('assets/data/sc_errors_by_position_error_counts_SD1.5.pt', weights_only=True)
    counts = torch.load('assets/data/sc_errors_by_position_counts_SD1.5.pt', weights_only=True)

    # plot
    fig, ax = plt.subplots(figsize=(4,3))
    tmp = (errors / counts).reshape(len(errors)//4,4,len(errors)//4,4).mean(dim=(1,3))  # downscale
    im = ax.imshow(tmp)
    ax.set_aspect('equal')
    ax.set_xlabel('relative x [px]')
    ax.set_ylabel('relative y [px]')
    ticks = np.linspace(-512, 512, 5)
    tick_positions = np.linspace(0, len(tmp), 5) - 0.5
    ax.set_xticks(tick_positions)
    ax.set_xticklabels([str(int(x)) for x in ticks], fontsize=9)
    ax.set_yticks(tick_positions)
    ax.set_yticklabels([str(int(y)) for y in ticks], fontsize=9)
    plt.colorbar(im, ax=ax, label='error rate')
    plt.tight_layout()


@cache_mpl_plot
def plot_sc_errors_over_relative_diatance():
    import torch
    import matplotlib.cm as cm
    import matplotlib.colors as mcolors

    # setup distance normalization
    n = torch.load('assets/data/sc_errors_by_position_error_counts_SD1.5.pt', weights_only=True).shape[0]
    distance_map = (((torch.arange(n).unsqueeze(1) - n//2)**2 + (torch.arange(n).unsqueeze(0) - n//2)**2).float())**.5
    distance_sorting = distance_map.flatten().argsort()
    x = np.array([i**2 for i in np.linspace(1, distance_map.max()**.5, 20)])
    x_counts = np.array([0] + [(distance_map.flatten() <= x[i]).to(torch.int).sum() for i in range(len(x))])

    # plot error rate over distance
    fig, ax = plt.subplots(figsize=(4,3))
    for model in ['SD1.5', 'SD2.1', 'SD-Turbo', 'SDXL', 'SDXL-Turbo']:
        errors = torch.load(f'assets/data/sc_errors_by_position_error_counts_{model}.pt', weights_only=True)
        counts = torch.load(f'assets/data/sc_errors_by_position_counts_{model}.pt', weights_only=True)
        rate = (errors / counts).flatten()[distance_sorting]
        y_rate = np.array([rate[x_counts[i]:x_counts[i+1]].nanmean() for i in range(len(x))])
        model_name = model.replace('SD', 'SD-') if '.' in model else model
        ax.plot(2*x*512/n, y_rate, label=model_name, alpha=0.7)

    # finish plot
    ax.legend()
    ax.set_xlabel('distance [px]')
    ax.set_ylabel('error rate')
    ax.set_ylim(0.38, 0.6)
    x_width = x.max()*512*2/n
    ax.set_xlim(-x_width*0.05, x_width*1.05)
    plt.tight_layout()


@cache_mpl_plot
def plot_sc_counts_by_relative_position():
    import torch
    import numpy as np

    # load data
    counts = torch.load('assets/data/sc_errors_by_position_counts_SD1.5.pt')
    n = len(counts)

    # plot
    fig, ax = plt.subplots(figsize=(4,3))
    tmp = counts.reshape(n//4,4,n//4,4).mean(dim=(1,3))  # downsample
    im = ax.imshow(tmp*n/512/2, vmin=0)  # normalize to number of pixels
    ax.set_aspect('equal')
    ax.set_xlabel('relative x [px]')
    ax.set_ylabel('relative y [px]')
    ticks = np.linspace(-512, 512, 5)
    tick_positions = np.linspace(0, len(tmp), 5) - 0.5
    ax.set_xticks(tick_positions)
    ax.set_xticklabels([str(int(x)) for x in ticks], fontsize=9)
    ax.set_yticks(tick_positions)
    ax.set_yticklabels([str(int(y)) for y in ticks], fontsize=9)
    plt.colorbar(im, ax=ax, label='number of samples [1/px]')
    plt.tight_layout()


@cache_mpl_plot
def plot_sc_counts_over_relative_distance():
    import torch
    import matplotlib.cm as cm
    import matplotlib.colors as mcolors
    import numpy as np

    # load data
    n = torch.load('assets/data/sc_errors_by_position_error_counts_SD1.5.pt', weights_only=True).shape[0]
    distance_map = (((torch.arange(n).unsqueeze(1) - n//2)**2 + (torch.arange(n).unsqueeze(0) - n//2)**2).float())**.5
    distance_sorting = distance_map.flatten().argsort()
    x = np.array([i**2 for i in np.linspace(1, distance_map.max()**.5, 20)])
    x_counts = np.array([0] + [(distance_map.flatten() <= x[i]).to(torch.int).sum() for i in range(len(x))])
    x_ = [0] + list(x)

    # plot counts
    fig, ax = plt.subplots(figsize=(4,3))
    for model in ['SD1.5', 'SD2.1', 'SD-Turbo', 'SDXL', 'SDXL-Turbo']:
        counts = torch.load(f'assets/data/sc_errors_by_position_counts_{model}.pt', weights_only=True)
        tmp = counts.flatten()[distance_sorting]
        tmp = np.array([tmp[x_counts[i]:x_counts[i+1]].sum()/(x_[i+1]**2-x_[i]**2)/np.pi for i in range(len(x))])
        model_name = model.replace('SD', 'SD-') if '.' in model else model
        ax.plot(2*x*512/n, tmp*n/512/2, label=model_name, alpha=0.7)


    # finish up plot
    ax.legend()
    ax.set_xlabel('distance [px]')
    ax.set_ylabel('number of samples [1/px]')
    x_width = x.max()*512*2/n
    ax.set_xlim(-x_width*0.05, x_width*1.05)
    plt.tight_layout()

```

**Semantic Correspondence on Synthetic Data.** @fig-appendix-sc-errors-by-relative-position-full shows the extended plots and additional information for @fig-sc-errors-by-relative-position. In @fig-appendix-sc-errors-by-relative-position-full-1, also the outer regions of the error rate map are shown, where the number of samples is low or zero, as visualized in @fig-appendix-sc-errors-by-relative-position-full-3. Interestingly, the error rate on the very top of the error map in @fig-appendix-sc-errors-by-relative-position-full-1 is very high, which means that relatively often, source keypoints at the bottom of the image are erroneously mapped to the top of the target image. A potential reason for this could be the corner and border anomalies discussed in @sec-anomalies-corner. Due to the low number of samples in the outer regions with high distances, a few border anomalies could already cause this high error rate. Furthermore, just as in @fig-sc-errors-by-relative-position, we see an increase in error rate at the center, where the predicted keypoint is erroneously mapped to the source keypoint. In @fig-appendix-sc-errors-by-relative-position-full-2, we see the same information for different models plotted over the distance relative to the source keypoint. As before, the error rate is increased for very low and very high distances. Notably, this observation holds true for all tested models. 

```{python}
#| label: fig-appendix-sc-errors-by-relative-position-spair-maps
#| fig-cap: Semantic correspondence error rate ($1 - \text{PCK}@0.1_{\text{bbox}}$) on \nameref{sec-datasets-spair} over the relative position of the prediction to the source keypoint for different blocks and time steps. Bin size is 32\times32 px. Bright colors indicate high error rates and white indicates no samples at that relative position.

# relevant notebook: semantic_correspondence/analyze_sc_hyper_results.ipynb
# original data generated on Hydra using `semantic_correspondence/semantic_correspondence_hyperparamsearch.py` on 2024-11-24

@cache_mpl_plot
def plot_sc_errors_by_relative_position_spair_maps():
    import numpy as np

    # config used for generating the data
    lim = 256
    downscale = 32
    lim_scaled_rel = (512-lim)//downscale

    # load data
    error_rates = np.load('assets/data/sc_errors_by_relative_position_spair_maps_SD1.5.npy')
    block_names = ['conv-in', 'down[0]', 'down[1]', 'down[2]', 'down[3]', 'mid', 'up[0]', 'up[1]', 'up[2]', 'up[3]', 'conv-out']
    noise_steps = [0, 10, 25, 50, 75, 100, 150, 200, 300, 500, 800]
    n, m, *_ = error_rates.shape

    # plot
    fig, axs = plt.subplots(n, m, figsize=(m*1+2, n*1+1))
    for i, block_name in enumerate(block_names):
        axs[i,0].text(-0.8, 0.5, block_name, ha='right', va='center', transform=axs[i,0].transAxes)
        for j, noise_step in enumerate(noise_steps):
            axs[i,j].imshow(error_rates[i,j,lim_scaled_rel:-lim_scaled_rel, lim_scaled_rel:-lim_scaled_rel], origin='lower', extent=(-lim+1, lim, -lim+1, lim))
            axs[i,j].tick_params(labelsize=8)
            if i != n-1: axs[i,j].set_xticks([])
            else: axs[i,j].set_xticks([-200, 0, 200])
            if j != 0: axs[i,j].set_yticks([])
            else: axs[i,j].set_yticks([-200, 0, 200])
            if i == n-1: axs[-1,j].text(0.5, -0.5, noise_steps[j], ha='center', va='top', transform=axs[-1,j].transAxes)

    # x/y-labels
    axs[0,0].text(-1.8, -n/2, 'block | relative y [px]', ha='right', va='center', transform=axs[0,0].transAxes, fontsize=12, rotation=90)
    axs[-1,0].text(m/2+1, -1.0, 'time step | relative x [px]', ha='center', va='top', transform=axs[-1,0].transAxes, fontsize=12)
```

**Semantic Correspondence on \nameref{sec-datasets-spair}.** @fig-appendix-sc-errors-by-relative-position-spair-maps shows the semantic correspondence error rate over the relative position of the prediction to the source keypoint, similar to @fig-appendix-sc-errors-by-relative-position-full-1, but over different blocks and time steps, and on the \nameref{sec-datasets-spair} dataset instead of on our synthetic dataset. Some blocks (`down[2]` - `up[0]`) show a grid-like pattern with an increased error rate at the center, and e.g. for very high noise levels (time step 500), some blocks also show a slightly increased error rate at the center. But more generally, and especially at the more relevant `up[1]` block, we do not observe a clear increase in error rate at the center, which is a difference between our synthetic dataset and the \nameref{sec-datasets-spair} dataset. This indicates that the positional bias might be less pronounced when using real world data, such as the \nameref{sec-datasets-spair} dataset. A general observation is that the error rate tends to increase with increased distance, which is a trend that might explain the missing observation of the increase in error rate at the center. This trend is likely caused by the uneven distribution of foreground objects and keypoints in the \nameref{sec-datasets-spair} dataset, due to the use of real world images. An additional reason why our observation of the increase in error rate at the center is not visible here, is the spatial averaging used to create the error rate maps. The bin size is 32\times32 px, so smaller details might be averaged out.

```{python}
#| label: fig-appendix-sc-errors-by-relative-position-spair-lines
#| fig-cap: Semantic correspondence error rate ($1 - \text{PCK}@0.1_{\text{bbox}}$) over the relative distance between the prediction and the source keypoint for different blocks and time steps. This is computed with SD-1.5 on the \nameref{sec-datasets-spair} dataset.

# relevant notebook: semantic_correspondence/analyze_sc_hyper_results.ipynb
# original data generated on Hydra using `semantic_correspondence/semantic_correspondence_hyperparamsearch.py` on 2024-11-24

@cache_mpl_plot
def plot_sc_errors_by_relative_position_spair_lines():
    import numpy as np

    # load data
    error_rates = np.load('assets/data/sc_errors_by_relative_position_spair_lines_SD1.5.npy')
    block_names = ['conv-in', 'down[0]', 'down[1]', 'down[2]', 'down[3]', 'mid', 'up[0]', 'up[1]', 'up[2]', 'up[3]', 'conv-out']
    noise_steps = [0, 10, 25, 50, 75, 100, 150, 200, 300, 500, 800]
    n, m, x_len = error_rates.shape

    # plot
    fig, axs = plt.subplots(n, m, figsize=(m*1+2, n*1+1))
    for i, block_name in enumerate(block_names):
        axs[i,0].text(-0.8, 0.5, block_name, ha='right', va='center', transform=axs[i,0].transAxes)
        for j, noise_step in enumerate(noise_steps):
            axs[i,j].plot(np.arange(x_len)**2, error_rates[i,j,:], marker='.', markersize=3)
            axs[i,j].fill_between(np.arange(x_len)**2, error_rates[i,j,:], alpha=0.2)
            axs[i,j].set_xlim(-5, 105)
            axs[i,j].set_ylim(np.nanmin(error_rates[i,j]), 1.0)
            axs[i,j].tick_params(labelsize=8)
            for spine in ['top', 'right', 'bottom', 'left']:
                axs[i,j].spines[spine].set_visible(False)
            if i != n-1: axs[i,j].set_xticks([])
            if j != 0: axs[i,j].set_yticks([])
            if i == n-1: axs[-1,j].text(0.5, -0.5, noise_steps[j], ha='center', va='top', transform=axs[-1,j].transAxes)

    # set same ylim for each row (relative to the third lowest value)
    for i in range(n):
        ymin = 1 - (1 - sorted(ax.get_ylim()[0] for ax in axs[i,:])[2]) * 1.1
        for ax in axs[i,:]:
            ax.set_ylim(ymin, 1 + (1 - ymin) * 0.1)

    # x/y-labels
    axs[0,0].text(-1.8, -n/2, 'block | error rate', ha='right', va='center', transform=axs[0,0].transAxes, fontsize=12, rotation=90)
    axs[-1,0].text(m/2+1, -1.0, 'time step | distance [px]', ha='center', va='top', transform=axs[-1,0].transAxes, fontsize=12)
```

@fig-appendix-sc-errors-by-relative-position-spair-lines shows the same data as @fig-appendix-sc-errors-by-relative-position-spair-maps, but plotted over the distance relative to the source keypoint instead of the relative position. This plot shows that for most blocks and time steps, the error rate is increased for very low distances and then drops sharply. However, the general trend is an increase in error rate with increased distance, which we already observed in the previous plot. The initial spike in error rate for very low distances shows that the positional embedding also causes issues when using the \nameref{sec-datasets-spair} dataset, and not only for our synthetic dataset. However, as it doesn't show up a in the error rate map in @fig-appendix-sc-errors-by-relative-position-spair-maps, the phenomenon seems to be less pronounced than in our synthetic dataset.


\clearpage
## Texture and Color Bias {#sec-appendix-texture-color-bias}

This section provides additional examples and information related to the texture and color bias experiments in @sec-texture-color-bias.

```{python}
#| label: fig-appendix-texture-color-bias-examples-1
#| fig-cap: Examples of texture and color bias in the representations of `conv-in` to `mid`. Continued in @fig-appendix-texture-color-bias-examples-2.

# The plotting function is defined in fig-texture-color-bias-examples

@cache_mpl_plot(watch=[plot_texture_color_bias_examples_general])
def plot_texture_color_bias_examples_appendix_1():
    plot_texture_color_bias_examples_general(['conv_in', 'down_blocks[0]', 'down_blocks[1]', 'down_blocks[2]', 'down_blocks[3]', 'mid_block'])
```

```{python}
#| label: fig-appendix-texture-color-bias-examples-2
#| fig-cap: Examples of texture and color bias in the representations of `up[0]` to `conv-out`. For each representation token from the respective block for the target image, the most similar representation token from the source is selected using cosine similarity. This mapping is visualized by transferring the image pixels of the source image (columns 1, 3, 5) and pixels of the colorwheel (columns 2, 4, 6) from their source location to the location defined by the mapping.

@cache_mpl_plot(watch=[plot_texture_color_bias_examples_general])
def plot_texture_color_bias_examples_appendix_2():
    plot_texture_color_bias_examples_general(['up_blocks[0]', 'up_blocks[1]', 'up_blocks[2]', 'up_blocks[3]', 'conv_out'])
```

**Dense Correspondence Examples for All Blocks.** @fig-appendix-texture-color-bias-examples-1 and @fig-appendix-texture-color-bias-examples-2 visualize the texture and color bias in all blocks of SD-1.5, compared to only a few in @fig-texture-color-bias-examples. As discussed in @sec-texture-color-bias, we can observe a higher color and texture bias in the upper blocks (`conv-in`, `down[0]`, `down[1]`, `up[1]`, `up[2]`, `up[3]`), and less so in the lower blocks (`down[2]`, `down[3]`, `mid`, `up[0]`). The separation into upper and lower blocks is not a clear-cut, but rather a trend. `conv-out` is, as expected, not at all usable for dense correspondence, as explained in @sec-methods-representation-extraction.

```{python}
#| label: fig-appendix-texture-bias-dense-correspondence-blur
#| fig-cap: Dense correspondence results on the \nameref{sec-datasets-imagenet-subset}, see (b), when blurring the image as visualized in (a). The radius of the Gaussian blur is gradually increased from 0 to 16 pixels. The ideal mapping (100% accuracy) would be if for each token in the target image, the token with the highest cosine similarity in the source image is at the same spatial location.
#| fig-subcap:
#| - Example of the gradual blurring of the image
#| - Accuracy, and change relative to the initial accuracy, for dense correspondence

@cache_mpl_plot(watch=[plot_color_texture_bias_dc_example_general])
def plot_texture_bias_blur_example():
    import numpy as np
    from PIL import ImageFilter
    plot_color_texture_bias_dc_example_general(lambda arr, x: np.array(Image.fromarray(arr).filter(ImageFilter.GaussianBlur(radius=16 * x))))


@cache_mpl_plot(watch=[plot_color_texture_bias_dc_accuracy_general])
def plot_texture_bias_blur_dense_correspondence():
    plot_color_texture_bias_dc_accuracy_general('assets/cached_data/texture_bias_dense_correspondence_blur_accuracies.npy')
```

```{python}
#| label: fig-appendix-texture-bias-dense-correspondence-noise
#| fig-cap: Dense correspondence results on the \nameref{sec-datasets-imagenet-subset}, see (b), when noising the image as visualized in (a). The interpolation step $i$ indicates the amount of noise from no noise to 50% uniform noise, i.e. $x' = (1-\frac{i}{2})\cdot x + \frac{i}{2}\cdot n$ where $x$ is the original image and $n\sim U[0,255)^{512\times512\times3}$. The ideal mapping (100% accuracy) would be if for each token in the target image, the token with the highest cosine similarity in the source image is at the same spatial location.
#| fig-subcap:
#| - Example of gradual noising of the image
#| - Accuracy, and change relative to the initial accuracy, for dense correspondence

@cache_mpl_plot(watch=[plot_color_texture_bias_dc_example_general])
def plot_texture_bias_noise_example():
    import numpy as np
    plot_color_texture_bias_dc_example_general(lambda arr, x: x/2 * np.random.rand(*arr.shape)*255 + (1-x/2) * arr)


@cache_mpl_plot(watch=[plot_color_texture_bias_dc_accuracy_general])
def plot_texture_bias_noise_dense_correspondence():
    plot_color_texture_bias_dc_accuracy_general('assets/cached_data/texture_bias_dense_correspondence_noise_accuracies.npy')
```

**Dense Correspondence for Blurring and Noising.** @fig-appendix-texture-bias-dense-correspondence-blur and @fig-appendix-texture-bias-dense-correspondence-noise show that the dense correspondence accuracy decreases when the image is blurred or noised, with the decrease being more pronounced for the upper blocks. Especially blurring the image primarily degrades fine texture details, while only slightly impacting color, thus showing texture bias somewhat independently of color bias.

```{python}
#| label: fig-appendix-texture-color-bias-dense-correspondence-with-offset
#| fig-cap: Dense correspondence results on the \nameref{sec-datasets-imagenet-subset}, similar to @fig-color-bias-rgb-bgr-dc and @fig-texture-bias-texture-overlay-dc. The difference is that the source and target images are crops of the original images, where the source image is in the top left and the target image in the bottom right, with an overlap in the middle. The dense correspondence accuracy is computed only for the overlapping region.
#| fig-subcap:
#| - Color channel permutation (RGB $\rightarrow$ BGR)
#| - Texture overlay

@cache_mpl_plot
def plot_color_bias_rgb_bgr_dense_correspondence_offset():

    # load data
    accuracies = np.load('assets/cached_data/color_bias_dense_correspondence_rgb_bgr_accuracies_offset.npy')
    block_names = [b.replace('_blocks', '').replace('_block', '').replace('_', '-') for b in sd15_all_blocks.keys()][:-1]
    n, m, _ = accuracies.shape

    # plot accuracy
    fig, axs = plt.subplots(1, 2, figsize=(8, 2.5))
    colors = plt.cm.rainbow(np.linspace(0, 1, n))
    for block_idx, (block, color) in enumerate(zip(block_names, colors)):
        axs[0].plot(np.linspace(0, 1, m), accuracies[block_idx].mean(axis=1)*100, label=block, color=color)
    axs[0].set_xlabel('interpolation step')
    axs[0].set_ylabel('accuracy [%]')

    # plot relative change in accuracy
    for block_idx, (block, color) in enumerate(zip(block_names, colors)):
        init_acc = accuracies[block_idx, 0].mean()
        acc_change = (accuracies[block_idx].mean(axis=1) - init_acc) / init_acc
        axs[1].plot(np.linspace(0, 1, m), acc_change*100, label=block, color=color)
    axs[1].set_xlabel('interpolation step')
    axs[1].set_ylabel('rel. change in accuracy [%]')
    axs[1].legend(bbox_to_anchor=(1.01, 1.05), loc='upper left', fontsize=10)

    plt.tight_layout()


@cache_mpl_plot
def plot_texture_bias_texture_overlay_dense_correspondence_offset():

    # load data
    accuracies = np.load('assets/cached_data/texture_bias_dense_correspondence_texture_overlay_accuracies_offset.npy')
    block_names = [b.replace('_blocks', '').replace('_block', '').replace('_', '-') for b in sd15_all_blocks.keys()][:-1]
    n, m, _ = accuracies.shape

    # plot accuracy
    fig, axs = plt.subplots(1, 2, figsize=(8, 2.5))
    colors = plt.cm.rainbow(np.linspace(0, 1, n))
    for block_idx, (block, color) in enumerate(zip(block_names, colors)):
        axs[0].plot(np.linspace(0, 0.5, m), accuracies[block_idx].mean(axis=1)*100, label=block, color=color)
    axs[0].set_xlabel('interpolation step')
    axs[0].set_ylabel('accuracy [%]')

    # plot relative change in accuracy
    for block_idx, (block, color) in enumerate(zip(block_names, colors)):
        init_acc = accuracies[block_idx, 0].mean()
        acc_change = (accuracies[block_idx].mean(axis=1) - init_acc) / init_acc
        axs[1].plot(np.linspace(0, 0.5, m), acc_change*100, label=block, color=color)
    axs[1].set_xlabel('interpolation step')
    axs[1].set_ylabel('rel. change in accuracy [%]')
    axs[1].legend(bbox_to_anchor=(1.01, 1.05), loc='upper left', fontsize=10)

    plt.tight_layout()

```

**Dense Correspondence over Differently Cropped Images.** In @fig-appendix-texture-color-bias-dense-correspondence-with-offset, we can see the dense correspondence accuracy when a transform is applied to the image, for @fig-appendix-texture-color-bias-dense-correspondence-with-offset-1, the color channels are permuted (just as in @fig-color-bias-rgb-bgr-dc), while for @fig-appendix-texture-color-bias-dense-correspondence-with-offset-2, an image is overlaid (just as in @fig-texture-bias-texture-overlay-dc). Additionally, the source and target images are cropped versions of the original images, where the source image is in the top left and the target image in the bottom right, with an overlap in the middle.
When comparing @fig-appendix-texture-color-bias-dense-correspondence-with-offset-1 with @fig-color-bias-rgb-bgr-dc, we see that the accuracy is decreased for some blocks, most visibly for `up[0]`. This was to be expected, as the absolute positional embedding (discussed in @sec-position-bias) is not directly helpful for this dense correspondence task anymore. The relative change in accuracy, however, is very similar, indicating that the insight of lower color bias in the lower blocks might be independent of positional embedding. When comparing the dense correspondence results for the texture overlay in @fig-appendix-texture-color-bias-dense-correspondence-with-offset-2 with @fig-texture-bias-texture-overlay-dc, a similar picture emerges, with partly lower accuracy, and a similar trend for the relative change in accuracy. In conclusion, @fig-appendix-texture-color-bias-dense-correspondence-with-offset supports the observation of color and texture bias in the upper blocks and suggests that the cause of the better performance of the lower blocks cannot be fully explained by positional embedding.


\clearpage
## Corner and Border Anomalies {#sec-appendix-corner-anomalies}

@fig-appendix-border-corner-similarity shows the relative cosine similarities of the corners, borders, and other tokens in the representations of different models over the blocks. The results for SD-1.5 are shown and described in @fig-border-corner-similarity, in @sec-anomalies-corner. The results for SD-2.1, SD-Turbo, SDXL, and SDXL-Turbo paint an overall similar picture as the results for SD-1.5, but with some differences. The similarity among corner tokens is generally higher than the similarity among all tokens. For SD-2.1 and SD-Turbo, the `up[0]` representations are a notable outlier, where this trend does not hold, but where the similarity is instead significantly decreased.

::: {#fig-appendix-border-corner-similarity layout="[[48,-4,48],[48,-4,48]]"}

```{python}
#| label: fig-appendix-border-corner-similarity-over-blocks-sd21
#| fig-cap: SD-2.1

@cache_mpl_plot(watch=[plot_border_corner_similarity_general])
def plot_border_corner_similarity_sd21():
    plot_border_corner_similarity_general('sd21')
```

```{python}
#| label: fig-appendix-border-corner-similarity-over-blocks-sd-turbo
#| fig-cap: SD-Turbo

@cache_mpl_plot(watch=[plot_border_corner_similarity_general])
def plot_border_corner_similarity_sdturbo():
    plot_border_corner_similarity_general('sd-turbo')
```

```{python}
#| label: fig-appendix-border-corner-similarity-over-blocks-sdxl
#| fig-cap: SDXL

@cache_mpl_plot(watch=[plot_border_corner_similarity_general])
def plot_border_corner_similarity_sdxl():
    plot_border_corner_similarity_general('sdxl')
```

```{python}
#| label: fig-appendix-border-corner-similarity-over-blocks-sdxl-turbo
#| fig-cap: SDXL-Turbo

@cache_mpl_plot(watch=[plot_border_corner_similarity_general])
def plot_border_corner_similarity_sdxl_turbo():
    plot_border_corner_similarity_general('sdxl-turbo')
```

Relative cosine similarities in the token groups containing the corners, the borders, or the other tokens. A value above 1 indicates increased similarities among a token group, when the corners/borders are already in the corners/borders during representation extraction.

:::


\clearpage
## Representation Norms {#sec-appendix-repr-norms}

Investigating the norms of the extracted representations can provide insights into the representations and help to identify anomalies or interesting features. There are different norms that can be calculated for a given token, such as the Manhattan norm (L1), the Euclidean norm (L2), and maximum norm (L$\infty$), as described in @sec-representation-similarities. We find similar results for these norms, and display the results for the L2 norm in the following figures.

**Norm Maps.** In @fig-appendix-spatial-norm, which shows the average norms over the 500 images of the \nameref{sec-datasets-imagenet-subset}, one can see that for many blocks and models, the border and/or the corners have a visibly higher or lower norm. For example, in `down[1]`, the top right corner in has a unusually high norm, or in `up[0]`, `up[1]`, and `up[2]` the borders and especially the corners have a relatively low norm. Also visible is a general trend of increasing norms towards the center, which is, however, most likely due to the tendency of foreground object having high norms and being located in the center. In contrast to this, the observations for borders and corners do not obviously correspond to some semantic meaning.

```{python}
#| label: fig-appendix-spatial-norm
#| fig-cap: L2 norm of the representations of different models and blocks. Averaged over the 500 images of the \nameref{sec-datasets-imagenet-subset}. Darker colors indicate higher norms.

# Could do: maybe put it together with next plot on the same page

@cache_mpl_plot
def plot_spatial_norm():
    import pickle

    # load data
    models = ['SD-1.5', 'SD-2.1', 'SD-Turbo']
    blocks = {
        'conv-in': 'conv_in',
        'down[0]': 'down_blocks[0]',
        'down[1]': 'down_blocks[1]',
        'down[2]': 'down_blocks[2]',
        'down[3]': 'down_blocks[3]',
        'mid': 'mid_block',
        'up[0]': 'up_blocks[0]',
        'up[1]': 'up_blocks[1]',
        'up[2]': 'up_blocks[2]',
        'up[3]': 'up_blocks[3]',
        'conv-out': 'conv_out',
    }
    with open('assets/cached_data/representation_norms.pkl', 'rb') as f:
        norms = pickle.load(f)
    assert all(m in norms for m in models), f'Missing models: {set(models) - set(norms.keys())}'

    # plot norms
    fig, axs = plt.subplots(len(blocks), len(models), figsize=(len(models)*0.8+0.5, len(blocks)*0.8))
    for i, (block_name, block) in enumerate(blocks.items()):
        axs[i, 0].text(-0.1, 0.5, block_name, va='center', ha='right', transform=axs[i, 0].transAxes, rotation='vertical', fontsize=8)
        for j, model in enumerate(models):
            if i==0: axs[i, j].set_title(model, fontsize=8)
            if model in norms:
                axs[i, j].imshow(norms[model][block], cmap='YlOrRd', aspect='equal')
            axs[i, j].axis('off')

    plt.tight_layout()
    plt.subplots_adjust(hspace=0.1, wspace=0.1)
```

**Norm Distributions.** @fig-appendix-histogram-norm-similarity shows the norm distributions for different blocks and time steps. They significantly differ between blocks, but mostly stay similar for different time steps. In the norm distributions of `up[1]` and `up[2]`, the number of outliers increases with higher time steps, which might be caused by the high-norm anomalies in the `up[1]` and `up[2]` blocks (see @sec-anomalies-high-norm).

```{python}
#| label: fig-appendix-histogram-norm-similarity
#| fig-cap: 2D-Histograms of the relative frequency of token pairs in \nameref{sec-datasets-imagenet-subset} over their L2 norm and cosine similarity.

# relevant notes: 2024-10-02

@cache_mpl_plot
def plot_histogram_norm_similarity(blocks_slice=slice(None)):
    import torch
    from matplotlib.colors import LogNorm

    histograms = torch.load('assets/cached_data/histograms_norm_cossim_sd15.pt')
    limits = torch.load('assets/cached_data/histograms_norm_cossim_sd15_limits.pt')
    target_limits = [60, 100, 220, 220, 220, 380, 200, 1400, 420, 200, 6]
    blocks = ['conv_in','down_blocks[0]','down_blocks[1]','down_blocks[2]','down_blocks[3]','mid_block','up_blocks[0]','up_blocks[1]','up_blocks[2]','up_blocks[3]','conv_out'][blocks_slice]

    plt_size = 1.0
    fig, axs = plt.subplots(len(blocks), 3+1, figsize=(3*plt_size+1.7, len(blocks)*plt_size+0.5), width_ratios=[1]*3 + [0.1])
    for i, (block, target_limit) in enumerate(zip(blocks, target_limits)):
        block_name = block.replace('_blocks', '').replace('_block', '').replace('_', '-')
        axs[i, 0].text(-1.6, target_limit/2, block_name, va='center', ha='right', fontsize=10)
        for j, noise_level in enumerate([0,50,200]):
            x_min, x_max = -0.5, 1.0
            y_min, y_max = limits[i][j]
            histogram = histograms[i][j]
            if histogram is None: continue
            axs[i, j].imshow(histogram/histogram.sum(), norm=LogNorm(vmin=1e-12, vmax=1e-2), cmap='YlOrRd', extent=(x_min, x_max, y_min, y_max), aspect='auto')
            axs[i, j].set_box_aspect(1)
            axs[i, j].set_ylim(0, target_limit)
            axs[i, j].set_xlim(x_min, x_max)
            axs[i, j].tick_params(labelsize=8)
            if j == 0: axs[i, j].set_ylabel('Norm', fontsize=9)
            else: axs[i, j].set_yticks([])
            if i == len(blocks)-1: axs[i, j].set_xlabel('Cosine\nSimilarity', fontsize=9)
            else: axs[i, j].set_xticks([])
            if i == 0: axs[i, j].set_title(f'time step {noise_level}', fontsize=9)
        sm = plt.cm.ScalarMappable(cmap='YlOrRd', norm=LogNorm(vmin=1e-12, vmax=1e-2))
        sm.set_array([])
        cbar = fig.colorbar(sm, cax=axs[i, 3], aspect=1, pad=0.1)
        # cbar.set_label('Rel. Frequency')
        cbar.ax.set_box_aspect(10)
        cbar.ax.tick_params(labelsize=8)

    plt.tight_layout()
    plt.subplots_adjust(hspace=0.1, wspace=0.1, left=0.265)  # left padding is to prevent cut-off text
```

**Similarity Distribution over Token Norms.** @fig-appendix-norm-norm-scatter shows the L2 norms of `up[1]` representation tokens of the \nameref{sec-datasets-imagenet-subset} in comparison with the norms of their most similar matches, by cosine similarity. Additionally, we plot the pairs of the tokens, where the first token has the highest norm in the corresponding representation. This scatter plot is accompanied by a histogram visualizing the distribution of the norms for both all pairs and the highest norm pairs.
We find a quite varied and spread out distribution for the group of all pairs. However, for the highest norm tokens, this distribution is heavily biased towards other very high-norm tokens. This means that the tokens with the highest norm in a representation have often the highest cosine similarity to other tokens with a high norm. This effect matches well to the observation of high norm anomalies with non-semantic cosine similarities between each other (see @sec-anomalies-high-norm).

```{python}
#| label: fig-appendix-norm-norm-scatter
#| fig-cap: Scatter plot of the L2 norms of tokens and the norm of their most similar matches. A random subset ($10^4$) of all possible pairs of tokens in the `up[1]` representations of the 500 images of the \nameref{sec-datasets-imagenet-subset} is shown in blue and all pairs with the highest norm token are shown in orange.

# relevant notebook: representation_exploration/norm-anomalies-search.ipynb

@cache_mpl_plot
def plot_norm_norm_scatter():
    from datasets import load_dataset
    from sdhelper import SD
    import torch

    # load data, extract representations, and calculate norms
    block = 'up_blocks[1]'
    dataset = [x['image'] for x in load_dataset('JonasLoos/imagenet_subset', split='train')]
    sd = SD('sd15', disable_progress_bar=True)
    representations = sd.img2repr(dataset, extract_positions=[block], step=50, seed=42)
    reprs_tmp = torch.stack([r[block].squeeze(0).flatten(start_dim=1).T for r in representations]).to('cuda')
    norms = reprs_tmp.norm(dim=2).cpu()

    # calculate (selection of) all norms
    x_all, y_all = [], []
    indices = torch.randint(reprs_tmp.shape[0]*reprs_tmp.shape[1], (10000,), generator=torch.Generator().manual_seed(42))
    for i in indices:
        others = reprs_tmp[torch.arange(reprs_tmp.shape[0])!=i//reprs_tmp.shape[1]].flatten(0,1)
        argmax = torch.cosine_similarity(reprs_tmp.flatten(0,1)[i][None, :], others, dim=1).argmax().cpu()
        x_all.append(norms.flatten()[i] / norms[i//norms.shape[1]].max())
        y_all.append((norms.flatten()[argmax] / norms[argmax//norms.shape[1],:].max().item()))

    # calculate highest norms
    x_highest, y_highest = [], []
    for i in range(len(reprs_tmp)):
        j = norms[i].argmax()
        others = reprs_tmp[torch.arange(reprs_tmp.shape[0])!=i].flatten(0,1)
        argmax = torch.cosine_similarity(reprs_tmp[i][j][None, :], others, dim=1).argmax().cpu()
        x_highest.append(1)
        y_highest.append(norms.flatten()[argmax] / norms[argmax//norms.shape[1],:].max().item())

    # plot
    axs = plt.subplots(1, 2, figsize=(4.5, 3.2), width_ratios=[2, 1])[1]
    axs[0].scatter(x_all, y_all, alpha=0.25, s=2, rasterized=True)
    axs[1].hist(y_all, bins=50, orientation='horizontal', density=True)
    axs[0].scatter(x_highest, y_highest, alpha=0.25, s=2, rasterized=True)
    axs[1].hist(y_highest, bins=50, orientation='horizontal', density=True, alpha=0.5)

    # configure plot
    axs[0].set_aspect('equal')
    axs[0].set_xlabel('Relative Norm of first', fontsize=9)
    axs[0].set_ylabel('Relative Norm of second', fontsize=9)
    axs[0].set_ylim(0, 1.05)
    axs[0].set_xlim(0, 1.05)
    axs[0].tick_params(labelsize=8)
    axs[0].set_title('Norms of tokens and\ntheir most similar matches', fontsize=9)
    axs[0].legend(['all', 'highest norm only'], fontsize=9)
    axs[1].set_xlabel('Relative Frequency', fontsize=9)
    axs[1].set_yticks([])
    axs[1].set_ylim(0, 1.05)
    axs[1].tick_params(labelsize=8)
    axs[1].set_title('Distribution of norms', fontsize=9)
    plt.tight_layout()
```


```{python}
#| output: false
clear_unused_cached_figures()
```
