{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdhelper import SD\n",
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "from matplotlib import pyplot as plt\n",
    "import PIL.Image\n",
    "from trainplot.trainplot import TrainPlotPlotlyExperimental as TrainPlot\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import Literal, Callable\n",
    "from functools import partial, cache\n",
    "ceil = lambda x: int(np.ceil(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SD('sd1.5')\n",
    "\n",
    "print(f'Using {sd.model_name} model, available extract positions: {sd.available_extract_positions}')\n",
    "dataset_pairs: datasets.Dataset = datasets.load_dataset('0jl/SPair-71k', trust_remote_code=True, split='test')\n",
    "repr_dataset_name = f'{sd.model_name}-SPair-71k-repr'\n",
    "\n",
    "def expand_and_resize(x: PIL.Image.Image, size = 960, border_pad=True):\n",
    "    n, m = x.size\n",
    "    s = max(n, m)\n",
    "    r = PIL.Image.new('RGB', (s, s))\n",
    "    r.paste(x, ((s-n)//2, (s-m)//2))\n",
    "    if border_pad:\n",
    "        # pad with border\n",
    "        if n > m:\n",
    "            r.paste(x.crop((0, 0, n, 1)).resize((n,(s-m)//2)), (0, 0))\n",
    "            r.paste(x.crop((0, m-1, n, m)).resize((n,(s-m)//2)), (0, m+(s-m)//2))\n",
    "        elif m > n:\n",
    "            r.paste(x.crop((0, 0, 1, m)).resize(((s-n)//2,m)), (0, 0))\n",
    "            r.paste(x.crop((n-1, 0, n, m)).resize(((s-n)//2,m)), (n+(s-n)//2, 0))\n",
    "    return r.resize((size, size))\n",
    "\n",
    "def expand_and_resize_keypoint(x, y, n, m, o, p):\n",
    "    s = max(n, m)\n",
    "    return (x + (s-n)//2) * o / s, (y + (s-m)//2) * p / s\n",
    "\n",
    "def expand(x: PIL.Image.Image, size = 960):\n",
    "    factor = size / min(x.size)\n",
    "    return x.resize((int(x.size[0]*factor), int(x.size[1]*factor)))\n",
    "\n",
    "def expand_keypoint(x, y, n, m, o, p):\n",
    "    return x * o / n, y * p / m\n",
    "\n",
    "def get_transforms(name, size=960):\n",
    "    match name:\n",
    "        case 'expand_and_resize':\n",
    "            return partial(expand_and_resize, size=size), expand_and_resize_keypoint\n",
    "        case 'expand':\n",
    "            return partial(expand, size=size), expand_keypoint\n",
    "        case None:\n",
    "            return lambda x, *args, **kwargs: x, lambda x, y, n, m, o, p: (x, y)\n",
    "        case _:\n",
    "            raise ValueError(f'Unknown transform name: {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache\n",
    "def get_empty_repr(pos: list[str], size: tuple[int, int]):\n",
    "    empty_reprs = []\n",
    "    for i in range(20):\n",
    "        empty_repr = sd.img2repr(PIL.Image.new('RGB', size), pos, 999, output_device='cpu')\n",
    "        assert isinstance(empty_repr, dict)\n",
    "        empty_reprs.append(empty_repr)\n",
    "    return {k: torch.mean(torch.stack([x[k] for x in empty_reprs]), dim=0) for k in empty_reprs[0]}\n",
    "\n",
    "\n",
    "\n",
    "def get_zoomed_reprs(img: PIL.Image.Image, pos: list[str], step: int = 1):\n",
    "    reprs = []\n",
    "    for zoom in range(-6,7):\n",
    "        zoom_factor = 2**(zoom/2)\n",
    "        zoomed_img = img.resize((int(img.size[0]*zoom_factor), int(img.size[1]*zoom_factor)))\n",
    "        zoomed_repr = sd.img2repr(zoomed_img, pos, step, output_device='cpu')\n",
    "        assert isinstance(zoomed_repr, dict)\n",
    "        reprs.append(concat_reprs(zoomed_repr))\n",
    "    return reprs\n",
    "\n",
    "\n",
    "\n",
    "shifted_reprs_cache = {}\n",
    "def get_shifted_reprs(img: PIL.Image.Image, pos: list[str], step: int, skip: int = 1, smothing=False) -> torch.Tensor:\n",
    "\n",
    "    # get representation sample to get representation sizes\n",
    "    sample_repr = sd.img2repr(img, pos, step, output_device='cpu')\n",
    "    assert isinstance(sample_repr, dict)\n",
    "    m,n = img.size\n",
    "\n",
    "    # create big image\n",
    "    img_arr = np.array(img)\n",
    "    big_img = np.zeros((n*3, m*3, 3), dtype=np.uint8)\n",
    "    big_img[n:2*n, m:2*m] = img_arr\n",
    "\n",
    "    # border pad big image\n",
    "    big_img[:n, :m] = img_arr[0, 0]  # top left\n",
    "    big_img[:n, 2*m:] = img_arr[0, -1]  # top right\n",
    "    big_img[2*n:, :m] = img_arr[-1, 0]  # bottom left\n",
    "    big_img[2*n:, 2*m:] = img_arr[-1, -1]  # bottom right\n",
    "    big_img[:n, m:2*m] = img_arr[None,0,:]  # top\n",
    "    big_img[2*n:, m:2*m] = img_arr[None,-1,:]  # bottom\n",
    "    big_img[n:2*n, :m] = img_arr[:,0,None]  # left\n",
    "    big_img[n:2*n, 2*m:] = img_arr[:,-1,None]  # right\n",
    "\n",
    "    # setup result tensor\n",
    "    result = torch.zeros((sum(sample_repr[x].shape[1] for x in pos), n, m))\n",
    "    \n",
    "    # shift image and calculate representations\n",
    "    i_channel = 0\n",
    "    for p in pos:\n",
    "        tile_size = min(*2**np.arange(0, 8), key=lambda x: abs(x - n // sample_repr[p].shape[-2]))  # get closest power of 2\n",
    "        num_channels = sample_repr[p].shape[1]\n",
    "        for i in range(skip//2, tile_size, skip):\n",
    "            for j in range(skip//2, tile_size, skip):\n",
    "                x = i - tile_size // 2\n",
    "                y = j - tile_size // 2\n",
    "                shifted_img = big_img[n+x:n+x+n, m+y:m+y+m]\n",
    "                shifted_repr = sd.img2repr(shifted_img, [p], step, output_device='cpu')[p][0]\n",
    "                x_shape = min(ceil((n-i)/tile_size), shifted_repr.shape[1])\n",
    "                y_shape = min(ceil((m-j)/tile_size), shifted_repr.shape[2])\n",
    "                result[i_channel:i_channel+num_channels, i:x_shape*tile_size:tile_size, j:y_shape*tile_size:tile_size] = shifted_repr[:, :x_shape, :y_shape]\n",
    "        i_channel += num_channels\n",
    "\n",
    "    result = result[:,skip//2::skip,skip//2::skip]\n",
    "\n",
    "    if smothing:\n",
    "        # gaussian smothing\n",
    "        kernel = torch.tensor([[1, 2, 1], [2, 4, 2], [1, 2, 1]], dtype=torch.float32, device=result.device) / 16\n",
    "        result = torch.nn.functional.conv2d(result[None], kernel)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def concat_reprs(reprs: dict[str, torch.Tensor]):\n",
    "    '''Concatenate representations with different spatial sizes into a single tensor with the largest spatial size.'''\n",
    "    # If the representation sizes are not multiples of each other, the bottom and right edges of the spatially larger representations will be 0-padded.\n",
    "    max_spatial = np.array(max(x.shape[-2:] for x in reprs.values()))\n",
    "    min_spatial = np.array(min(x.shape[-2:] for x in reprs.values()))\n",
    "    while (max_spatial > min_spatial).any(): min_spatial *= 2\n",
    "    spatial = min_spatial\n",
    "    num_features1 = sum(x.shape[1] for x in reprs.values())\n",
    "    repr_full = torch.zeros((num_features1, *spatial), device=sd.device)\n",
    "    i = 0\n",
    "    for p in reprs:\n",
    "        r1 = reprs[p]\n",
    "        _, num_channels1, n1, m1 = r1.shape\n",
    "        tmp1 = r1.repeat_interleave(spatial[0]//n1, dim=-2).repeat_interleave(spatial[1]//m1, dim=-1)\n",
    "        repr_full[i:i+num_channels1, :tmp1.shape[-2], :tmp1.shape[-1]] = tmp1.to(repr_full.device)\n",
    "        i += num_channels1\n",
    "    return repr_full\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sc(\n",
    "        sample: dict,\n",
    "        plot: bool = True,\n",
    "        precomputed_reprs: list | None = None,\n",
    "        extraction_step: int = 1,\n",
    "        pos: list[str] = ['up_blocks[1]'],\n",
    "        transform: Callable = lambda x: x,\n",
    "        transform_keypoint: Callable = lambda x, y, *_: (x, y),\n",
    "        subtract_empty_repr: bool = False,\n",
    "        shift_reprs: bool = False,\n",
    "        shift_skip: int = 8,\n",
    "        zoom_reprs: bool = False,\n",
    "    ):\n",
    "    '''Solve semantic correspondence for a single sample.'''\n",
    "\n",
    "    if shift_reprs:\n",
    "        assert precomputed_reprs is None\n",
    "        assert not subtract_empty_repr\n",
    "        if sample['src_name'] not in shifted_reprs_cache:\n",
    "            shifted_reprs_cache[sample['src_name']] = get_shifted_reprs(transform(sample['src_img']), pos, extraction_step, shift_skip)\n",
    "        repr1_full = shifted_reprs_cache[sample['src_name']]\n",
    "        if sample['trg_name'] not in shifted_reprs_cache:\n",
    "            shifted_reprs_cache[sample['trg_name']] = get_shifted_reprs(transform(sample['trg_img']), pos, extraction_step, shift_skip)\n",
    "        repr2_full = shifted_reprs_cache[sample['trg_name']]\n",
    "    else:\n",
    "        # load representations\n",
    "        if precomputed_reprs is not None:\n",
    "            repr1 = precomputed_reprs[sample['src_data_index']]\n",
    "            repr2 = precomputed_reprs[sample['trg_data_index']]\n",
    "            assert set(repr1.keys()) == set(repr2.keys())\n",
    "            pos = list(repr1.keys())\n",
    "        else:\n",
    "            category = dataset_pairs.features['category'].names[sample['category']]\n",
    "            repr1 = sd.img2repr(transform(sample['src_img']), extract_positions=pos, step=extraction_step, prompt=category)\n",
    "            repr2 = sd.img2repr(transform(sample['trg_img']), extract_positions=pos, step=extraction_step, prompt=category)\n",
    "        assert isinstance(repr1, dict) and isinstance(repr2, dict)\n",
    "\n",
    "        # concatenate representations\n",
    "        repr1_full = concat_reprs(repr1)\n",
    "        repr2_full = concat_reprs(repr2)\n",
    "        if subtract_empty_repr:\n",
    "            repr1_empty = concat_reprs(get_empty_repr(tuple(pos), transform(sample['src_img']).size))\n",
    "            repr2_empty = concat_reprs(get_empty_repr(tuple(pos), transform(sample['trg_img']).size))\n",
    "            repr1_full = repr1_full - repr1_empty\n",
    "            repr2_full = repr2_full - repr2_empty\n",
    "\n",
    "    # get images\n",
    "    src_img = transform(sample['src_img'])\n",
    "    trg_img = transform(sample['trg_img'])\n",
    "    sn, sm = src_img.size\n",
    "    tn, tm = trg_img.size\n",
    "    assert len(sample['src_kps']) == len(sample['trg_kps'])\n",
    "\n",
    "    # get bounding box\n",
    "    sbb = np.array(sample['src_bndbox'])\n",
    "    sbb[:2] = transform_keypoint(*sbb[:2], *sample['src_img'].size, sn, sm)\n",
    "    sbb[2:] = transform_keypoint(*sbb[2:], *sample['src_img'].size, sn, sm)\n",
    "    tbb = np.array(sample['trg_bndbox'])\n",
    "    tbb[:2] = transform_keypoint(*tbb[:2], *sample['trg_img'].size, tn, tm)\n",
    "    tbb[2:] = transform_keypoint(*tbb[2:], *sample['trg_img'].size, tn, tm)\n",
    "    tbb_max = max(tbb[2] - tbb[0], tbb[3] - tbb[1])\n",
    "\n",
    "    # solve semantic correspondence for each keypoint pair\n",
    "    pcks = []\n",
    "    for ([sx, sy],[tx,ty]) in zip(sample['src_kps'], sample['trg_kps']):\n",
    "\n",
    "        # transform keypoints and bb\n",
    "        sx, sy = transform_keypoint(sx, sy, *sample['src_img'].size, sn, sm)\n",
    "        tx, ty = transform_keypoint(tx, ty, *sample['trg_img'].size, tn, tm)\n",
    "\n",
    "        # calc similarities\n",
    "        if shift_reprs:\n",
    "            point = repr1_full[:, int(sy)//shift_skip, int(sx)//shift_skip, None, None]\n",
    "        else:\n",
    "            max_spatial1 = np.array(max(repr1[x].shape[-2:] for x in pos))\n",
    "            point = repr1_full[:, int(sy/(sm/max_spatial1[-2])), int(sx/(sn/max_spatial1[-1])), None, None]\n",
    "        similarities = torch.nn.functional.cosine_similarity(repr2_full, point, dim=0).cpu()  # cossim\n",
    "        # similarities = (repr2_full - point).abs().mean(dim=0).cpu()  # MAE - doesn't seem to work well\n",
    "        max_i = similarities.argmax().item()\n",
    "        x_max = max_i % repr2_full.shape[-1]\n",
    "        y_max = max_i // repr2_full.shape[-1]\n",
    "\n",
    "        # calculate error distance -> PCK\n",
    "        x_max_pixel = x_max*shift_skip if shift_reprs else (x_max+.5) * tn / repr2_full.shape[-1]\n",
    "        y_max_pixel = y_max*shift_skip if shift_reprs else (y_max+.5) * tm / repr2_full.shape[-2]\n",
    "        dist = ((x_max_pixel - tx)**2 + (y_max_pixel - ty)**2)**0.5\n",
    "        dist_rel = dist / tbb_max\n",
    "        pck = dist_rel <= 0.1\n",
    "        pcks.append(pck)\n",
    "\n",
    "        if not plot:\n",
    "            continue\n",
    "\n",
    "        # setup plot\n",
    "        plt.figure(figsize=(9, 3))\n",
    "        plt.suptitle(f'{sample[\"src_name\"].split(\"/\")[0]} (id:{sample[\"pair_id\"]}) - rel.dist.: {dist_rel:.2f}')\n",
    "\n",
    "        # plot source image\n",
    "        plt.subplot(131)\n",
    "        plt.title('source')\n",
    "        plt.imshow(src_img)\n",
    "        plt.scatter([sx], [sy], c='r')  # source keypoint\n",
    "        plt.plot([sbb[0], sbb[2], sbb[2], sbb[0], sbb[0]], [sbb[1], sbb[1], sbb[3], sbb[3], sbb[1]], c='gray')  # bounding box\n",
    "        plt.axis('off')\n",
    "\n",
    "        # plot target image\n",
    "        plt.subplot(132)\n",
    "        plt.title('target')\n",
    "        plt.imshow(trg_img)\n",
    "        plt.scatter([tx], [ty], c='r')  # target keypoint\n",
    "        plt.plot([tbb[0], tbb[2], tbb[2], tbb[0], tbb[0]], [tbb[1], tbb[1], tbb[3], tbb[3], tbb[1]], c='gray')  # bounding box\n",
    "        plt.axis('off')\n",
    "\n",
    "        # plot similarities\n",
    "        plt.subplot(133)\n",
    "        plt.title('similarities')\n",
    "        plt.imshow(similarities.view(*repr2_full.shape[-2:]).numpy())  # similarities\n",
    "        plt.scatter([x_max], [y_max], c='b')  # predicted keypoint\n",
    "        plt.scatter([tx/tn*repr2_full.shape[-1]-.5], [ty/tm*repr2_full.shape[-2]-.5], c='r')  # true target keypoint\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return pcks\n",
    "\n",
    "\n",
    "def sc_plot_random(\n",
    "        i: int | None = None,\n",
    "        pos: list[str] = ['up_blocks[0]'],\n",
    "        trans: Literal['expand'] | Literal['expand_and_resize'] | None = None,\n",
    "        step: int = 1,\n",
    "        **kwargs,\n",
    "):\n",
    "    if i is None:\n",
    "        i = torch.randint(len(dataset_pairs), (1,)).item()\n",
    "        print(f'{i = }')\n",
    "    transform, transform_keypoint = get_transforms(trans)\n",
    "    sc(dataset_pairs[i], plot=True, extraction_step=step, pos=pos, transform=transform, transform_keypoint=transform_keypoint, **kwargs)\n",
    "\n",
    "\n",
    "def sc_calc_dataset(\n",
    "        pos: list[str] = ['up_blocks[0]'],\n",
    "        trans: Literal['expand'] | Literal['expand_and_resize'] | None = None,\n",
    "        repr_extraction: Callable | None = None,\n",
    "        step: int = 1,\n",
    "        shift_reprs: bool = False,\n",
    "    ):\n",
    "    transform, transform_keypoint = get_transforms(trans, size=512)\n",
    "    if repr_extraction is None:\n",
    "        repr_extraction_fn = lambda x, pos, step, prompt: sd.img2repr(x, pos, step, prompt, spatial_avg=False, output_device='cpu')\n",
    "\n",
    "    # precalculate representations\n",
    "    if not shift_reprs:\n",
    "        data_dataset = datasets.load_dataset('0jl/SPair-71k', 'data', trust_remote_code=True, split='train')\n",
    "        dataset_reprs = [repr_extraction_fn(transform(x['img']), pos, step, x['name'].split('/')[0]) for x in tqdm(data_dataset, desc='calculating representations')]\n",
    "    else:\n",
    "        dataset_reprs = None\n",
    "\n",
    "    # tp = TrainPlot()\n",
    "    pcks = []\n",
    "    cpcks = {}\n",
    "    try:\n",
    "        for sample in tqdm(dataset_pairs, desc='processing samples'):\n",
    "            assert isinstance(sample, dict)\n",
    "            new_pcks = sc(sample, plot=False, precomputed_reprs=dataset_reprs, extraction_step=step, pos=pos, transform=transform, transform_keypoint=transform_keypoint)\n",
    "            pcks += new_pcks\n",
    "            c = dataset_pairs.features['category'].names[sample['category']]\n",
    "            cpcks[c] = cpcks.get(c, []) + new_pcks\n",
    "            # tp(**{k: np.mean(v) for k, v in cpcks.items()})\n",
    "    finally:\n",
    "        print('SC Results:')\n",
    "        print(f'PCK: {np.mean(pcks):5.2%} ({len(pcks)})')\n",
    "        for k, v in cpcks.items():\n",
    "            print(f'{k+\":\":<15} {np.mean(v):5.1%} {f\"({len(v)})\":>7}')\n",
    "        return pcks, cpcks\n",
    "\n",
    "\n",
    "def sc_calc_dataset_small(\n",
    "        num_samples=50,\n",
    "        seed=42,\n",
    "        pos: list[str] = ['up_blocks[0]', 'up_blocks[1]'],\n",
    "        trans: Literal['expand'] | Literal['expand_and_resize'] | None = None,\n",
    "        step: int = 1,\n",
    "    ):\n",
    "    transform, transform_keypoint = get_transforms(trans)\n",
    "    rng = random.Random(seed)\n",
    "\n",
    "    # calculate PCK\n",
    "    pcks = []\n",
    "    for _ in trange(num_samples):\n",
    "        i = rng.randint(0, len(dataset_pairs)-1)\n",
    "        new_pcks = sc(dataset_pairs[i], plot=False, extraction_step=step, pos=pos, transform=transform, transform_keypoint=transform_keypoint)\n",
    "        pcks += new_pcks\n",
    "    print(f'PCK: {np.mean(pcks):5.1%} ({len(pcks)})')\n",
    "\n",
    "\n",
    "\n",
    "def random_hyper_opt():\n",
    "    available_pos = [x for x in sd.available_extract_positions if any(f'{y}_block' in x for y in ['down', 'mid', 'up'])]\n",
    "    runs = []\n",
    "    try:\n",
    "        for _ in trange(int(1e10)):\n",
    "            # randomize hyperparameters\n",
    "            t = ['expand_and_resize', 'expand', None][random.randint(0, 2)]\n",
    "            p = random.sample(available_pos, random.randint(1, len(available_pos)))\n",
    "            s = random.randint(1, 999)\n",
    "\n",
    "            # run\n",
    "            t1, t2 = get_transforms(t)\n",
    "            sample = dataset_pairs[random.randint(0, len(dataset_pairs)-1)]\n",
    "            pcks = sc(sample, plot=False, extraction_step=s, pos=p, transform=t1, transform_keypoint=t2)\n",
    "            for pck in pcks:\n",
    "                runs.append((dict(t=t,p=p,s=s), pck))\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        print('Random Hyperparameter Optimization Results:')\n",
    "        res = np.mean([x[1] for x in runs])\n",
    "        print(f'PCK: {res:.2%} ({len(runs)} runs)')\n",
    "\n",
    "        print('transforms:')\n",
    "        for t in ['expand_and_resize', 'expand', None]:\n",
    "            r = [x[1] for x in runs if x[0][\"t\"] == t]\n",
    "            print(f'  {str(t)+\":\":<20} {np.mean(r):5.1%} ({np.mean(r)-res:+6.1%}) {f\"({len(r)})\":>7}')\n",
    "        \n",
    "        print('positions:')\n",
    "        for p in available_pos:\n",
    "            r = [x[1] for x in runs if p in x[0][\"p\"]]\n",
    "            r_ = [x[1] for x in runs if p not in x[0][\"p\"]]\n",
    "            print(f'  {p+\":\":<20} with: {np.mean(r):5.1%} ({np.mean(r)-res:+6.1%}) {f\"({len(r)})\":>7}, without: {np.mean(r_):.1%} ({np.mean(r_)-res:+6.1%}) {f\"({len(r_)})\":>7}')\n",
    "\n",
    "        print('position combinations: ')\n",
    "        pos_combinations = defaultdict(list)\n",
    "        for x in runs:\n",
    "            pos_combinations[tuple(sorted(x[0][\"p\"]))].append(x[1])\n",
    "        for pos, pck in sorted(pos_combinations.items(), key=lambda x: -np.mean(x[1]))[:20]:\n",
    "            print(f'  {str(pos):50}: {np.mean(pck):5.1%} ({np.mean(pck)-res:+6.1%}) {f\"({len(pck)})\":>7}')\n",
    "\n",
    "        print('steps:')\n",
    "        for s in range(0, 1000, 100):\n",
    "            r = [x[1] for x in runs if s < x[0][\"s\"] <= x[0][\"s\"]+100]\n",
    "            print(f'  {f\"{s}-{s+100}\":20} {np.mean(r):5.1%} ({np.mean(r)-res:+6.1%}) {f\"({len(r)})\":>7}')\n",
    "\n",
    "        return runs\n",
    "    \n",
    "def multi_repr_extraction(x, pos, step, prompt, count = 4):\n",
    "    reprs = [sd.img2repr(x, pos, step, prompt, spatial_avg=False, output_device='cpu') for _ in range(count)]\n",
    "    result = {}\n",
    "    for k in reprs[0]:\n",
    "        result[k] = torch.stack([x[k] for x in reprs], dim=0).mean(dim=0)\n",
    "    return result\n",
    "\n",
    "# sc_plot_random(pos=['up_blocks[1]'], trans=None, step=100, shift_reprs=True)\n",
    "# sc_calc_dataset_small()\n",
    "pcks, cpcks = sc_calc_dataset(\n",
    "    pos=['up_blocks[1]'],\n",
    "    trans='expand',\n",
    "    repr_extraction=None,\n",
    "    step=100,\n",
    "    shift_reprs=True,\n",
    ")\n",
    "# runs = random_hyper_opt()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
