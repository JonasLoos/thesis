{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sdhelper import SD\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm.autonotebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"high_norm_anomalies_imagenet_subset_step50_seed42_heavy_only.npy\"\n",
    "seed = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(data_path)\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.zeros((500,), dtype=np.int32)\n",
    "for i, x, y in data:\n",
    "    counts[i] += 1\n",
    "\n",
    "bars = np.zeros(counts.max()+1)\n",
    "for x in counts:\n",
    "    bars[x] += 1\n",
    "\n",
    "plt.bar(np.arange(len(bars)), bars)\n",
    "plt.title(\"Number of anomalies per image\")\n",
    "# plt.yscale(\"log\")\n",
    "plt.xlabel(\"Number of anomalies\")\n",
    "plt.ylabel(\"Number of images\")\n",
    "plt.show()\n",
    "\n",
    "print(f'{1-bars[0]/500:.2%} of images have anomalies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_subset = load_dataset(\"JonasLoos/imagenet_subset\", split=\"train\")\n",
    "sd = SD()\n",
    "representations = sd.img2repr([x['image'] for x in imagenet_subset], extract_positions=['up_blocks[1]'], step=50, seed=seed)\n",
    "representations = torch.stack([r['up_blocks[1]'].squeeze(0) for r in representations]).to(dtype=torch.float32)\n",
    "norms = torch.linalg.norm(representations, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm histogram\n",
    "norms_of_selected = norms[data[:, 0], data[:, 2], data[:, 1]]\n",
    "print(f'norms of selected: min {norms_of_selected.min():.2f}, max {norms_of_selected.max():.2f}, mean {norms_of_selected.mean():.2f}')\n",
    "\n",
    "# mean norm of 2x2 patches around selected anomalies\n",
    "reprs_of_patches = torch.concat([\n",
    "    representations[data[:, 0], :, data[:, 2]+0, data[:, 1]+0],\n",
    "    representations[data[:, 0], :, data[:, 2]+0, data[:, 1]+1],\n",
    "    representations[data[:, 0], :, data[:, 2]+1, data[:, 1]+0],\n",
    "    representations[data[:, 0], :, data[:, 2]+1, data[:, 1]+1],\n",
    "], dim=0)\n",
    "norms_of_patches = torch.linalg.norm(reprs_of_patches, dim=1)\n",
    "print(f'norms 4x4 patches: min {norms_of_patches.min():.2f}, max {norms_of_patches.max():.2f}, mean {norms_of_patches.mean():.2f}')\n",
    "print(f'norms of all:      min {norms.min():.2f}, max {norms.max():.2f}, mean {norms.mean():.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(reprs_of_patches.mean(dim=0), 'imagenet_subset_high_norm_anomalies_step50_seed42_heavy_only_reprs_of_patches_mean.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected similarity\n",
    "reprs_of_selected = representations[data[:, 0], :, data[:, 2], data[:, 1]]\n",
    "similarities = torch.cosine_similarity(reprs_of_selected[:, None], reprs_of_selected[None, :], dim=2)\n",
    "print(f'mean similarity of all selected: {similarities.mean():.4f}')\n",
    "\n",
    "# similarity of top 20% of selected\n",
    "reprs_of_selected_top20 = reprs_of_selected[torch.argsort(norms_of_selected, descending=True)[:int(len(reprs_of_selected)*0.2)]]\n",
    "similarities_top20 = torch.cosine_similarity(reprs_of_selected_top20[:, None], reprs_of_selected_top20[None, :], dim=2)\n",
    "print(f'mean similarity of top 20% of selected: {similarities_top20.mean():.4f}')\n",
    "\n",
    "\n",
    "# all similarity (random subset of 1000)\n",
    "all_reprs = representations.permute(0, 2, 3, 1).flatten(0,2)[torch.randperm(representations.shape[0])[:1000]]\n",
    "similarities = torch.cosine_similarity(all_reprs[:, None], all_reprs[None, :], dim=2)\n",
    "print(f'mean similarity of random subset of all: {similarities.mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "\n",
    "# cosine similarity\n",
    "cosine_similarity = lambda x: torch.cosine_similarity(x[:, None], x[None, :], dim=2)\n",
    "\n",
    "# euclidean distance\n",
    "def euclidean_similarity(x):\n",
    "    distance = ((x[:, None, :] - x[None, :, :])**2).mean(dim=-1)**.5\n",
    "    return 1-distance/distance.max()\n",
    "\n",
    "similarity_measure = cosine_similarity\n",
    "\n",
    "# single tokens\n",
    "for pos_name, (dx, dy) in {'top-left': (0, 0), 'top-right': (0, 1), 'bottom-left': (1, 0), 'bottom-right': (1, 1)}.items():\n",
    "    tmp_reprs = representations[data[:, 0], :, data[:, 2]+dx, data[:, 1]+dy]\n",
    "    tmp_reprs_sorted = tmp_reprs[torch.argsort(norms_of_selected, descending=True)]\n",
    "    similarities_selected = similarity_measure(tmp_reprs_sorted)\n",
    "    mean_similarities_selected = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        top_i = int(len(tmp_reprs_sorted)*(i+1)/n)\n",
    "        # normalize while accounting for self-similarity (1s on diagonal)\n",
    "        mean_similarities_selected[i] = (similarities_selected[:top_i, :top_i].sum() - top_i) / (top_i*(top_i-1))\n",
    "    plt.plot(np.linspace(1, 100, n), mean_similarities_selected, label=pos_name)\n",
    "\n",
    "# patches\n",
    "reprs_of_patches_sorted = reprs_of_patches[torch.argsort(norms_of_patches, descending=True)]\n",
    "similarities_patches = similarity_measure(reprs_of_patches_sorted)\n",
    "mean_similarities_patches = np.zeros(n)\n",
    "for i in range(n):\n",
    "    top_i = int(len(reprs_of_patches)*(i+1)/n)\n",
    "    mean_similarities_patches[i] = (similarities_patches[:top_i, :top_i].sum() - top_i) / (top_i*(top_i-1))\n",
    "plt.plot(np.linspace(1, 100, n), mean_similarities_patches, label=\"patches\")\n",
    "\n",
    "# all (subset)\n",
    "all_reprs_sorted = all_reprs[torch.argsort(all_reprs.norm(dim=1), descending=True)]\n",
    "similarities_all = similarity_measure(all_reprs_sorted)\n",
    "mean_similarities_all = np.zeros(n)\n",
    "for i in range(n):\n",
    "    top_i = int(len(all_reprs)*(i+1)/n)\n",
    "    mean_similarities_all[i] = (similarities_all[:top_i, :top_i].sum() - top_i) / (top_i*(top_i-1))\n",
    "plt.plot(np.linspace(1, 100, n), mean_similarities_all, label=\"all (random subset)\")\n",
    "\n",
    "plt.title(\"Mean similarity of top k% by norm\")\n",
    "plt.xlabel(\"Top k% by norm\")\n",
    "plt.ylabel(\"Mean similarity\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(similarities_patches.sum() - len(similarities_patches)) / (len(similarities_patches)*(len(similarities_patches)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity to average repr\n",
    "\n",
    "bins = 100\n",
    "\n",
    "repr_means = {\n",
    "    'patch': reprs_of_patches.mean(dim=0),\n",
    "    'top-20%-patch': reprs_of_patches_sorted[:int(len(reprs_of_patches)*0.2)].mean(dim=0),\n",
    "    'selected': reprs_of_selected.mean(dim=0),\n",
    "    'top-20%-selected': reprs_of_selected_top20.mean(dim=0),\n",
    "}\n",
    "\n",
    "\n",
    "for repr_name, repr_mean in repr_means.items():\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # all (subset)\n",
    "    all_similarities_to_mean = torch.cosine_similarity(repr_mean[None, :], all_reprs, dim=1)\n",
    "    all_hist = torch.histc(all_similarities_to_mean, bins=bins, min=-0.2, max=1.0)\n",
    "    # all_hist /= all_hist.sum() / bins  # normalization to avg bin size = 1\n",
    "    plt.bar(np.linspace(-0.2, 1.0, bins), all_hist.numpy(), width=0.012, label='all (subset)', alpha=0.6, color='blue')\n",
    "\n",
    "    # patches\n",
    "    patch_similarities_to_mean = torch.cosine_similarity(repr_mean[None, :], reprs_of_patches, dim=1)\n",
    "    patch_hist = torch.histc(patch_similarities_to_mean, bins=bins, min=-0.2, max=1.0)\n",
    "    # patch_hist /= patch_hist.sum() / bins  # normalization to avg bin size = 1\n",
    "    plt.bar(np.linspace(-0.2, 1.0, bins), patch_hist.numpy(), width=0.012, label='patches', alpha=0.6, color='purple')\n",
    "\n",
    "    # patches top 20%\n",
    "    patch_top20_similarities_to_mean = torch.cosine_similarity(repr_mean[None, :], reprs_of_patches_sorted[:int(len(reprs_of_patches)*0.2)], dim=1)\n",
    "    patch_top20_hist = torch.histc(patch_top20_similarities_to_mean, bins=bins, min=-0.2, max=1.0)\n",
    "    # patch_top20_hist /= patch_top20_hist.sum() / bins  # normalization to avg bin size = 1\n",
    "    plt.bar(np.linspace(-0.2, 1.0, bins), patch_top20_hist.numpy(), width=0.012, label='patches top 20%', alpha=0.6, color='green')\n",
    "\n",
    "    # selected\n",
    "    selected_similarities_to_mean = torch.cosine_similarity(repr_mean[None, :], reprs_of_selected, dim=1)\n",
    "    selected_hist = torch.histc(selected_similarities_to_mean, bins=bins, min=-0.2, max=1.0)\n",
    "    # selected_hist /= selected_hist.sum() / bins  # normalization to avg bin size = 1\n",
    "    plt.bar(np.linspace(-0.2, 1.0, bins), selected_hist.numpy(), width=0.012, label='selected', alpha=0.6, color='orange')\n",
    "\n",
    "    # selected top 20% by norm\n",
    "    selected_top20_similarities_to_mean = torch.cosine_similarity(repr_mean[None, :], reprs_of_selected_top20, dim=1)\n",
    "    selected_top20_hist = torch.histc(selected_top20_similarities_to_mean, bins=bins, min=-0.2, max=1.0)\n",
    "    # selected_top20_hist /= selected_top20_hist.sum() / bins  # normalization to avg bin size = 1\n",
    "    plt.bar(np.linspace(-0.2, 1.0, bins), selected_top20_hist.numpy(), width=0.012, label='selected top 20%', alpha=0.6, color='red')\n",
    "\n",
    "    # scatter plots for better visibility\n",
    "    plt.scatter(np.linspace(-0.2, 1.0, bins), all_hist.numpy(), c='blue', s=5)\n",
    "    plt.scatter(np.linspace(-0.2, 1.0, bins), patch_hist.numpy(), c='purple', s=5)\n",
    "    plt.scatter(np.linspace(-0.2, 1.0, bins), patch_top20_hist.numpy(), c='green', s=5)\n",
    "    plt.scatter(np.linspace(-0.2, 1.0, bins), selected_hist.numpy(), c='orange', s=5)\n",
    "    plt.scatter(np.linspace(-0.2, 1.0, bins), selected_top20_hist.numpy(), c='red', s=5)\n",
    "\n",
    "    plt.title(f\"Cosine Similarity to average anomaly {repr_name} repr\")\n",
    "    plt.xlabel(\"Similarity\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of anomaly norms\n",
    "plt.hist(norms_of_selected, bins=50)\n",
    "plt.title(\"Histogram of anomaly norms\")\n",
    "plt.xlabel(\"Norm\")\n",
    "plt.ylabel(\"Number of anomalies\")\n",
    "\n",
    "# Add vertical line at top 25 percentile\n",
    "top_25_percentile = np.percentile(norms_of_selected, 75)\n",
    "plt.axvline(x=top_25_percentile, color='r', linestyle='--', label='Top 25% threshold')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap of anomaly positions\n",
    "heatmap = torch.zeros((32, 32))\n",
    "for d in data:\n",
    "    heatmap[d[1], d[2]] += 1\n",
    "plt.imshow(heatmap, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title(\"Heatmap of anomaly positions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_of_patches = torch.argsort(norms_of_patches, descending=True)\n",
    "indices_of_patches_top25 = indices_of_patches[:int(len(reprs_of_patches)*0.25)]\n",
    "\n",
    "# Group patches by image\n",
    "data_for_patches_top25 = {}\n",
    "for i in indices_of_patches_top25:\n",
    "    d = data[i%len(data)]\n",
    "    patch_pos = i//len(data)\n",
    "    x = d[1] + (patch_pos == 1) + (patch_pos == 3)\n",
    "    y = d[2] + (patch_pos == 2) + (patch_pos == 3)\n",
    "    img_idx = int(d[0])\n",
    "    if img_idx not in data_for_patches_top25:\n",
    "        data_for_patches_top25[img_idx] = []\n",
    "    data_for_patches_top25[img_idx].append((i, x, y, norms_of_patches[i]))\n",
    "\n",
    "\n",
    "# Sort images by their highest patch norm\n",
    "data_for_patches_top25_sorted = sorted(data_for_patches_top25.items(), key=lambda x: max(patch[3] for patch in x[1]), reverse=True)\n",
    "\n",
    "print(f'{len(data_for_patches_top25_sorted)}/500 images have top 25% anomalies -> {len(data_for_patches_top25_sorted)/500:.2%}')\n",
    "for i in range(max(len(x[1]) for x in data_for_patches_top25_sorted)):\n",
    "    count = sum(i == len(x[1]) for x in data_for_patches_top25_sorted)\n",
    "    print(f'{i}: {count: 3} - {count/len(data_for_patches_top25_sorted): 7.2%}')\n",
    "\n",
    "\n",
    "# handle others\n",
    "data_for_patches_bot75 = {}\n",
    "for i in indices_of_patches[int(len(indices_of_patches)*0.25):]:\n",
    "    d = data[i%len(data)]\n",
    "    patch_pos = i//len(data)\n",
    "    x = d[1] + (patch_pos == 1) + (patch_pos == 3)\n",
    "    y = d[2] + (patch_pos == 2) + (patch_pos == 3)\n",
    "    img_idx = int(d[0])\n",
    "    if img_idx not in data_for_patches_bot75:\n",
    "        data_for_patches_bot75[img_idx] = []\n",
    "    data_for_patches_bot75[img_idx].append((i, x, y, norms_of_patches[i]))\n",
    "\n",
    "data_for_patches_bot75_sorted = sorted(data_for_patches_bot75.items(), key=lambda x: max(patch[3] for patch in x[1]), reverse=True)\n",
    "print(f'{len(data_for_patches_bot75_sorted)}/500 images have bottom 75% anomalies -> {len(data_for_patches_bot75_sorted)/500:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot images with high norm patches (top 20%)\n",
    "count = 5\n",
    "\n",
    "fig, axs = plt.subplots(count, 2, figsize=(6, 3*count))\n",
    "for i, (img_idx, patches) in enumerate(data_for_patches_top25_sorted[:count]):  # Limit to top 50 images\n",
    "    img = imagenet_subset[img_idx]['image']\n",
    "\n",
    "    axs[i, 0].imshow(img, extent=(0, 1, 0, 1))\n",
    "    for _, x, y, norm in patches:\n",
    "        axs[i, 0].scatter((x+0.5)/32, (32-y-0.5)/32, c='red', s=30)\n",
    "    axs[i, 0].set_title(f\"Image {img_idx}\")\n",
    "    axs[i, 0].axis('off')\n",
    "    \n",
    "    axs[i, 1].imshow(norms[img_idx].squeeze(0), cmap='YlOrRd', aspect='equal', extent=(0, 1, 0, 1))\n",
    "    for _, x, y, norm in patches:\n",
    "        axs[i, 1].scatter((x+0.5)/32, (32-y-0.5)/32, c='red', s=3)\n",
    "    axs[i, 1].set_title(f\"Norm map (max: {norms[img_idx].max():.2f})\")\n",
    "    axs[i, 1].axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot images with high norm patches (top 20%)\n",
    "count = 5\n",
    "\n",
    "fig, axs = plt.subplots(count, 2, figsize=(6, 3*count))\n",
    "for i, (img_idx, patches) in enumerate(data_for_patches_bot75_sorted[-count:]):  # Limit to top 50 images\n",
    "    img = imagenet_subset[img_idx]['image']\n",
    "\n",
    "    axs[i, 0].imshow(img, extent=(0, 1, 0, 1))\n",
    "    for _, x, y, norm in patches:\n",
    "        axs[i, 0].scatter((x+0.5)/32, (32-y-0.5)/32, c='blue', s=30)\n",
    "    axs[i, 0].set_title(f\"Image {img_idx}\")\n",
    "    axs[i, 0].axis('off')\n",
    "    \n",
    "    axs[i, 1].imshow(norms[img_idx].squeeze(0), cmap='YlOrRd', aspect='equal', extent=(0, 1, 0, 1))\n",
    "    for _, x, y, norm in patches:\n",
    "        axs[i, 1].scatter((x+0.5)/32, (32-y-0.5)/32, c='blue', s=3)\n",
    "    axs[i, 1].set_title(f\"Norm map (max: {norms[img_idx].max():.2f})\")\n",
    "    axs[i, 1].axis('off')\n",
    "    \n",
    "plt.suptitle(\"Images with bottom 75% anomalies (highest norm first)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
