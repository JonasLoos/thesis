{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation Autoencoder\n",
    "\n",
    "goal: modify representation channels so that 2 channels contain the positional information and the others don't.\n",
    "\n",
    "WIP: doesn't work (yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdhelper import SD\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datasets\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "import PIL.Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and data\n",
    "sd = SD('sd15')\n",
    "data = datasets.load_dataset('0jl/SPair-71k', 'data', split='train', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "p = 'up_blocks[1]'\n",
    "img_size = 512\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precalculate representations\n",
    "\n",
    "def expand_and_resize(x: PIL.Image.Image, size, border_pad=True):\n",
    "    n, m = x.size\n",
    "    s = max(n, m)\n",
    "    r = PIL.Image.new('RGB', (s, s))\n",
    "    r.paste(x, ((s-n)//2, (s-m)//2))\n",
    "    if border_pad:\n",
    "        # pad with border\n",
    "        if n > m:\n",
    "            r.paste(x.crop((0, 0, n, 1)).resize((n,(s-m)//2)), (0, 0))\n",
    "            r.paste(x.crop((0, m-1, n, m)).resize((n,(s-m)//2)), (0, m+(s-m)//2))\n",
    "        elif m > n:\n",
    "            r.paste(x.crop((0, 0, 1, m)).resize(((s-n)//2,m)), (0, 0))\n",
    "            r.paste(x.crop((n-1, 0, n, m)).resize(((s-n)//2,m)), (n+(s-n)//2, 0))\n",
    "    return r.resize((size, size))\n",
    "\n",
    "transform_img = lambda img: expand_and_resize(img, img_size, True)\n",
    "\n",
    "representations = []\n",
    "for x in tqdm(data, desc='Calculating representations'):\n",
    "    r = sd.img2repr(transform_img(x['img']), [p], 100, prompt=x['name'].split('/')[0])\n",
    "    representations.append(r.data[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels, H, W = representations[0].shape[1:]\n",
    "assert H == W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDReprAutoencoder(nn.Module):\n",
    "    def __init__(self, num_channels, encoded_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(num_channels, encoded_channels)\n",
    "        self.decoder = nn.Linear(encoded_channels, num_channels)\n",
    "\n",
    "ae = SDReprAutoencoder(num_channels, num_channels+H+W).to(device)\n",
    "ae_optimizer = torch.optim.Adam(ae.parameters(), lr=1e-4)\n",
    "position_estimator = nn.Sequential(\n",
    "    nn.Linear(num_channels, H+W, device=device),\n",
    ")\n",
    "pe_optimizser = torch.optim.Adam(position_estimator.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training progress\n",
    "from trainplot.trainplot import TrainPlotPlotlyExperimental as TrainPlot\n",
    "tp = TrainPlot(threaded=True)\n",
    "tp.fig.update_yaxes(type=\"log\");\n",
    "tp_acc = TrainPlot(threaded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "for epoch in trange(50):\n",
    "    accucacies = []\n",
    "    losses = []\n",
    "    for i, reprs in enumerate(representations):\n",
    "\n",
    "        # setup input\n",
    "        r = reprs[0].to(dtype=torch.float32, device=device)\n",
    "        x_positions = (torch.arange(H, device=device)).repeat(W)\n",
    "        y_positions = (torch.arange(W, device=device)).repeat_interleave(H)\n",
    "        positions = torch.cat([\n",
    "            F.one_hot(x_positions, H).float(),\n",
    "            F.one_hot(y_positions, W).float()\n",
    "        ], dim=1)\n",
    "        r = r.flatten(1,2).T.to(device)\n",
    "        r = (r - r.mean()) / r.std()\n",
    "\n",
    "        # forward pass\n",
    "        encoded = ae.encoder(r)\n",
    "        position_estimation = position_estimator(encoded[:,:num_channels])\n",
    "        # decoded = ae.decoder(torch.cat([encoded[:,:num_channels], positions], dim=1))  # use real positions for decoding\n",
    "        decoded = ae.decoder(encoded)\n",
    "\n",
    "        # calculate losses\n",
    "        # pos_loss = F.mse_loss(F.softmax(encoded[:,num_channels:], dim=-1), positions)\n",
    "        pos_loss = F.cross_entropy(encoded[:,num_channels:], positions)\n",
    "        # pe_loss = F.mse_loss(position_estimation, positions)\n",
    "        pe_loss = F.cross_entropy(position_estimation, positions)\n",
    "        ae_loss = F.mse_loss(decoded, r) + F.l1_loss(decoded, r)\n",
    "        ae_loss_full = 10*ae_loss + 1*pos_loss - .5*pe_loss\n",
    "        # TODO: maybe add loss trying to have the embedding be close to the actual representation\n",
    "\n",
    "        # optimize autoencoder\n",
    "        ae_optimizer.zero_grad()\n",
    "        ae_loss_full.backward(retain_graph=True)\n",
    "        ae_optimizer.step()\n",
    "\n",
    "        # optimize position estimator\n",
    "        pe_optimizser.zero_grad()\n",
    "        pe_loss.backward()\n",
    "        pe_optimizser.step()\n",
    "\n",
    "        # log\n",
    "        with torch.no_grad():\n",
    "            pos_acc = (encoded[:,num_channels:].unflatten(-1,(2,H)).argmax(dim=-1) == positions.unflatten(-1,(2,H)).argmax(dim=-1)).float().mean(axis=0).cpu()\n",
    "            pe_acc = (position_estimation.unflatten(-1,(2,H)).argmax(dim=-1) == positions.unflatten(-1,(2,H)).argmax(dim=-1)).float().mean(axis=0).cpu()\n",
    "            accucacies.append(dict(pos_x=pos_acc[0].item(), pos_y=pos_acc[1].item(), pe_x=pe_acc[0].item(), pe_y=pe_acc[1].item()))\n",
    "            losses.append(dict(pos=pos_loss.item(), pe=pe_loss.item(), ae=ae_loss.item()))\n",
    "            if i % 100 == 0:\n",
    "                tp_acc(**{k: np.mean([x[k] for x in accucacies]) for k in accucacies[0].keys()})\n",
    "                tp(**{k: np.mean([x[k] for x in losses]) for k in losses[0].keys()})\n",
    "                accucacies = []\n",
    "                losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Semantic Correspondence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = datasets.load_dataset('0jl/SPair-71k', 'pairs', split='test', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference\n",
    "# calculate percentage of correct keypoints at 10% of the bounding box (PCK@0.1_bbox)\n",
    "\n",
    "correct_reference = []\n",
    "positions_reference = []\n",
    "for x in (t:=tqdm(pairs, desc='Calculating SC')):\n",
    "    a = representations[x['src_data_index']].squeeze(0)\n",
    "    b = representations[x['trg_data_index']].squeeze(0)\n",
    "    tbb_max = max(x['trg_bndbox'][2] - x['trg_bndbox'][0], x['trg_bndbox'][3] - x['trg_bndbox'][1])\n",
    "    for ([sx, sy],[tx,ty]) in zip(x['src_kps'], x['trg_kps']):\n",
    "        src_repr = a[:, \n",
    "            int((sy + (max(x['src_img'].size) - x['src_img'].size[1])/2) * a.shape[1] / max(x['src_img'].size)),\n",
    "            int((sx + (max(x['src_img'].size) - x['src_img'].size[0])/2) * a.shape[2] / max(x['src_img'].size)),\n",
    "        ]\n",
    "        cossim = (b * src_repr[:,None,None]).sum(dim=0)\n",
    "        y_max, x_max = np.unravel_index(cossim.argmax().cpu(), cossim.shape)\n",
    "        x_max_pixel = x_max / b.shape[2] * max(x['trg_img'].size) - (max(x['trg_img'].size) - x['trg_img'].size[0]) / 2\n",
    "        y_max_pixel = y_max / b.shape[1] * max(x['trg_img'].size) - (max(x['trg_img'].size) - x['trg_img'].size[1]) / 2\n",
    "        relative_distance = ((x_max_pixel - tx)**2 + (y_max_pixel - ty)**2) ** 0.5 / tbb_max\n",
    "        correct_reference.append(relative_distance < 0.1)\n",
    "        positions_reference.append((x_max_pixel, y_max_pixel))\n",
    "    if len(correct_reference) % 100 == 0:\n",
    "        t.set_postfix(pck=np.mean(correct_reference)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using AE representations\n",
    "\n",
    "encoded_representation = [\n",
    "    ae.encoder(r.view(-1, H*W).T.to(device=device, dtype=torch.float32)).T.unflatten(1, (H,W))[:num_channels,:,:].to('cpu')\n",
    "    for r in tqdm(representations, desc='Calculating encoded representations')\n",
    "]\n",
    "\n",
    "# calculate percentage of correct keypoints at 10% of the bounding box (PCK@0.1_bbox)\n",
    "correct = []\n",
    "positions = []\n",
    "for x in (t:=tqdm(pairs, desc='Calculating SC')):\n",
    "    a = encoded_representation[x['src_data_index']]\n",
    "    b = encoded_representation[x['trg_data_index']]\n",
    "    tbb_max = max(x['trg_bndbox'][2] - x['trg_bndbox'][0], x['trg_bndbox'][3] - x['trg_bndbox'][1])\n",
    "    for ([sx, sy],[tx,ty]) in zip(x['src_kps'], x['trg_kps']):\n",
    "        src_repr = a[:, \n",
    "            int((sy + (max(x['src_img'].size) - x['src_img'].size[1])/2) * a.shape[1] / max(x['src_img'].size)),\n",
    "            int((sx + (max(x['src_img'].size) - x['src_img'].size[0])/2) * a.shape[2] / max(x['src_img'].size)),\n",
    "        ]\n",
    "        cossim = (b * src_repr[:,None,None]).sum(dim=0)\n",
    "        y_max, x_max = np.unravel_index(cossim.argmax().cpu(), cossim.shape)\n",
    "        x_max_pixel = x_max / b.shape[2] * max(x['trg_img'].size) - (max(x['trg_img'].size) - x['trg_img'].size[0]) / 2\n",
    "        y_max_pixel = y_max / b.shape[1] * max(x['trg_img'].size) - (max(x['trg_img'].size) - x['trg_img'].size[1]) / 2\n",
    "        relative_distance = ((x_max_pixel - tx)**2 + (y_max_pixel - ty)**2) ** 0.5 / tbb_max\n",
    "        correct.append(relative_distance < 0.1)\n",
    "        positions.append((x_max_pixel, y_max_pixel))\n",
    "    if len(correct) % 100 == 0:\n",
    "        t.set_postfix(pck=np.mean(correct)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'{np.mean(correct):.2%} instead of {np.mean(correct_reference):.2%}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with ae output for verification\n",
    "\n",
    "decoded_representation = [\n",
    "    ae.decoder(ae.encoder(r.reshape(-1, H*W).T.to(device=device, dtype=torch.float32))).T.unflatten(1, (H,W)).to('cpu')\n",
    "    for r in tqdm(representations, desc='Calculating decoded representations')\n",
    "]\n",
    "\n",
    "correct_reference = []\n",
    "positions_reference = []\n",
    "for x in (t:=tqdm(pairs, desc='Calculating SC')):\n",
    "    a = decoded_representation[x['src_data_index']]\n",
    "    b = decoded_representation[x['trg_data_index']]\n",
    "    tbb_max = max(x['trg_bndbox'][2] - x['trg_bndbox'][0], x['trg_bndbox'][3] - x['trg_bndbox'][1])\n",
    "    for ([sx, sy],[tx,ty]) in zip(x['src_kps'], x['trg_kps']):\n",
    "        src_repr = a[:, \n",
    "            int((sy + (max(x['src_img'].size) - x['src_img'].size[1])/2) * a.shape[1] / max(x['src_img'].size)),\n",
    "            int((sx + (max(x['src_img'].size) - x['src_img'].size[0])/2) * a.shape[2] / max(x['src_img'].size)),\n",
    "        ]\n",
    "        cossim = (b * src_repr[:,None,None]).sum(dim=0)\n",
    "        y_max, x_max = np.unravel_index(cossim.argmax().cpu(), cossim.shape)\n",
    "        x_max_pixel = x_max / b.shape[2] * max(x['trg_img'].size) - (max(x['trg_img'].size) - x['trg_img'].size[0]) / 2\n",
    "        y_max_pixel = y_max / b.shape[1] * max(x['trg_img'].size) - (max(x['trg_img'].size) - x['trg_img'].size[1]) / 2\n",
    "        relative_distance = ((x_max_pixel - tx)**2 + (y_max_pixel - ty)**2) ** 0.5 / tbb_max\n",
    "        correct_reference.append(relative_distance < 0.1)\n",
    "        positions_reference.append((x_max_pixel, y_max_pixel))\n",
    "    if len(correct_reference) % 100 == 0:\n",
    "        t.set_postfix(pck=np.mean(correct_reference)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
