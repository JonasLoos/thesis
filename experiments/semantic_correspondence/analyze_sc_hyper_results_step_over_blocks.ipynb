{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SC over blocks and noise step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.autonotebook import tqdm\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'SDXL'\n",
    "blocks = ['conv_in','down_blocks[0]','down_blocks[1]','down_blocks[2]','mid_block','up_blocks[0]','up_blocks[1]','up_blocks[2]','conv_out']\n",
    "# blocks = ['conv_in','down_blocks[0]','down_blocks[1]','down_blocks[2]','down_blocks[3]','mid_block','up_blocks[0]','up_blocks[1]','up_blocks[2]','up_blocks[3]','conv_out']\n",
    "noise_steps = [0, 10, 25, 50, 75, 100, 150, 200, 300, 500, 800]\n",
    "base_path = Path(f'PATH_TO_RESULTS/step_over_blocks_{model.replace(\".\", \"\")}/')\n",
    "\n",
    "data = np.zeros((len(blocks), len(noise_steps), 88328, 12), dtype=int)\n",
    "for i, block in enumerate(tqdm(blocks, desc='blocks')):\n",
    "    for j, noise_step in enumerate(noise_steps):\n",
    "        data_path = base_path / f'{model}-{block}-expand_and_resize-512-{noise_step}.npy'\n",
    "        if not data_path.exists():\n",
    "            print(data_path.name, 'missing')\n",
    "            continue\n",
    "        tmp = np.load(data_path)\n",
    "        data[i,j,:,:] = tmp\n",
    "        # for correct, sx, sy, tx, ty, pred_x, pred_y, sn, sm, tn, tm, category_id in tmp:\n",
    "        #     pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pck = data[:,:,:,0].mean(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_block_idx = pck[:,0].argmax()\n",
    "ranking = pck[best_block_idx,:].argsort()[::-1]\n",
    "print(f'noise step ranking for best block {blocks[best_block_idx]}:')\n",
    "for i, idx in enumerate(ranking):\n",
    "    print(f'{i+1:2}. {noise_steps[idx]:3}: {pck[best_block_idx,idx]:6.2%}')\n",
    "\n",
    "print()\n",
    "print(f'noise step ranking for average over all blocks:')\n",
    "ranking = pck.mean(axis=0).argsort()[::-1]\n",
    "for i, idx in enumerate(ranking):\n",
    "    print(f'{i+1:2}. {noise_steps[idx]:3}: {pck.mean(axis=0)[idx]:6.2%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_names = [x.replace('_blocks', '').replace('_block', '').replace('_', '-') for x in blocks]\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.0, 0.9, len(noise_steps)))\n",
    "for i, (x, color) in enumerate(zip(pck.T, colors)):\n",
    "    plt.plot(block_names, x*100, color=color, label=noise_steps[i], marker='o', alpha=0.7)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('block')\n",
    "plt.ylabel('PCK [%]')\n",
    "plt.ylim(0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'sc_pck_spair_over_blocks_noise_{model.replace(\".\", \"\")}.npy', pck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_names = [x.replace('_blocks', '').replace('_block', '').replace('_', '-') for x in blocks]\n",
    "colors = plt.cm.rainbow(np.linspace(0.0, 1.0, len(blocks)))\n",
    "for i in range(len(blocks)):\n",
    "    plt.plot(noise_steps, pck[i,:]*100, color=colors[i], label=block_names[i], marker='o', alpha=0.7)\n",
    "plt.xlabel('noise step')\n",
    "plt.ylabel('PCK [%]')\n",
    "plt.ylim(0)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of sample count over predicted position relative to source position\n",
    "# not really relevant by itself, potentially in combination with the error rate\n",
    "\n",
    "lim = 128\n",
    "bins = 16\n",
    "\n",
    "fig, axs = plt.subplots(len(blocks), len(noise_steps), figsize=(len(blocks)*1, len(noise_steps)*1))\n",
    "for i, block in enumerate(tqdm(blocks, desc='blocks')):\n",
    "    axs[i,0].text(-0.1, 0.5, block_names[i], ha='right', va='center', transform=axs[i,0].transAxes)\n",
    "    for j, noise_step in enumerate(noise_steps):\n",
    "        tmp = data[i, j]\n",
    "        dx = tmp[:,1] - tmp[:,5]\n",
    "        dy = tmp[:,2] - tmp[:,6]\n",
    "\n",
    "        hist, *_ = np.histogram2d(dx, dy, bins=bins, range=[[-lim,lim],[-lim,lim]], density=True)\n",
    "        axs[i,j].imshow(hist, origin='lower', extent=(-lim, lim, -lim, lim), interpolation='nearest')\n",
    "        if i != len(blocks)-1: axs[i,j].set_xticks([])\n",
    "        else: axs[i,j].tick_params(labelsize=8)\n",
    "        if j != 0: axs[i,j].set_yticks([])\n",
    "        else: axs[i,j].tick_params(labelsize=8)\n",
    "        if i == len(blocks)-1: axs[-1,j].text(0.5, -0.5, noise_steps[j], ha='center', va='top', transform=axs[-1,j].transAxes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error rate over prediction position relative to source position\n",
    "\n",
    "lim = 256\n",
    "downscale = 4\n",
    "lim_scaled_rel = (512-lim)//downscale\n",
    "\n",
    "error_rates = np.zeros((len(blocks), len(noise_steps), 2*512//downscale, 2*512//downscale))\n",
    "n, m, *_ = error_rates.shape\n",
    "for i, block in enumerate(tqdm(blocks, desc='blocks')):\n",
    "    for j, noise_step in enumerate(noise_steps):\n",
    "        tmp = data[i, j]\n",
    "        error_counts = np.zeros((2*512, 2*512), dtype=int)\n",
    "        total_counts = np.zeros((2*512, 2*512), dtype=int)\n",
    "        for correct, sx, sy, tx, ty, pred_x, pred_y, sn, sm, tn, tm, category_id in tmp:\n",
    "            error_counts[sx-pred_x+512, sy-pred_y+512] += not correct\n",
    "            total_counts[sx-pred_x+512, sy-pred_y+512] += 1\n",
    "\n",
    "        error_count_scaled = error_counts.reshape(len(error_counts)//downscale, downscale, len(error_counts)//downscale, downscale).mean(axis=(1,3))\n",
    "        total_count_scaled = total_counts.reshape(len(total_counts)//downscale, downscale, len(total_counts)//downscale, downscale).mean(axis=(1,3))\n",
    "        error_rates[i,j,:,:] = error_count_scaled / total_count_scaled\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(n, m, figsize=(m*1, n*1))\n",
    "for i, block_name in enumerate(block_names):\n",
    "    axs[i,0].text(-0.8, 0.5, block_name, ha='right', va='center', transform=axs[i,0].transAxes)\n",
    "    for j, noise_step in enumerate(noise_steps):\n",
    "        axs[i,j].imshow(error_rates[i,j,lim_scaled_rel:-lim_scaled_rel, lim_scaled_rel:-lim_scaled_rel], origin='lower', extent=(-lim, lim, -lim, lim))\n",
    "        axs[i,j].tick_params(labelsize=8)\n",
    "        if i != n-1: axs[i,j].set_xticks([])\n",
    "        else: axs[i,j].set_xticks([-200, 0, 200])\n",
    "        if j != 0: axs[i,j].set_yticks([])\n",
    "        else: axs[i,j].set_yticks([-200, 0, 200])\n",
    "        if i == n-1: axs[-1,j].text(0.5, -0.5, noise_steps[j], ha='center', va='top', transform=axs[-1,j].transAxes)\n",
    "\n",
    "# x/y-labels\n",
    "axs[0,0].text(-1.8, -n/2, 'block | error rate', ha='right', va='center', transform=axs[0,0].transAxes, fontsize=12, rotation=90)\n",
    "axs[-1,0].text(m/2+1, -1.0, 'noise step | distance [px]', ha='center', va='top', transform=axs[-1,0].transAxes, fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "# np.save('sc_errors_by_relative_position_spair_maps_SD1.5.npy', error_rates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error rate over prediction position relative to source position\n",
    "\n",
    "lim = 128\n",
    "downscale = 16\n",
    "lim_scaled_rel = (512-lim)//downscale\n",
    "\n",
    "# compute error rates\n",
    "error_rates = np.zeros((len(blocks), len(noise_steps), int((2**.5*512)**.5)))\n",
    "for i, block in enumerate(tqdm(blocks, desc='blocks')):\n",
    "    for j, noise_step in enumerate(noise_steps):\n",
    "        tmp = data[i, j]\n",
    "\n",
    "        error_counts = np.zeros(int((2**.5*512)**.5), dtype=int)\n",
    "        total_counts = np.zeros(int((2**.5*512)**.5), dtype=int)\n",
    "        for correct, sx, sy, tx, ty, pred_x, pred_y, sn, sm, tn, tm, category_id in tmp:\n",
    "            dist = ((sx-pred_x)**2 + (sy-pred_y)**2)**.25\n",
    "            error_counts[int(dist)] += not correct\n",
    "            total_counts[int(dist)] += 1\n",
    "\n",
    "        error_rates[i,j,:] = error_counts / total_counts\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(len(blocks), len(noise_steps), figsize=(len(noise_steps)*1, len(blocks)*1))\n",
    "for i, block in enumerate(blocks):\n",
    "    axs[i,0].text(-0.8, 0.5, block_names[i], ha='right', va='center', transform=axs[i,0].transAxes)\n",
    "    for j, noise_step in enumerate(noise_steps):\n",
    "        error_rate = error_rates[i,j,:]\n",
    "\n",
    "        axs[i,j].plot(np.arange(len(error_rate))**2, error_rate)\n",
    "        axs[i,j].set_xlim(-5, 105)\n",
    "        if i != len(blocks)-1: axs[i,j].set_xticks([])\n",
    "        else: axs[i,j].tick_params(labelsize=8)\n",
    "        if j != 0: axs[i,j].set_yticks([])\n",
    "        else: axs[i,j].tick_params(labelsize=8)\n",
    "        if i == len(blocks)-1: axs[-1,j].text(0.5, -0.5, noise_steps[j], ha='center', va='top', transform=axs[-1,j].transAxes)\n",
    "\n",
    "# set same ylim for each row (relative to the third lowest value)\n",
    "for i in range(len(blocks)):\n",
    "    ymin = 1 - (1 - sorted(ax.get_ylim()[0] for ax in axs[i,:])[2]) * 1.1\n",
    "    for ax in axs[i,:]:\n",
    "        ax.set_ylim(ymin, 1 + (1 - ymin) * 0.1)\n",
    "\n",
    "# x/y-labels\n",
    "axs[0,0].text(-1.8, -len(blocks)/2, 'block | error rate', ha='right', va='center', transform=axs[0,0].transAxes, fontsize=12, rotation=90)\n",
    "axs[-1,0].text(len(noise_steps)/2, -1.0, 'noise step | relative distance [px]', ha='center', va='top', transform=axs[-1,0].transAxes, fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "# np.save('sc_errors_by_relative_position_spair_lines_SD1.5.npy', error_rates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_index = 3\n",
    "\n",
    "block_names = [x.replace('_blocks', '').replace('_block', '').replace('_', '-') for x in blocks]\n",
    "colors = plt.cm.rainbow(np.linspace(0.0, 1.0, len(blocks)))\n",
    "fig, ax = plt.subplots()\n",
    "for i, block in enumerate(blocks):\n",
    "    tmp = data[i, noise_index]\n",
    "    error_counts = np.zeros(int((2**.5*512)**.5), dtype=int)\n",
    "    total_counts = np.zeros(int((2**.5*512)**.5), dtype=int)\n",
    "    for correct, sx, sy, tx, ty, pred_x, pred_y, sn, sm, tn, tm, category_id in tmp:\n",
    "        dist = ((sx-pred_x)**2 + (sy-pred_y)**2)**.25\n",
    "        error_counts[int(dist)] += not correct\n",
    "        total_counts[int(dist)] += 1\n",
    "    error_rate = error_counts / total_counts\n",
    "    ax.plot(np.arange(len(error_rate))**2, error_rate, label=block_names[i], alpha=0.7, color=colors[i])\n",
    "    ax.set_xlim(-5, 55)\n",
    "\n",
    "    # print error rate stats\n",
    "    print(block_names[i])\n",
    "    lowest_error_idx = error_rate[:-1].argmin()\n",
    "    print(f'min error rate at {lowest_error_idx**2} px')\n",
    "    for i in range(lowest_error_idx+1):\n",
    "        print(f'  {i**2} <= dist < {(i+1)**2} px: error rate {error_rate[i]:.2%}, count {total_counts[i]}')\n",
    "    extra_errors = sum((error_rate[i]-error_rate[lowest_error_idx])*total_counts[i] for i in range(lowest_error_idx))\n",
    "    print(f'-> extra errors: {extra_errors} ({extra_errors/total_counts.sum():.4%})')\n",
    "    print()\n",
    "ax.legend(loc='upper right', fontsize=8)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = data[7, 3]  # up_blocks[3], noise_step 50\n",
    "error_counts = np.zeros(int((2**.5*512)**.5), dtype=int)\n",
    "total_counts = np.zeros(int((2**.5*512)**.5), dtype=int)\n",
    "for correct, sx, sy, tx, ty, pred_x, pred_y, sn, sm, tn, tm, category_id in tmp:\n",
    "    dist = ((sx-pred_x)**2 + (sy-pred_y)**2)**.25\n",
    "    error_counts[int(dist)] += not correct\n",
    "    total_counts[int(dist)] += 1\n",
    "error_rate = error_counts / total_counts\n",
    "\n",
    "plt.plot(np.arange(len(error_rate))**2, error_rate)\n",
    "plt.show()\n",
    "\n",
    "lowest_error_idx = error_rate[:-1].argmin()\n",
    "print(f'min error rate at {lowest_error_idx**2} px')\n",
    "for i in range(lowest_error_idx+1):\n",
    "    print(f'  {i**2} <= dist < {(i+1)**2} px: error rate {error_rate[i]:.2%}, count {total_counts[i]}')\n",
    "extra_errors = sum((error_rate[i]-error_rate[lowest_error_idx])*total_counts[i] for i in range(lowest_error_idx))\n",
    "print(f'-> extra errors: {extra_errors} ({extra_errors/total_counts.sum():.4%})')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error rate over prediction position relative to target position \n",
    "\n",
    "lim = 128\n",
    "downscale = 16\n",
    "lim_scaled_rel = (512-lim)//downscale\n",
    "\n",
    "fig, axs = plt.subplots(len(blocks), len(noise_steps), figsize=(len(blocks)*1, len(noise_steps)*1))\n",
    "for i, block in enumerate(tqdm(blocks, desc='blocks')):\n",
    "    axs[i,0].text(-0.8, 0.5, block_names[i], ha='right', va='center', transform=axs[i,0].transAxes)\n",
    "    for j, noise_step in enumerate(noise_steps):\n",
    "        tmp = data[i, j]\n",
    "\n",
    "        error_counts = np.zeros((2*512, 2*512), dtype=int)\n",
    "        total_counts = np.zeros((2*512, 2*512), dtype=int)\n",
    "        for correct, sx, sy, tx, ty, pred_x, pred_y, sn, sm, tn, tm, category_id in tmp:\n",
    "            error_counts[tx-pred_x+512, ty-pred_y+512] += not correct\n",
    "            total_counts[tx-pred_x+512, ty-pred_y+512] += 1\n",
    "\n",
    "        error_count_scaled = error_counts.reshape(len(error_counts)//downscale, downscale, len(error_counts)//downscale, downscale).mean(axis=(1,3))\n",
    "        total_count_scaled = total_counts.reshape(len(total_counts)//downscale, downscale, len(total_counts)//downscale, downscale).mean(axis=(1,3))\n",
    "        error_rate_scaled = error_count_scaled / total_count_scaled\n",
    "\n",
    "        axs[i,j].imshow(error_rate_scaled[lim_scaled_rel:-lim_scaled_rel, lim_scaled_rel:-lim_scaled_rel], origin='lower', extent=(-lim, lim, -lim, lim))\n",
    "        if i != len(blocks)-1: axs[i,j].set_xticks([])\n",
    "        else: axs[i,j].tick_params(labelsize=8)\n",
    "        if j != 0: axs[i,j].set_yticks([])\n",
    "        else: axs[i,j].tick_params(labelsize=8)\n",
    "        if i == len(blocks)-1: axs[-1,j].text(0.5, -0.5, noise_steps[j], ha='center', va='top', transform=axs[-1,j].transAxes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texture bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spair_data = datasets.load_dataset('0jl/SPair-71k', 'data', split='train', trust_remote_code=True)\n",
    "spair_pairs = datasets.load_dataset('0jl/SPair-71k', 'pairs', split='test', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spair_images = [x['img'] for x in tqdm(spair_data)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spair_images_max_x = 0\n",
    "spair_images_max_y = 0\n",
    "for img in spair_images:\n",
    "    spair_images_max_x = max(spair_images_max_x, img.size[0])\n",
    "    spair_images_max_y = max(spair_images_max_y, img.size[1])\n",
    "\n",
    "spair_images_data = np.zeros((len(spair_images), spair_images_max_y, spair_images_max_x, 3), dtype=np.uint8)\n",
    "for i, img in enumerate(tqdm(spair_images)):\n",
    "    spair_images_data[i, :img.size[1], :img.size[0], :] = np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spair_kps_list = []\n",
    "for pair in tqdm(spair_pairs):\n",
    "    for (sx, sy), (tx, ty) in zip(pair['src_kps'], pair['trg_kps']):\n",
    "        spair_kps_list.append((pair['src_data_index'], sx, sy, pair['trg_data_index'], tx, ty))\n",
    "\n",
    "spair_kps = np.array(spair_kps_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape, spair_kps.shape, spair_images_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_colors = spair_images_data[*spair_kps[:,[0,2,1]].T]\n",
    "trg_colors = spair_images_data[*spair_kps[:,[3,5,4]].T]\n",
    "\n",
    "src_colors.shape, trg_colors.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_diffs = np.abs(src_colors - trg_colors).mean(axis=-1)\n",
    "fig, axs = plt.subplots(len(blocks), 1, figsize=(4,2*len(blocks)))\n",
    "for i in range(len(blocks)):\n",
    "    correct_at_50 = data[i,3,:,0]\n",
    "    axs[i].hist(color_diffs[correct_at_50==1], bins=np.linspace(0, 256, 257), density=True, label='correct')\n",
    "    axs[i].hist(color_diffs[correct_at_50==0], bins=np.linspace(0, 256, 257), density=True, alpha=0.5, label='wrong')\n",
    "    axs[i].legend()\n",
    "    axs[i].set_title(block_names[i])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# if there is some color bias in some layers, we maybe would expect to be more correct for smaller color differences\n",
    "# this doesn't seem to be the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=7\n",
    "spair_images_data_padded = np.zeros((spair_images_data.shape[0], spair_images_data.shape[1]+2*d, spair_images_data.shape[2]+2*d, 3), dtype=spair_images_data.dtype)\n",
    "spair_images_data_padded[:,d:-d,d:-d,:] = spair_images_data\n",
    "src_regions = np.array([spair_images_data_padded[i, y:y+2*d+1, x:x+2*d+1] for i,x,y in spair_kps[:,[0,1,2]]])\n",
    "trg_regions = np.array([spair_images_data_padded[i, y:y+2*d+1, x:x+2*d+1] for i,x,y in spair_kps[:,[3,4,5]]])\n",
    "\n",
    "src_regions.shape, trg_regions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texture_diffs = np.abs(src_regions - trg_regions).mean(axis=-1).mean(axis=-1).mean(axis=-1)\n",
    "fig, axs = plt.subplots(len(blocks), 1, figsize=(4,2*len(blocks)))\n",
    "for i in range(len(blocks)):\n",
    "    correct_at_50 = data[i,3,:,0]\n",
    "    axs[i].hist(color_diffs[correct_at_50==1], bins=np.linspace(0, 256, 257), density=True, label='correct')\n",
    "    axs[i].hist(color_diffs[correct_at_50==0], bins=np.linspace(0, 256, 257), density=True, alpha=0.5, label='wrong')\n",
    "    axs[i].legend()\n",
    "    axs[i].set_title(block_names[i])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# if there is some texture bias in some layers, we maybe would expect to be more correct for smaller texture differences\n",
    "# this doesn't seem to be the case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
