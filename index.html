<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jonas Loos">

<title>An analysis of representation similarities in latent diffusion models and implications for representation extraction</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/quarto.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="index_files/libs/quarto-html/anchor.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="index_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="index_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="index_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="index_files/libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul class="collapse">
  <li><a href="#sec-introduction" id="toc-sec-introduction" class="nav-link active" data-scroll-target="#sec-introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#sec-related-work" id="toc-sec-related-work" class="nav-link" data-scroll-target="#sec-related-work">Related work</a></li>
  </ul></li>
  <li><a href="#sec-methods" id="toc-sec-methods" class="nav-link" data-scroll-target="#sec-methods">Methods</a>
  <ul class="collapse">
  <li><a href="#sec-methods-diffusion-models" id="toc-sec-methods-diffusion-models" class="nav-link" data-scroll-target="#sec-methods-diffusion-models">Diffusion Models</a></li>
  <li><a href="#sec-methods-representation-extraction" id="toc-sec-methods-representation-extraction" class="nav-link" data-scroll-target="#sec-methods-representation-extraction">Representation Extraction</a></li>
  <li><a href="#sec-representation-similarities" id="toc-sec-representation-similarities" class="nav-link" data-scroll-target="#sec-representation-similarities">Representation Similarities</a></li>
  <li><a href="#sec-methods-downstream-tasks" id="toc-sec-methods-downstream-tasks" class="nav-link" data-scroll-target="#sec-methods-downstream-tasks">Downstream Tasks</a></li>
  <li><a href="#sec-datasets" id="toc-sec-datasets" class="nav-link" data-scroll-target="#sec-datasets">Datasets</a></li>
  </ul></li>
  <li><a href="#sec-representation-extraction-exploration" id="toc-sec-representation-extraction-exploration" class="nav-link" data-scroll-target="#sec-representation-extraction-exploration">Representation Extraction and Exploration</a>
  <ul class="collapse">
  <li><a href="#sec-sdhelper" id="toc-sec-sdhelper" class="nav-link" data-scroll-target="#sec-sdhelper"><code>sdhelper</code> Package</a></li>
  <li><a href="#sec-repr-sim-explorer" id="toc-sec-repr-sim-explorer" class="nav-link" data-scroll-target="#sec-repr-sim-explorer">Representation Similarity Explorer</a></li>
  <li><a href="#sec-similarities" id="toc-sec-similarities" class="nav-link" data-scroll-target="#sec-similarities">Similarity Exploration</a></li>
  </ul></li>
  <li><a href="#sec-downstream-tasks" id="toc-sec-downstream-tasks" class="nav-link" data-scroll-target="#sec-downstream-tasks">Performance Evaluation on Downstream Tasks</a>
  <ul class="collapse">
  <li><a href="#sec-linear-probe-classification" id="toc-sec-linear-probe-classification" class="nav-link" data-scroll-target="#sec-linear-probe-classification">Linear Probe Classification</a></li>
  <li><a href="#sec-semantic-correspondence" id="toc-sec-semantic-correspondence" class="nav-link" data-scroll-target="#sec-semantic-correspondence">Semantic Correspondence</a></li>
  <li><a href="#sec-sc-improvements" id="toc-sec-sc-improvements" class="nav-link" data-scroll-target="#sec-sc-improvements">Improvements for Semantic Correspondence</a></li>
  </ul></li>
  <li><a href="#sec-biases" id="toc-sec-biases" class="nav-link" data-scroll-target="#sec-biases">Biases in the Representations</a>
  <ul class="collapse">
  <li><a href="#sec-position-bias" id="toc-sec-position-bias" class="nav-link" data-scroll-target="#sec-position-bias">Position Bias</a></li>
  <li><a href="#sec-texture-color-bias" id="toc-sec-texture-color-bias" class="nav-link" data-scroll-target="#sec-texture-color-bias">Texture and Color Bias</a></li>
  <li><a href="#sec-anomalies" id="toc-sec-anomalies" class="nav-link" data-scroll-target="#sec-anomalies">Anomalies</a></li>
  </ul></li>
  <li><a href="#sec-discussion" id="toc-sec-discussion" class="nav-link" data-scroll-target="#sec-discussion">Discussion</a>
  <ul class="collapse">
  <li><a href="#sec-discussion-results" id="toc-sec-discussion-results" class="nav-link" data-scroll-target="#sec-discussion-results">Summary of Results</a></li>
  <li><a href="#sec-limitations" id="toc-sec-limitations" class="nav-link" data-scroll-target="#sec-limitations">Limitations</a></li>
  <li><a href="#sec-future-work" id="toc-sec-future-work" class="nav-link" data-scroll-target="#sec-future-work">Future Work</a></li>
  </ul></li>
  <li><a href="#sec-conclusion" id="toc-sec-conclusion" class="nav-link" data-scroll-target="#sec-conclusion">Conclusion</a></li>
  <li><a href="#sec-acknowledgements" id="toc-sec-acknowledgements" class="nav-link" data-scroll-target="#sec-acknowledgements">Acknowledgements</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  <li><a href="#sec-appendix" id="toc-sec-appendix" class="nav-link" data-scroll-target="#sec-appendix">Appendix</a>
  <ul class="collapse">
  <li><a href="#sec-appendix-experimental-setup" id="toc-sec-appendix-experimental-setup" class="nav-link" data-scroll-target="#sec-appendix-experimental-setup">Experimental Setup</a></li>
  <li><a href="#sec-appendix-sd-models" id="toc-sec-appendix-sd-models" class="nav-link" data-scroll-target="#sec-appendix-sd-models">Stable Diffusion Models</a></li>
  <li><a href="#sec-appendix-position-bias" id="toc-sec-appendix-position-bias" class="nav-link" data-scroll-target="#sec-appendix-position-bias">Position Bias</a></li>
  <li><a href="#sec-appendix-texture-color-bias" id="toc-sec-appendix-texture-color-bias" class="nav-link" data-scroll-target="#sec-appendix-texture-color-bias">Texture and Color Bias</a></li>
  <li><a href="#sec-appendix-corner-anomalies" id="toc-sec-appendix-corner-anomalies" class="nav-link" data-scroll-target="#sec-appendix-corner-anomalies">Corner and Border Anomalies</a></li>
  <li><a href="#sec-appendix-repr-norms" id="toc-sec-appendix-repr-norms" class="nav-link" data-scroll-target="#sec-appendix-repr-norms">Representation Norms</a></li>
  </ul></li>
  </ul>
<div class="quarto-other-links"><h2>Other Links</h2><ul><li><a href="https://github.com/JonasLoos/thesis/releases/download/thesis-latest/thesis_an_analysis_of_representation_similarities_in_latent_diffusion_models_and_implications_for_representation_extraction.pdf"><i class="bi bi-link-45deg"></i>Download PDF</a></li><li><a href="https://github.com/JonasLoos/thesis"><i class="bi bi-link-45deg"></i>GitHub</a></li></ul></div></nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">An analysis of representation similarities in latent diffusion models and implications for representation extraction</h1>
<p class="subtitle lead">Master’s Thesis</p>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Jonas Loos </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            TU Berlin
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<section id="abstract" class="level2 unnumbered unlisted">
<h2 class="unnumbered unlisted anchored" data-anchor-id="abstract">Abstract</h2>
<p>Diffusion models have become a cornerstone of generative modeling, achieving state-of-the-art performance in producing high-quality outputs across diverse modalities, especially in image generation. Beyond their generative capabilities, these models encode meaningful semantic representations that can facilitate various downstream tasks, such as classification, semantic correspondence, and depth estimation. This thesis investigates the properties of diffusion model representations and their similarities, revealing biases that include sensitivity to absolute image positions, prioritization of texture and color over semantic content, and anomalies with high representation norms. By evaluating the representations on downstream tasks, we quantify the impact of these biases and their implications for representation quality. Our findings provide new insights and guidelines for leveraging latent diffusion models as representation learners in computer vision.</p>
<details>
<summary>
Definitions and Abbreviations
</summary>
<div id="details-definitions-abbreviations">
<section id="important-definitions" class="level2 unnumbered unlisted">
<h2 class="unnumbered unlisted anchored" data-anchor-id="important-definitions">Important Definitions</h2>
<ul>
<li><strong>Stable Diffusion</strong> (SD): a series of open-source latent diffusion models for image generation <span class="citation" data-cites="rombach2022highresolution">[<a href="#ref-rombach2022highresolution" role="doc-biblioref">1</a>]</span></li>
<li><strong>U-Net</strong>: a U-shaped neural network using an encoder-decoder architecture with skip connections <span class="citation" data-cites="ronneberger2015unet">[<a href="#ref-ronneberger2015unet" role="doc-biblioref">2</a>]</span></li>
<li><strong>Block</strong>: one submodule of the SD U-Net, e.g.&nbsp;<code>mid</code>, or <code>up[1]</code>; can also refer to the output of a block</li>
<li><strong>Upper/Lower Blocks</strong>: the blocks that are higher in the U-Net, i.e.&nbsp;closer to input/output (<code>conv-in</code>, <code>down[0]</code>, <code>up[3]</code>, …), or lower, i.e.&nbsp;near the center (<code>down[3]</code>, <code>mid</code>, <code>up[0]</code>, …); the transition is gradual, i.e.&nbsp;there is no sharp separation</li>
<li><strong>Layer</strong>: a part of a block, e.g.&nbsp;the <code>mid</code> block of SD-1.5 has the one attention layer and two ResNet layers; can also refer to the output of a layer</li>
<li><strong>Representation</strong>: the output of a block/layer, with shape (channels, height, width), e.g.&nbsp;(1280, 8, 8) for SD-1.5 <code>mid</code> block at the default image size; also called “feature map” in the literature <span class="citation" data-cites="tang2023emergent zhang2023tale">[<a href="#ref-tang2023emergent" role="doc-biblioref">3</a>, <a href="#ref-zhang2023tale" role="doc-biblioref">4</a>]</span></li>
<li><strong>(Representation) Token</strong>: a vector containing the channels values of a representation at a specific spatial position</li>
<li><strong>Colorwheel</strong>: an image containing a cyclic color gradient with the goal of better visualizing which regions of an image are mapped where</li>
</ul>
</section>
<section id="common-abbreviations" class="level2 unnumbered unlisted">
<h2 class="unnumbered unlisted anchored" data-anchor-id="common-abbreviations">Common Abbreviations</h2>
<ul>
<li><strong>SD</strong>: Stable Diffusion</li>
<li><strong>VAE</strong>: Variational Autoencoder</li>
<li><strong>PCK</strong>: Percentage of Correct Keypoints</li>
<li><strong>PCK@0.1<span class="math inline">\(_{\text{bbox}}\)</span></strong>: PCK at a threshold of 10% of the bounding box size</li>
<li><strong>MSE</strong>: Mean Squared Error</li>
<li><strong>RMSE</strong>: Root Mean Square Error</li>
<li><strong>RGB</strong>: Red Green Blue (image color channels)</li>
<li><strong>AI</strong>: Artificial Intelligence <!-- 
* PCA: Principal Component Analysis
 --></li>
</ul>
</section>
</div>
</details>
</section>
<section id="sec-introduction" class="level1">
<h1>Introduction</h1>
<p>Diffusion models <span class="citation" data-cites="pmlr-v37-sohl-dickstein15 ho2020denoising">[<a href="#ref-pmlr-v37-sohl-dickstein15" role="doc-biblioref">5</a>, <a href="#ref-ho2020denoising" role="doc-biblioref">6</a>]</span> have rapidly advanced the field of generative modeling, achieving state-of-the-art sample quality and training stability across various modalities, such as images <span class="citation" data-cites="dhariwal2021diffusion">[<a href="#ref-dhariwal2021diffusion" role="doc-biblioref">7</a>]</span>, video <span class="citation" data-cites="videoworldsimulators2024">[<a href="#ref-videoworldsimulators2024" role="doc-biblioref">8</a>]</span>, and audio <span class="citation" data-cites="kong2021diffwave">[<a href="#ref-kong2021diffwave" role="doc-biblioref">9</a>]</span>, but can also be used in other contexts like text or code generation <span class="citation" data-cites="singh2023codefusion">[<a href="#ref-singh2023codefusion" role="doc-biblioref">10</a>]</span>. Especially in the field of image generation, diffusion models are currently state-of-the-art <span class="citation" data-cites="dhariwal2021diffusion rombach2022highresolution nichol2022glidephotorealisticimagegeneration podell2023sdxl betker2023improving blackforestlabs2024fluxAnnouncement">[<a href="#ref-rombach2022highresolution" role="doc-biblioref">1</a>, <a href="#ref-dhariwal2021diffusion" role="doc-biblioref">7</a>, <a href="#ref-nichol2022glidephotorealisticimagegeneration" role="doc-biblioref">11</a>, <a href="#ref-podell2023sdxl" role="doc-biblioref">12</a>, <a href="#ref-betker2023improving" role="doc-biblioref">13</a>, <a href="#ref-blackforestlabs2024fluxAnnouncement" role="doc-biblioref">14</a>]</span>, which raises the question, if they also learn semantic representations of various concepts present in the training images. If yes, these representations could be used for various tasks in computer vision, such as semantic correspondence, semantic segmentation, depth estimation, and more. In this thesis, we set out to investigate the properties of the learned representations of diffusion models, the similarities between them, and their usefulness for downstream tasks.</p>
<p>Many tasks in computer vision would greatly benefit from meaningful representations that effectively capture the semantic information of input data, however, the task of learning such representations remains a key challenge in the field. There are different approaches to learning these meaningful representations of images, such as self-supervised learning with self-distillation in the case of the DINOv2 model <span class="citation" data-cites="oquab2024dinov">[<a href="#ref-oquab2024dinov" role="doc-biblioref">15</a>]</span>, or contrastive learning in the case of CLIP models <span class="citation" data-cites="radford2021learning">[<a href="#ref-radford2021learning" role="doc-biblioref">16</a>]</span>. Other options include using internal representations of vision models trained on other tasks, such as image classification models or diffusion models for image generation.</p>
<p>For these diffusion models, proprietary implementations lead the leaderboard in terms of image generation quality, but there are also multiple open-source diffusion models in the top 10 <span class="citation" data-cites="artificialanalysis2024quality">[<a href="#ref-artificialanalysis2024quality" role="doc-biblioref">17</a>]</span>. The most prominent series of open-source diffusion models for image generation is Stable Diffusion (SD), introduced by <span class="citation" data-cites="rombach2022highresolution">[<a href="#ref-rombach2022highresolution" role="doc-biblioref">1</a>]</span>. SD models sample images by iteratively transforming noise towards the target image distribution in the latent space of a pre-trained autoencoder. This transformation is achieved by removing noise using a U-Net <span class="citation" data-cites="ronneberger2015unet">[<a href="#ref-ronneberger2015unet" role="doc-biblioref">2</a>]</span> architecture, which is a U-shaped neural network.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>SD already received broad attention as foundation models providing representations that can be used for various tasks. Several works use SD U-Net representations for tasks such as semantic correspondence <span class="citation" data-cites="zhang2023tale zhang2024telling banani2024probing tang2023emergent luo2023dhf hedlin2023unsupervised li2023sd4match stracke2024clean fundel2024distillationdiffusionfeaturessemantic mariotti2024improving kim2025matchme">[<a href="#ref-tang2023emergent" role="doc-biblioref">3</a>, <a href="#ref-zhang2023tale" role="doc-biblioref">4</a>, <a href="#ref-zhang2024telling" role="doc-biblioref">18</a>, <a href="#ref-banani2024probing" role="doc-biblioref">19</a>, <a href="#ref-luo2023dhf" role="doc-biblioref">20</a>, <a href="#ref-hedlin2023unsupervised" role="doc-biblioref">21</a>, <a href="#ref-li2023sd4match" role="doc-biblioref">22</a>, <a href="#ref-stracke2024clean" role="doc-biblioref">23</a>, <a href="#ref-fundel2024distillationdiffusionfeaturessemantic" role="doc-biblioref">24</a>, <a href="#ref-mariotti2024improving" role="doc-biblioref">25</a>, <a href="#ref-kim2025matchme" role="doc-biblioref">26</a>]</span>, classification <span class="citation" data-cites="xiang2023denoising clark2024text li2023diffusion mukhopadhyay2023diffusion hudson2023soda stracke2024clean">[<a href="#ref-stracke2024clean" role="doc-biblioref">23</a>, <a href="#ref-xiang2023denoising" role="doc-biblioref">27</a>, <a href="#ref-clark2024text" role="doc-biblioref">28</a>, <a href="#ref-li2023diffusion" role="doc-biblioref">29</a>, <a href="#ref-mukhopadhyay2023diffusion" role="doc-biblioref">30</a>, <a href="#ref-hudson2023soda" role="doc-biblioref">31</a>]</span>, depth estimation <span class="citation" data-cites="Chen2023BeyondSS Patni2024ECoDepth zhao2023unleashing zhang2025three stracke2024clean">[<a href="#ref-stracke2024clean" role="doc-biblioref">23</a>, <a href="#ref-Chen2023BeyondSS" role="doc-biblioref">32</a>, <a href="#ref-Patni2024ECoDepth" role="doc-biblioref">33</a>, <a href="#ref-zhao2023unleashing" role="doc-biblioref">34</a>, <a href="#ref-zhang2025three" role="doc-biblioref">35</a>]</span>, semantic segmentation <span class="citation" data-cites="baranchuk2022labelefficient ji2024diffusion couairon2024zeroshot zhao2023unleashing tian2024diffuse zhang2025three couairon2024diffcutcatalyzingzeroshotsemantic Yang2023Diffusion">[<a href="#ref-zhao2023unleashing" role="doc-biblioref">34</a>, <a href="#ref-zhang2025three" role="doc-biblioref">35</a>, <a href="#ref-baranchuk2022labelefficient" role="doc-biblioref">36</a>, <a href="#ref-ji2024diffusion" role="doc-biblioref">37</a>, <a href="#ref-couairon2024zeroshot" role="doc-biblioref">38</a>, <a href="#ref-tian2024diffuse" role="doc-biblioref">39</a>, <a href="#ref-couairon2024diffcutcatalyzingzeroshotsemantic" role="doc-biblioref">40</a>, <a href="#ref-Yang2023Diffusion" role="doc-biblioref">41</a>]</span>, robot control <span class="citation" data-cites="gupta2024pretrained shridhar2024generativeimageactionmodels tsagkas2024clickgraspzeroshotprecise">[<a href="#ref-gupta2024pretrained" role="doc-biblioref">42</a>, <a href="#ref-shridhar2024generativeimageactionmodels" role="doc-biblioref">43</a>, <a href="#ref-tsagkas2024clickgraspzeroshotprecise" role="doc-biblioref">44</a>]</span>, and more <span class="citation" data-cites="ye2024stablenormalreducingdiffusionvariance de2024genziqa">[<a href="#ref-ye2024stablenormalreducingdiffusionvariance" role="doc-biblioref">45</a>, <a href="#ref-de2024genziqa" role="doc-biblioref">46</a>]</span>. However, comprehensive studies exploring the properties, visual biases, and anomalies of diffusion models remain scarce <span class="citation" data-cites="park2023understanding jaini2024intriguing tang2023emergent">[<a href="#ref-tang2023emergent" role="doc-biblioref">3</a>, <a href="#ref-park2023understanding" role="doc-biblioref">47</a>, <a href="#ref-jaini2024intriguing" role="doc-biblioref">48</a>]</span>, leaving significant gaps in the understanding of SD representations.</p>
<p>These representations are typically extracted from different parts of the U-Net, whereby the quality and usefulness of the representations heavily depends on the details of the representation extraction process <span class="citation" data-cites="fuest2024diffusionmodelsrepresentationlearning tang2023emergent">[<a href="#ref-tang2023emergent" role="doc-biblioref">3</a>, <a href="#ref-fuest2024diffusionmodelsrepresentationlearning" role="doc-biblioref">49</a>]</span>. The shape of the representations is dependent on the position in the U-Net, and consists of spatial dimensions that are relative to the size of the input image, downscaled by a power of two, and a channel dimension. We can compare the feature vectors at specific spatial positions, i.e.&nbsp;the different representation tokens, and analyze the similarities between them. The resulting similarity maps ideally show the semantic relationships between the different parts of the image and can be used for downstream tasks such as semantic correspondence.</p>
<p>In this thesis, we investigate the properties of the learned representations of SD, their similarities, and their usefulness for downstream tasks<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. A significant part is dedicated to investigating different biases, such as position bias, texture and color bias, and anomalous tokens with high norm in the learned representations. For this, we introduce novel approaches to demonstrate and quantify these biases. Our experiments often build upon the similarities between representations, but also include investigations of the representation norms and downstream tasks utilizing linear probes. The results tend to follow the structure of first observing and qualitatively studying a found property, followed by a quantitative analysis and an investigation of the impact on downstream tasks.</p>
<p>The main contributions of this thesis, sorted by relevance, are:</p>
<ul>
<li>An extensive investigation of different biases in the learned representations, including position bias, texture and color bias, and anomalies in the learned representations. To the best of our knowledge, this is the first work to systematically study the position bias and describe the high-norm anomalies occurring in the SD representations.</li>
<li>A detailed evaluation of the performance of the learned SD representations on the tasks of linear probe classification and semantic correspondence.</li>
<li>The development of representation extraction and exploration tools<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> to facilitate our analyses.</li>
</ul>
<p>The general structure of this thesis is as follows: After introducing relevant concepts and existing methods in <a href="#sec-related-work" class="quarto-xref">Section&nbsp;1.1</a>, we start by describing our approach to representation extraction, and the different tasks and datasets used in the experiments, in <a href="#sec-methods" class="quarto-xref">Section&nbsp;2</a>. We then detail our tools and results for representation extraction and exploration in <a href="#sec-representation-extraction-exploration" class="quarto-xref">Section&nbsp;3</a>. Next, we evaluate the performance of the learned SD representations on the tasks of linear probe classification and semantic correspondence, in <a href="#sec-downstream-tasks" class="quarto-xref">Section&nbsp;4</a>. Following this, we investigate the different biases qualitatively and quantitatively in <a href="#sec-biases" class="quarto-xref">Section&nbsp;5</a>. Finally, we discuss our results, the limitations, and future work in <a href="#sec-discussion" class="quarto-xref">Section&nbsp;6</a> and present final conclusions in <a href="#sec-conclusion" class="quarto-xref">Section&nbsp;7</a>. The appendix contains supplementary information about experiment setup, models, and additional results.</p>
<!-- 
potential additional topics:
* more representation learning
* XAI
* foundation models
-->
<section id="sec-related-work" class="level2">
<h2 class="anchored" data-anchor-id="sec-related-work">Related work</h2>
<p>This section details key literature on diffusion models, focusing on their use for representation learning. First, we introduce the general concept of diffusion models and SD in particular. Then, we discuss self-supervised representation learning, how diffusion models can be employed for this, and how the learned SD representations can be used for downstream tasks.</p>
<section id="diffusion-models" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="diffusion-models">Diffusion Models</h3>
<p>Initially inspired by the physical process of diffusion, diffusion models iteratively transform a distribution of noise into a desired target distribution through a sequence of learned reverse steps <span class="citation" data-cites="pmlr-v37-sohl-dickstein15 ho2020denoising">[<a href="#ref-pmlr-v37-sohl-dickstein15" role="doc-biblioref">5</a>, <a href="#ref-ho2020denoising" role="doc-biblioref">6</a>]</span>. In 2015, <span class="citation" data-cites="pmlr-v37-sohl-dickstein15">[<a href="#ref-pmlr-v37-sohl-dickstein15" role="doc-biblioref">5</a>]</span> introduced diffusion probabilistic models, but only in 2020 <span class="citation" data-cites="ho2020denoising">[<a href="#ref-ho2020denoising" role="doc-biblioref">6</a>]</span> showed that the diffusion process can be used to train a generative model for generating realistic high quality images using <em>Denoising Diffusion Probabilistic Models</em> (DDPM). They propose a parameterized Markov chain that reverses a gradual noising process over 1000 steps. The architecture of the denoising model they used is a U-Net <span class="citation" data-cites="ronneberger2015unet">[<a href="#ref-ronneberger2015unet" role="doc-biblioref">2</a>]</span>, a U-shaped neural network, as visualized in <a href="#fig-unet" class="quarto-xref">Figure&nbsp;1</a>. It consists of a series of down- and up-sampling blocks, which are connected by skip connections and a mid block at the lowest level. The outputs of the down blocks progressively increase in the channel dimensionality while the spatial dimensionality decreases, whereas this is reversed for the up blocks. <!-- Maybe mention DDIM: Replacing the Markov chain during sampling with a non-markovian forward process iteratively removing noise can lead to up to 50x faster sampling [@song2022denoising]. --></p>
<p>In 2021, <span class="citation" data-cites="dhariwal2021diffusion">[<a href="#ref-dhariwal2021diffusion" role="doc-biblioref">7</a>]</span> showed that, for image generation, diffusion models can outperform the previous state-of-the-art, generative adversarial networks <span class="citation" data-cites="goodfellow2020generative">[<a href="#ref-goodfellow2020generative" role="doc-biblioref">50</a>]</span>, in terms of sample quality and training stability. <span class="citation" data-cites="nichol2022glidephotorealisticimagegeneration">[<a href="#ref-nichol2022glidephotorealisticimagegeneration" role="doc-biblioref">11</a>]</span> showed that the diffusion process can be guided by text prompts, improving the usability of the models. Furthermore, <span class="citation" data-cites="rombach2022highresolution">[<a href="#ref-rombach2022highresolution" role="doc-biblioref">1</a>]</span> introduced the first version of SD, which employs the U-Net in the latent space of a pretrained variational autoencoder to increase computational efficiency. Following works have improved the quality of the generated images <span class="citation" data-cites="podell2023sdxl">[<a href="#ref-podell2023sdxl" role="doc-biblioref">12</a>]</span>, and computational efficiency <span class="citation" data-cites="sauer2023adversarial lin2024sdxllightning">[<a href="#ref-sauer2023adversarial" role="doc-biblioref">51</a>, <a href="#ref-lin2024sdxllightning" role="doc-biblioref">52</a>]</span>.</p>
<p>In general, there is a lot of progress in the field of image generation, with recent diffusion models reaching a level where general users cannot distinguish AI-generated art from human-created art anymore <span class="citation" data-cites="ha2024organicdiffuseddistinguishhuman">[<a href="#ref-ha2024organicdiffuseddistinguishhuman" role="doc-biblioref">53</a>]</span>. New alternative frameworks such as flow matching <span class="citation" data-cites="lipman2023flow">[<a href="#ref-lipman2023flow" role="doc-biblioref">54</a>]</span> promise to improve training and sampling speed, and new model architectures such as diffusion transformers <span class="citation" data-cites="peebles2023scalable esser2024scaling">[<a href="#ref-peebles2023scalable" role="doc-biblioref">55</a>, <a href="#ref-esser2024scaling" role="doc-biblioref">56</a>]</span> allow for better image quality, prompt adherence, and scalability. Furthermore, diffusion based video generation models have been released, such as SORA <span class="citation" data-cites="videoworldsimulators2024">[<a href="#ref-videoworldsimulators2024" role="doc-biblioref">8</a>]</span>, which are able to generate high quality videos.</p>
</section>
<section id="stable-diffusion" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="stable-diffusion">Stable Diffusion</h3>
<p>SD is a series of latent diffusion models for image generation <span class="citation" data-cites="rombach2022highresolution">[<a href="#ref-rombach2022highresolution" role="doc-biblioref">1</a>]</span>. Initially introduced in 2022, many follow-up checkpoints and models have been released by the original authors and others. The models SD-1.1 to SD-1.4 <span class="citation" data-cites="rombach2022highresolution">[<a href="#ref-rombach2022highresolution" role="doc-biblioref">1</a>]</span> were released by CompVis<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>, SD-1.5 <span class="citation" data-cites="rombach2022highresolution">[<a href="#ref-rombach2022highresolution" role="doc-biblioref">1</a>]</span> by Runway<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, and SD-2.0 <span class="citation" data-cites="rombach2022highresolution">[<a href="#ref-rombach2022highresolution" role="doc-biblioref">1</a>]</span>, SD-2.1 <span class="citation" data-cites="rombach2022highresolution">[<a href="#ref-rombach2022highresolution" role="doc-biblioref">1</a>]</span>, SD-Turbo <span class="citation" data-cites="sauer2023adversarial">[<a href="#ref-sauer2023adversarial" role="doc-biblioref">51</a>]</span>, SDXL <span class="citation" data-cites="podell2023sdxl">[<a href="#ref-podell2023sdxl" role="doc-biblioref">12</a>]</span>, and SDXL-Turbo <span class="citation" data-cites="sauer2023adversarial">[<a href="#ref-sauer2023adversarial" role="doc-biblioref">51</a>]</span> by Stability AI<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>. The main differences between the models are the number of parameters, the used text-encoders, the training schedule and the training data. They share a similar architecture and in particular have a similar U-Net structure. For more information on the different U-Net based models, see <a href="#sec-appendix-sd-models" class="quarto-xref">Section&nbsp;9.2</a>.</p>
<p>Most SD models have been made publicly available as open-source, e.g.&nbsp;on Hugging Face or GitHub. Different Hugging Face <code>diffusers</code> <span class="citation" data-cites="vonplaten2022diffusers">[<a href="#ref-vonplaten2022diffusers" role="doc-biblioref">57</a>]</span> pipelines allow the application of SD models for different tasks, such as text-to-image, image-to-image, inpainting, depth-to-image, and super-resolution <span class="citation" data-cites="hfsd">[<a href="#ref-hfsd" role="doc-biblioref">58</a>]</span>.</p>
<p>Recently, SD-3, SD-3.5 <span class="citation" data-cites="esser2024scaling stabilityai2024sd35">[<a href="#ref-esser2024scaling" role="doc-biblioref">56</a>, <a href="#ref-stabilityai2024sd35" role="doc-biblioref">59</a>]</span>, and state-of-the-art in image generation more generally, shifted towards diffusion transformers instead of U-Net based models <span class="citation" data-cites="peebles2023scalable esser2024scaling blackforestlabs2024fluxRepo blackforestlabs2024fluxAnnouncement yu2024representationalignmentgenerationtraining">[<a href="#ref-blackforestlabs2024fluxAnnouncement" role="doc-biblioref">14</a>, <a href="#ref-peebles2023scalable" role="doc-biblioref">55</a>, <a href="#ref-esser2024scaling" role="doc-biblioref">56</a>, <a href="#ref-blackforestlabs2024fluxRepo" role="doc-biblioref">60</a>, <a href="#ref-yu2024representationalignmentgenerationtraining" role="doc-biblioref">61</a>]</span>. For more information see <a href="#sec-future-work" class="quarto-xref">Section&nbsp;6.3</a>.</p>
</section>
<section id="self-supervised-representation-learning" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="self-supervised-representation-learning">Self-Supervised Representation Learning</h3>
<p>Self-supervised learning allows for the training of large-scale foundation models, including diffusion models, without explicit labels. Such foundation models can produce features rivaling those of fully supervised systems across a variety of downstream tasks, including both zero-shot and fine-tuning scenarios <span class="citation" data-cites="he2022maskedautoencoders oquab2024dinov radford2021learning">[<a href="#ref-oquab2024dinov" role="doc-biblioref">15</a>, <a href="#ref-radford2021learning" role="doc-biblioref">16</a>, <a href="#ref-he2022maskedautoencoders" role="doc-biblioref">62</a>]</span>. One can train these models on different objectives, such as inpainting <span class="citation" data-cites="he2022maskedautoencoders">[<a href="#ref-he2022maskedautoencoders" role="doc-biblioref">62</a>]</span>, predicting transformations <span class="citation" data-cites="gidaris2018unsupervisedrepresentationlearningpredicting">[<a href="#ref-gidaris2018unsupervisedrepresentationlearningpredicting" role="doc-biblioref">63</a>]</span>, or reordering patches <span class="citation" data-cites="misra2020selfsupervised noroozi2016unsupervised doersch2015unsupervised">[<a href="#ref-misra2020selfsupervised" role="doc-biblioref">64</a>, <a href="#ref-noroozi2016unsupervised" role="doc-biblioref">65</a>, <a href="#ref-doersch2015unsupervised" role="doc-biblioref">66</a>]</span>, as well as discriminative and contrastive learning strategies <span class="citation" data-cites="misra2020selfsupervised caron2021emerging oquab2024dinov chen2021exploring">[<a href="#ref-oquab2024dinov" role="doc-biblioref">15</a>, <a href="#ref-misra2020selfsupervised" role="doc-biblioref">64</a>, <a href="#ref-caron2021emerging" role="doc-biblioref">67</a>, <a href="#ref-chen2021exploring" role="doc-biblioref">68</a>]</span>. Generative models, including generative adversarial networks <span class="citation" data-cites="goodfellow2020generative">[<a href="#ref-goodfellow2020generative" role="doc-biblioref">50</a>]</span> and diffusion-based approaches, reveal that internal representations learned through the synthesis of images can also yield high-quality representations <span class="citation" data-cites="chen2016infogan baranchuk2022labelefficient">[<a href="#ref-baranchuk2022labelefficient" role="doc-biblioref">36</a>, <a href="#ref-chen2016infogan" role="doc-biblioref">69</a>]</span>.</p>
<p>Among these paradigms, DINO <span class="citation" data-cites="caron2021emerging">[<a href="#ref-caron2021emerging" role="doc-biblioref">67</a>]</span> and DINOv2 <span class="citation" data-cites="oquab2024dinov">[<a href="#ref-oquab2024dinov" role="doc-biblioref">15</a>]</span> are particularly notable for producing semantically coherent, dense descriptors via a self-distillation mechanism that enforces consistency under varying augmentations. Meanwhile, CLIP <span class="citation" data-cites="radford2021learning">[<a href="#ref-radford2021learning" role="doc-biblioref">16</a>]</span> exemplifies how contrastive learning across modalities (image-text) enables zero-shot generalization and robust transfer. Both approaches build upon vision transformers that are shown to contain artifacts in their attention maps, which slightly degrade performance in downstream tasks, but can be remedied by additional register tokens <span class="citation" data-cites="darcet2024vision">[<a href="#ref-darcet2024vision" role="doc-biblioref">70</a>]</span>.</p>
<p>By learning from unlabeled data at scale, self-supervised representation learning sets the stage for general-purpose representations that excel in a broad range of downstream applications. <!-- Maybe add references to the recent sucesses in unsupervised language model training--></p>
</section>
<section id="diffusion-models-for-representation-learning" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="diffusion-models-for-representation-learning">Diffusion Models for Representation Learning</h3>
<p>Diffusion models, and SD in particular, have been analyzed and used for representation learning in a variety of downstream tasks. While some works modify the model architecture or training process specifically for representation learning <span class="citation" data-cites="hudson2023soda chen2024deconstructingdenoisingdiffusionmodels">[<a href="#ref-hudson2023soda" role="doc-biblioref">31</a>, <a href="#ref-chen2024deconstructingdenoisingdiffusionmodels" role="doc-biblioref">71</a>]</span>, most works instead use the learned intermediate representations. Common SD versions used in the literature are SD-1.5 and SD-2.1 <span class="citation" data-cites="luo2023dhf zhao2023unleashing zhang2023tale zhang2024telling stracke2024clean linhardt2024analysis">[<a href="#ref-zhang2023tale" role="doc-biblioref">4</a>, <a href="#ref-zhang2024telling" role="doc-biblioref">18</a>, <a href="#ref-luo2023dhf" role="doc-biblioref">20</a>, <a href="#ref-stracke2024clean" role="doc-biblioref">23</a>, <a href="#ref-zhao2023unleashing" role="doc-biblioref">34</a>, <a href="#ref-linhardt2024analysis" role="doc-biblioref">72</a>]</span>. An interesting observation is that the performance on downstream tasks tends to increase with the extend of pretraining <span class="citation" data-cites="zhang2025three zhao2023unleashing">[<a href="#ref-zhao2023unleashing" role="doc-biblioref">34</a>, <a href="#ref-zhang2025three" role="doc-biblioref">35</a>]</span>.</p>
<p><strong>Localized Representations.</strong> Diffusion model representations are dense visual descriptors, where the information at a given spatial position in the representations corresponds to the image content at the corresponding position in the image <span class="citation" data-cites="zhang2023tale tang2023emergent luo2023dhf hedlin2023unsupervised li2023sd4match stracke2024clean fundel2024distillationdiffusionfeaturessemantic kim2025matchme ji2024diffusion tian2024diffuse">[<a href="#ref-tang2023emergent" role="doc-biblioref">3</a>, <a href="#ref-zhang2023tale" role="doc-biblioref">4</a>, <a href="#ref-luo2023dhf" role="doc-biblioref">20</a>, <a href="#ref-hedlin2023unsupervised" role="doc-biblioref">21</a>, <a href="#ref-li2023sd4match" role="doc-biblioref">22</a>, <a href="#ref-stracke2024clean" role="doc-biblioref">23</a>, <a href="#ref-fundel2024distillationdiffusionfeaturessemantic" role="doc-biblioref">24</a>, <a href="#ref-kim2025matchme" role="doc-biblioref">26</a>, <a href="#ref-ji2024diffusion" role="doc-biblioref">37</a>, <a href="#ref-tian2024diffuse" role="doc-biblioref">39</a>]</span>. This is the base for many downstream tasks, such as semantic correspondence, semantic segmentation, depth estimation, and surface normal estimation.</p>
<p><strong>Choice of layer.</strong> Recent surveys <span class="citation" data-cites="wang2024diffusionmodels3dvision fuest2024diffusionmodelsrepresentationlearning">[<a href="#ref-fuest2024diffusionmodelsrepresentationlearning" role="doc-biblioref">49</a>, <a href="#ref-wang2024diffusionmodels3dvision" role="doc-biblioref">73</a>]</span> provide comprehensive overviews of diffusion models for representation learning and their applications across various domains. Notably, most works find that the up-blocks of the U-Net contain the most useful representations for downstream tasks <span class="citation" data-cites="linhardt2024analysis zhang2023tale banani2024probing stracke2024clean">[<a href="#ref-zhang2023tale" role="doc-biblioref">4</a>, <a href="#ref-banani2024probing" role="doc-biblioref">19</a>, <a href="#ref-stracke2024clean" role="doc-biblioref">23</a>, <a href="#ref-linhardt2024analysis" role="doc-biblioref">72</a>]</span>. However, this is not universal across all applications - for instance, <span class="citation" data-cites="gupta2024pretrained">[<a href="#ref-gupta2024pretrained" role="doc-biblioref">42</a>]</span> find that down-blocks were more effective for robot control tasks and <span class="citation" data-cites="couairon2024diffcutcatalyzingzeroshotsemantic">[<a href="#ref-couairon2024diffcutcatalyzingzeroshotsemantic" role="doc-biblioref">40</a>]</span> use down block outputs for semantic segmentation. <span class="citation" data-cites="tang2023emergent">[<a href="#ref-tang2023emergent" role="doc-biblioref">3</a>]</span> suggest that up-blocks lower in the U-Net yield more semantically-aware representations, while up-blocks higher in the U-Net focus more on more low-level details. Instead of the output of U-Net blocks, some works also use the attention maps of the attention layers inside the blocks instead <span class="citation" data-cites="zhao2023unleashing hedlin2023unsupervised">[<a href="#ref-hedlin2023unsupervised" role="doc-biblioref">21</a>, <a href="#ref-zhao2023unleashing" role="doc-biblioref">34</a>]</span>.</p>
<p><strong>Choice of noise level.</strong> Before extracting the representations from the U-Net, the noise is usually added to the latent image, based on the time step <span class="math inline">\(t\)</span> for the noise-scheduler, ranging between <span class="math inline">\(0\)</span> and <span class="math inline">\(1000\)</span> for SD, with higher values indicating more noise <span class="citation" data-cites="rombach2022highresolution">[<a href="#ref-rombach2022highresolution" role="doc-biblioref">1</a>]</span>. Different works extract representations at varying noise levels, ranging from time step <span class="math inline">\(0\)</span> to <span class="math inline">\(500\)</span>. While <span class="citation" data-cites="zhao2023unleashing">[<a href="#ref-zhao2023unleashing" role="doc-biblioref">34</a>]</span> use no noise (<span class="math inline">\(t=0\)</span>), others use high noise levels of <span class="math inline">\(t=261\)</span> <span class="citation" data-cites="tang2023emergent li2023sd4match">[<a href="#ref-tang2023emergent" role="doc-biblioref">3</a>, <a href="#ref-li2023sd4match" role="doc-biblioref">22</a>]</span> or even <span class="math inline">\(500\)</span> <span class="citation" data-cites="li2023diffusion">[<a href="#ref-li2023diffusion" role="doc-biblioref">29</a>]</span>. Most works fall somewhere in between, with common time steps ranging from <span class="math inline">\(50\)</span> to <span class="math inline">\(100\)</span> <span class="citation" data-cites="banani2024probing zhang2025three xiang2023denoising mukhopadhyay2023diffusion zhang2023tale couairon2024diffcutcatalyzingzeroshotsemantic">[<a href="#ref-zhang2023tale" role="doc-biblioref">4</a>, <a href="#ref-banani2024probing" role="doc-biblioref">19</a>, <a href="#ref-xiang2023denoising" role="doc-biblioref">27</a>, <a href="#ref-mukhopadhyay2023diffusion" role="doc-biblioref">30</a>, <a href="#ref-zhang2025three" role="doc-biblioref">35</a>, <a href="#ref-couairon2024diffcutcatalyzingzeroshotsemantic" role="doc-biblioref">40</a>]</span>. Some approaches even use different noise levels for training versus inference <span class="citation" data-cites="li2023sd4match">[<a href="#ref-li2023sd4match" role="doc-biblioref">22</a>]</span>, learn <span class="math inline">\(t\)</span> via reinforcement learning <span class="citation" data-cites="Yang2023Diffusion">[<a href="#ref-Yang2023Diffusion" role="doc-biblioref">41</a>]</span>, or introduce a fine-tuning method to remove the need for noising <span class="citation" data-cites="stracke2024clean">[<a href="#ref-stracke2024clean" role="doc-biblioref">23</a>]</span>.</p>
<p><strong>Representation quality improvements.</strong> Many approaches have been proposed to further enhance the quality of the representations and to improve their suitability for downstream tasks. <span class="citation" data-cites="luo2023dhf">[<a href="#ref-luo2023dhf" role="doc-biblioref">20</a>]</span> consolidate multi-scale and multi-timestep representations intro per-pixel feature descriptors. <span class="citation" data-cites="li2023sd4match">[<a href="#ref-li2023sd4match" role="doc-biblioref">22</a>]</span> optimize the prompt conditioning to improve representation quality. <span class="citation" data-cites="zhang2023tale">[<a href="#ref-zhang2023tale" role="doc-biblioref">4</a>]</span> fuse the SD representations with DINOv2 <span class="citation" data-cites="oquab2024dinov">[<a href="#ref-oquab2024dinov" role="doc-biblioref">15</a>]</span> and apply dimensionality reduction. Building on this, <span class="citation" data-cites="zhang2024telling">[<a href="#ref-zhang2024telling" role="doc-biblioref">18</a>]</span> propose a test-time adaptive pose alignment strategy. <span class="citation" data-cites="stracke2024clean">[<a href="#ref-stracke2024clean" role="doc-biblioref">23</a>]</span> introduce an unsupervised fine-tuning method to reduce noise in the representations and to allow extraction of high quality representations at time step 0. <span class="citation" data-cites="fundel2024distillationdiffusionfeaturessemantic">[<a href="#ref-fundel2024distillationdiffusionfeaturessemantic" role="doc-biblioref">24</a>]</span> distill SD and DINOv2 representations into a student model, improving efficiency.</p>
<p>Notably, learning to generate images does not only lead to semantically coherent representations, but the connection also seems to go the other way around. <span class="citation" data-cites="yu2024representationalignmentgenerationtraining">[<a href="#ref-yu2024representationalignmentgenerationtraining" role="doc-biblioref">61</a>]</span> show that regularizing diffusion model training by including external visual representations leads to improvements in efficiency and quality.</p>
<p>Overall, representations of diffusion models and especially SD have been used widely, but there is no clear consensus on the optimal block and noise level for representation extraction. Many methods have been proposed to improve their quality and suitability for downstream tasks.</p>
</section>
<section id="sd-representations-for-downstream-tasks" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="sd-representations-for-downstream-tasks">SD Representations for Downstream Tasks</h3>
<p>Various works have investigated different aspects of the learned representations, finding that semantic information is captured in the bottleneck layers of the U-Net <span class="citation" data-cites="kwon2023diffusion park2023unsupervised">[<a href="#ref-kwon2023diffusion" role="doc-biblioref">74</a>, <a href="#ref-park2023unsupervised" role="doc-biblioref">75</a>]</span>. Further works have explored the suitability of extracted representations for classification <span class="citation" data-cites="xiang2023denoising clark2024text li2023diffusion mukhopadhyay2023diffusion stracke2024clean">[<a href="#ref-stracke2024clean" role="doc-biblioref">23</a>, <a href="#ref-xiang2023denoising" role="doc-biblioref">27</a>, <a href="#ref-clark2024text" role="doc-biblioref">28</a>, <a href="#ref-li2023diffusion" role="doc-biblioref">29</a>, <a href="#ref-mukhopadhyay2023diffusion" role="doc-biblioref">30</a>]</span>, semantic correspondence <span class="citation" data-cites="zhang2023tale zhang2024telling banani2024probing tang2023emergent luo2023dhf hedlin2023unsupervised li2023sd4match stracke2024clean fundel2024distillationdiffusionfeaturessemantic mariotti2024improving kim2025matchme">[<a href="#ref-tang2023emergent" role="doc-biblioref">3</a>, <a href="#ref-zhang2023tale" role="doc-biblioref">4</a>, <a href="#ref-zhang2024telling" role="doc-biblioref">18</a>, <a href="#ref-banani2024probing" role="doc-biblioref">19</a>, <a href="#ref-luo2023dhf" role="doc-biblioref">20</a>, <a href="#ref-hedlin2023unsupervised" role="doc-biblioref">21</a>, <a href="#ref-li2023sd4match" role="doc-biblioref">22</a>, <a href="#ref-stracke2024clean" role="doc-biblioref">23</a>, <a href="#ref-fundel2024distillationdiffusionfeaturessemantic" role="doc-biblioref">24</a>, <a href="#ref-mariotti2024improving" role="doc-biblioref">25</a>, <a href="#ref-kim2025matchme" role="doc-biblioref">26</a>]</span>, semantic segmentation <span class="citation" data-cites="baranchuk2022labelefficient ji2024diffusion couairon2024zeroshot zhao2023unleashing tian2024diffuse zhang2025three">[<a href="#ref-zhao2023unleashing" role="doc-biblioref">34</a>, <a href="#ref-zhang2025three" role="doc-biblioref">35</a>, <a href="#ref-baranchuk2022labelefficient" role="doc-biblioref">36</a>, <a href="#ref-ji2024diffusion" role="doc-biblioref">37</a>, <a href="#ref-couairon2024zeroshot" role="doc-biblioref">38</a>, <a href="#ref-tian2024diffuse" role="doc-biblioref">39</a>]</span>, depth estimation <span class="citation" data-cites="Chen2023BeyondSS Patni2024ECoDepth zhao2023unleashing zhang2025three stracke2024clean">[<a href="#ref-stracke2024clean" role="doc-biblioref">23</a>, <a href="#ref-Chen2023BeyondSS" role="doc-biblioref">32</a>, <a href="#ref-Patni2024ECoDepth" role="doc-biblioref">33</a>, <a href="#ref-zhao2023unleashing" role="doc-biblioref">34</a>, <a href="#ref-zhang2025three" role="doc-biblioref">35</a>]</span>, and for modification of the image generation process <span class="citation" data-cites="park2023unsupervised jeong2024trainingfree haas2023discovering gambashidze2024aligningdiffusionmodelsnoiseconditioned hudson2023soda park2023understanding">[<a href="#ref-hudson2023soda" role="doc-biblioref">31</a>, <a href="#ref-park2023understanding" role="doc-biblioref">47</a>, <a href="#ref-park2023unsupervised" role="doc-biblioref">75</a>, <a href="#ref-jeong2024trainingfree" role="doc-biblioref">76</a>, <a href="#ref-haas2023discovering" role="doc-biblioref">77</a>, <a href="#ref-gambashidze2024aligningdiffusionmodelsnoiseconditioned" role="doc-biblioref">78</a>]</span>. Other works investigated their alignment to human representations and human-like shape bias <span class="citation" data-cites="linhardt2024analysis jaini2024intriguing">[<a href="#ref-jaini2024intriguing" role="doc-biblioref">48</a>, <a href="#ref-linhardt2024analysis" role="doc-biblioref">72</a>]</span>. The representations have also been used for robotics tasks, such as robot control <span class="citation" data-cites="gupta2024pretrained shridhar2024generativeimageactionmodels">[<a href="#ref-gupta2024pretrained" role="doc-biblioref">42</a>, <a href="#ref-shridhar2024generativeimageactionmodels" role="doc-biblioref">43</a>]</span> and grasping <span class="citation" data-cites="tsagkas2024clickgraspzeroshotprecise">[<a href="#ref-tsagkas2024clickgraspzeroshotprecise" role="doc-biblioref">44</a>]</span>. Additionally, they have been applied to 3D scene understanding <span class="citation" data-cites="Man2024Lexicon3DPV">[<a href="#ref-Man2024Lexicon3DPV" role="doc-biblioref">79</a>]</span>, surface normal estimation <span class="citation" data-cites="Ke2024RepurposingDI Lee2024ExploitingDI xu2024diffusionmodelstrainedlarge ye2024stablenormalreducingdiffusionvariance">[<a href="#ref-ye2024stablenormalreducingdiffusionvariance" role="doc-biblioref">45</a>, <a href="#ref-Ke2024RepurposingDI" role="doc-biblioref">80</a>, <a href="#ref-Lee2024ExploitingDI" role="doc-biblioref">81</a>, <a href="#ref-xu2024diffusionmodelstrainedlarge" role="doc-biblioref">82</a>]</span>, and image quality assessment <span class="citation" data-cites="de2024genziqa">[<a href="#ref-de2024genziqa" role="doc-biblioref">46</a>]</span>.</p>
<p>In this thesis, we evaluate the SD representations on the tasks of linear probe classification, monocular depth estimation, and unsupervised zero-shot semantic correspondence.</p>
<p><strong>Linear probe classification</strong> utilizes a trainable linear layer on top of the representations for classification and is primarily only used to evaluate the quality of the representations. Common evaluation datasets are <a href="#sec-datasets-cifar"> CIFAR</a> <span class="citation" data-cites="Krizhevsky2009LearningML">[<a href="#ref-Krizhevsky2009LearningML" role="doc-biblioref">83</a>]</span> and <a href="#sec-datasets-imagenet"> Imagenet</a> <span class="citation" data-cites="ILSVRC15">[<a href="#ref-ILSVRC15" role="doc-biblioref">84</a>]</span>. <span class="citation" data-cites="mukhopadhyay2023diffusion">[<a href="#ref-mukhopadhyay2023diffusion" role="doc-biblioref">30</a>]</span> achieve an accuracy of up to 65.18% on <a href="#sec-datasets-imagenet"> Imagenet</a>-1k.</p>
<p><strong>Monocular depth estimation</strong> is the task of estimating the depth information (distance relative to camera) for each pixel based on a single RGB image. This can also be done using a linear probe <span class="citation" data-cites="stracke2024clean Chen2023BeyondSS">[<a href="#ref-stracke2024clean" role="doc-biblioref">23</a>, <a href="#ref-Chen2023BeyondSS" role="doc-biblioref">32</a>]</span>, more complex methods on top of the SD representations <span class="citation" data-cites="zhang2025three zhao2023unleashing Patni2024ECoDepth">[<a href="#ref-Patni2024ECoDepth" role="doc-biblioref">33</a>, <a href="#ref-zhao2023unleashing" role="doc-biblioref">34</a>, <a href="#ref-zhang2025three" role="doc-biblioref">35</a>]</span>, or even by finetuning or modifying the SD model itself <span class="citation" data-cites="Lee2024ExploitingDI He2024LotusDV Ke2024RepurposingDI xu2024diffusionmodelstrainedlarge fu2024geowizard zhang2024betterdepthplugandplaydiffusionrefiner zhang2024atlantis">[<a href="#ref-Ke2024RepurposingDI" role="doc-biblioref">80</a>, <a href="#ref-Lee2024ExploitingDI" role="doc-biblioref">81</a>, <a href="#ref-xu2024diffusionmodelstrainedlarge" role="doc-biblioref">82</a>, <a href="#ref-He2024LotusDV" role="doc-biblioref">85</a>, <a href="#ref-fu2024geowizard" role="doc-biblioref">86</a>, <a href="#ref-zhang2024betterdepthplugandplaydiffusionrefiner" role="doc-biblioref">87</a>, <a href="#ref-zhang2024atlantis" role="doc-biblioref">88</a>]</span>. <span class="citation" data-cites="stracke2024clean">[<a href="#ref-stracke2024clean" role="doc-biblioref">23</a>]</span> report a root mean squared error (RMSE) of 0.469 on the <a href="#sec-datasets-nyu-v2"> NYU Depth v2</a> <span class="citation" data-cites="Silberman2012Indoor">[<a href="#ref-Silberman2012Indoor" role="doc-biblioref">89</a>]</span> dataset using a linear probe on the SD-2.1 representations, while <span class="citation" data-cites="Patni2024ECoDepth">[<a href="#ref-Patni2024ECoDepth" role="doc-biblioref">33</a>]</span> report a RMSE of as low as 0.218 for their method using additional contextual embeddings and representation upsampling.</p>
<p><strong>Unsupervised zero-shot semantic correspondence</strong> is the task of finding corresponding keypoints between two images without any examples or further training. Notable methods using SD representations include DIFT <span class="citation" data-cites="tang2023emergent">[<a href="#ref-tang2023emergent" role="doc-biblioref">3</a>]</span>, which achieves a performance of 61.2 percentage correct keypoints (PCK) on the <a href="#sec-datasets-spair"> SPair-71k</a> <span class="citation" data-cites="min2019spair71k">[<a href="#ref-min2019spair71k" role="doc-biblioref">90</a>]</span> dataset. Pairing the SD representations with DINOv2 representations <span class="citation" data-cites="zhang2023tale">[<a href="#ref-zhang2023tale" role="doc-biblioref">4</a>]</span> increases the performance to 64.0 PCK and adding test-time pose alignment <span class="citation" data-cites="zhang2024telling">[<a href="#ref-zhang2024telling" role="doc-biblioref">18</a>]</span> leads to 69.6 PCK. Adding fine-tuning for representation denoising <span class="citation" data-cites="stracke2024clean">[<a href="#ref-stracke2024clean" role="doc-biblioref">23</a>]</span> results in 69.99 PCK, the current state-of-the-art in unsupervised zero-shot semantic correspondence on the <a href="#sec-datasets-spair"> SPair-71k</a> dataset.</p>
<!-- Could do: add dense correspondence (maybe not necessary, as it can be considered a special case of semantic correspondence) -->
</section>
</section>
</section>
<section id="sec-methods" class="level1">
<h1>Methods</h1>
<p>In this thesis, we primarily extract representations from SD-1.5, partly complemented by SD-2.1, SD-Turbo, SDXL, and SDXL-Turbo to show the generalizability of our findings. The extracted representations consist of representation tokens over the different spatial positions, over which different similarity measures can be evaluated. We use either these similarities or linear probes as a base for downstream tasks on various datasets. The downstream tasks we use to assess the performance of the SD models are linear probe classification, semantic correspondence, dense correspondence, and depth estimation. In the following, we first describe diffusion models, representation extraction, and representation similarities. Subsequently, we detail the downstream tasks and datasets used in this thesis.</p>
<section id="sec-methods-diffusion-models" class="level2">
<h2 class="anchored" data-anchor-id="sec-methods-diffusion-models">Diffusion Models</h2>
<p>Diffusion models define a generative process that transforms noise into complex samples, in our case images, by reversing a predefined forward diffusion process. Following the DDPM formulation by <span class="citation" data-cites="ho2020denoising">[<a href="#ref-ho2020denoising" role="doc-biblioref">6</a>]</span>, let <span class="math inline">\(\mathbf{x}_0 \in \mathbb{R}^d\)</span> be an input image. The forward process transforms <span class="math inline">\(\mathbf{x}_0\)</span> into a noisy version <span class="math inline">\(\mathbf{x}_t\)</span> at step <span class="math inline">\(t\)</span> through linear interpolation with Gaussian noise:</p>
<p><span class="math display">\[\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\,\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\,\boldsymbol{\epsilon},\]</span></p>
<p>with <span class="math inline">\(\boldsymbol{\epsilon}\sim \mathcal{N}(0,\mathbf{I})\)</span> and <span class="math inline">\(\bar{\alpha}_t=\prod_{s=1}^t \alpha_s\)</span>, given a noise schedule <span class="math inline">\(\{\alpha_t\}_{t=1}^T\)</span> where <span class="math inline">\(0&lt;\alpha_t&lt;1\)</span>. As <span class="math inline">\(t \to T\)</span>, <span class="math inline">\(\mathbf{x}_t\)</span> approaches pure noise.</p>
<p>The goal is to approximate the reverse process that removes noise step-by-step:</p>
<p><span class="math display">\[p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \approx q(\mathbf{x}_{t-1} \mid \mathbf{x}_t,\mathbf{x}_0),\]</span></p>
<p>where <span class="math inline">\(q\)</span> denotes the true (but intractable) reverse distribution that is approximated by the parameterized Gaussian distribution <span class="math inline">\(p_\theta\)</span>, with <span class="math inline">\(\theta\)</span> being the model weights. To achieve this, a model <span class="math inline">\(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,t)\)</span> is trained to predict the noise <span class="math inline">\(\boldsymbol{\epsilon}\)</span> present at each step, from which the mean of <span class="math inline">\(p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)\)</span> can be derived. A common training objective is to minimize the mean squared error between the predicted and true noise:</p>
<p><span class="math display">\[\mathcal{L}(\theta) = \mathbb{E}_{\mathbf{x}_0,\boldsymbol{\epsilon},t}\bigl[\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,t)\|^2\bigr],\]</span></p>
<p>where <span class="math inline">\(\mathbf{x}_0\)</span> is sampled from the training data distribution, <span class="math inline">\(t\)</span> is typically chosen uniformly at random from <span class="math inline">\(\{1,\dots,T\}\)</span>, and <span class="math inline">\(\boldsymbol{\epsilon}\)</span> is drawn from a standard normal distribution <span class="math inline">\(\mathcal{N}(0,\mathbf{I})\)</span>. Given these samples, we form the noisy image <span class="math inline">\(\mathbf{x}_t\)</span> using the forward process. After training, new samples are generated by starting from pure noise <span class="math inline">\(\mathbf{x}_T \sim \mathcal{N}(0,\mathbf{I})\)</span> and iteratively applying the learned reverse transitions until <span class="math inline">\(\mathbf{x}_0\)</span> is obtained.</p>
<p>In the case of SD, the denoising model <span class="math inline">\(\boldsymbol{\epsilon}_\theta\)</span> is typically a U-Net <span class="citation" data-cites="ronneberger2015unet rombach2022highresolution">[<a href="#ref-rombach2022highresolution" role="doc-biblioref">1</a>, <a href="#ref-ronneberger2015unet" role="doc-biblioref">2</a>]</span>. As SD is a series of latent diffusion models, the generative process is not applied in pixel space, but rather in the latent space of a pretrained variational autoencoder (VAE) <span class="citation" data-cites="rombach2022highresolution">[<a href="#ref-rombach2022highresolution" role="doc-biblioref">1</a>]</span>. This means that <span class="math inline">\(x_0\)</span> is the output of the VAE encoder.</p>
</section>
<section id="sec-methods-representation-extraction" class="level2">
<h2 class="anchored" data-anchor-id="sec-methods-representation-extraction">Representation Extraction</h2>
<div id="cell-fig-unet" class="cell" data-execution_count="3">
<div class="cell-output cell-output-display">
<div id="fig-unet" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-unet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-unet-output-1.png" width="585" height="278" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-unet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: SD U-Net architecture with variational autoencoder (VAE) in the representation extraction pipeline. The VAE encoder is not used during image generation, while the VAE decoder is not used during representation extraction for existing images. Representations are extracted at the end of each block.
</figcaption>
</figure>
</div>
</div>
</div>
<p>For our analyses, we primarily focus on SD-1.5, partly complemented by SD-2.1, SD-Turbo, SDXL, or/and SDXL-Turbo. This decision is led by the popularity of SD-1.5 and that running all models for all experiments would be computationally expensive. If not stated otherwise, results are for SD-1.5.</p>
<p>The SD U-Net, visualized in <a href="#fig-unet" class="quarto-xref">Figure&nbsp;1</a>, consists of a series of down- and up-blocks, which are connected by skip connections and a mid block at the lowest level. Each block consists of a combination of ResNet and attention layers, and a final down- or up-sampling operation where applicable. The output of these blocks are the representations we use. They have shape <span class="math inline">\((w, h, c) \in \mathbb{N}^3\)</span>, with <span class="math inline">\(w\)</span> and <span class="math inline">\(h\)</span> being width and height, and <span class="math inline">\(c\)</span> being the number of channels. One can also extract the representations from the different layers in the blocks, however, we primarily focus on the block outputs. The number of spatial dimensions and channels depends on the input image size and the block, as described in <a href="#tbl-repr-shapes" class="quarto-xref">Table&nbsp;1</a>. We refer to the representation at a given spatial position as a token, with shape <span class="math inline">\((c)\)</span>.</p>
<div id="tbl-repr-shapes" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-repr-shapes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Representation shapes (width, height, channels) for different SD models and blocks. All values are for the default image sizes of the respective models, which is noted as <code>Input</code> in the table. SDXL and SDXL-Turbo only have 3 <code>down</code> and <code>up</code> blocks.
</figcaption>
<div aria-describedby="tbl-repr-shapes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 11%">
<col style="width: 17%">
<col style="width: 17%">
<col style="width: 17%">
<col style="width: 17%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Block</th>
<th style="text-align: center;">SD-1.5</th>
<th style="text-align: center;">SD-2.1</th>
<th style="text-align: center;">SD-Turbo</th>
<th style="text-align: center;">SDXL</th>
<th style="text-align: center;">SDXL-Turbo</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>Input</code></td>
<td style="text-align: center;">512, 512, 3</td>
<td style="text-align: center;">768, 768, 3</td>
<td style="text-align: center;">512, 512, 3</td>
<td style="text-align: center;">1024, 1024, 3</td>
<td style="text-align: center;">512, 512, 3</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>conv-in</code></td>
<td style="text-align: center;">64, 64, 320</td>
<td style="text-align: center;">96, 96, 320</td>
<td style="text-align: center;">64, 64, 320</td>
<td style="text-align: center;">128, 128, 320</td>
<td style="text-align: center;">64, 64, 320</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>down[0]</code></td>
<td style="text-align: center;">32, 32, 320</td>
<td style="text-align: center;">48, 48, 320</td>
<td style="text-align: center;">32, 32, 320</td>
<td style="text-align: center;">64, 64, 320</td>
<td style="text-align: center;">32, 32, 320</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>down[1]</code></td>
<td style="text-align: center;">16, 16, 640</td>
<td style="text-align: center;">24, 24, 640</td>
<td style="text-align: center;">16, 16, 640</td>
<td style="text-align: center;">32, 32, 640</td>
<td style="text-align: center;">16, 16, 640</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>down[2]</code></td>
<td style="text-align: center;">8, 8, 1280</td>
<td style="text-align: center;">12, 12, 1280</td>
<td style="text-align: center;">8, 8, 1280</td>
<td style="text-align: center;">32, 32, 1280</td>
<td style="text-align: center;">16, 16, 1280</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>down[3]</code></td>
<td style="text-align: center;">8, 8, 1280</td>
<td style="text-align: center;">12, 12, 1280</td>
<td style="text-align: center;">8, 8, 1280</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>mid</code></td>
<td style="text-align: center;">8, 8, 1280</td>
<td style="text-align: center;">12, 12, 1280</td>
<td style="text-align: center;">8, 8, 1280</td>
<td style="text-align: center;">32, 32, 1280</td>
<td style="text-align: center;">16, 16, 1280</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>up[0]</code></td>
<td style="text-align: center;">16, 16, 1280</td>
<td style="text-align: center;">24, 24, 1280</td>
<td style="text-align: center;">16, 16, 1280</td>
<td style="text-align: center;">64, 64, 1280</td>
<td style="text-align: center;">32, 32, 1280</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>up[1]</code></td>
<td style="text-align: center;">32, 32, 1280</td>
<td style="text-align: center;">48, 48, 1280</td>
<td style="text-align: center;">32, 32, 1280</td>
<td style="text-align: center;">128, 128, 640</td>
<td style="text-align: center;">64, 64, 640</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>up[2]</code></td>
<td style="text-align: center;">64, 64, 640</td>
<td style="text-align: center;">96, 96, 640</td>
<td style="text-align: center;">64, 64, 640</td>
<td style="text-align: center;">128, 128, 320</td>
<td style="text-align: center;">64, 64, 320</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>up[3]</code></td>
<td style="text-align: center;">64, 64, 320</td>
<td style="text-align: center;">96, 96, 320</td>
<td style="text-align: center;">64, 64, 320</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>conv-out</code></td>
<td style="text-align: center;">64, 64, 4</td>
<td style="text-align: center;">96, 96, 4</td>
<td style="text-align: center;">64, 64, 4</td>
<td style="text-align: center;">4, 128, 128</td>
<td style="text-align: center;">4, 64, 64</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The SD U-Net can be broadly categorized into upper blocks (<code>conv-in</code>, <code>down[0]</code>, <code>down[1]</code>, <code>up[2]</code>, <code>up[3]</code>), and lower blocks (<code>down[2]</code>, <code>down[3]</code>, <code>mid</code>, <code>up[0]</code>, <code>up[1]</code>). While upper and lower blocks tend to have different properties and biases, the distinction is usually not clear-cut, but rather a continuum. The output of <code>conv-out</code> is not used for any downstream tasks, as it does not directly contain semantic information, but rather the noise prediction. We therefore exclude it from most of our experiments.</p>
<p>As indicated in <a href="#fig-unet" class="quarto-xref">Figure&nbsp;1</a>, to extract representations of a given image, the image is first fed through the encoder of the VAE. Then, Gaussian noise is added according to a given time step for the noise-scheduler, ranging between 0 and 1000 for SD, with higher values indicating more noise, as visualized in <a href="#fig-noise-levels" class="quarto-xref">Figure&nbsp;2</a>. Finally, it is fed into the U-Net, and the representations are extracted from the desired blocks. For example, an image of shape <span class="math inline">\((512, 512, 3)\)</span> fed into SD-1.5 has shape <span class="math inline">\((64, 64, 4)\)</span> in the latent space of the VAE, and shape <span class="math inline">\((8, 8, 1280)\)</span> at the <code>mid</code> block.</p>
<div id="cell-fig-noise-levels" class="cell" data-execution_count="4">
<div class="cell-output cell-output-display">
<div id="fig-noise-levels" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-noise-levels-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-noise-levels-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-noise-levels-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Example of different noise levels, i.e.&nbsp;time steps <span class="math inline">\(t\)</span>. The image is encoded using the VAE, interpolated with Gaussian noise, and then decoded back to image space.
</figcaption>
</figure>
</div>
</div>
</div>
<p>As this thesis focuses on the representations, when referring to a U-Net block, we are not talking about the block itself, i.e.&nbsp;its architecture or parameters, but rather about the representations extracted from it.</p>
<p>The amount of noise added to the latent image is a hotly debated topic in the literature. As described in <a href="#sec-related-work" class="quarto-xref">Section&nbsp;1.1</a>, time steps <span class="math inline">\(t\)</span> between <span class="math inline">\(0\)</span> and <span class="math inline">\(500\)</span> are used, with common values ranging between <span class="math inline">\(50\)</span> and <span class="math inline">\(100\)</span>. For our experiments presented in <a href="#sec-downstream-tasks" class="quarto-xref">Section&nbsp;4</a>, we find time step <span class="math inline">\(t=50\)</span> to be a good choice. However, slightly lower or moderately higher values tend to yield similar results. Unless stated otherwise, we use time step 50 as default for all our experiments.</p>
</section>
<section id="sec-representation-similarities" class="level2">
<h2 class="anchored" data-anchor-id="sec-representation-similarities">Representation Similarities</h2>
<p>As described in <a href="#sec-methods-representation-extraction" class="quarto-xref">Section&nbsp;2.2</a>, SD representations have shape <span class="math inline">\((w, h, c)\)</span>. Representation tokens, i.e.&nbsp;the channel vectors at given spatial positions, correspond semantically to the image content at the corresponding position in the image. This property is widely described in the literature, as detailed in <a href="#sec-related-work" class="quarto-xref">Section&nbsp;1.1</a>, and visualized in <a href="#sec-similarities" class="quarto-xref">Section&nbsp;3.3</a>. It is the basis for using similarities between tokens as a measure of semantic similarity between image regions.</p>
<p>Following common practice <span class="citation" data-cites="amir2022deepvitfeaturesdense muttenthaler2023human stracke2024clean">[<a href="#ref-stracke2024clean" role="doc-biblioref">23</a>, <a href="#ref-amir2022deepvitfeaturesdense" role="doc-biblioref">91</a>, <a href="#ref-muttenthaler2023human" role="doc-biblioref">92</a>]</span>, we use cosine similarity to measure the similarity between the representation tokens, which is defined as</p>
<p><span class="math display">\[ \text{cosine similarity}(R_{x,y}, R'_{x',y'}) = \frac{R_{x,y} \cdot R'_{x',y'}}{||R_{x,y}|| \cdot ||R'_{x',y'}||} \]</span></p>
<p>where <span class="math inline">\(R\)</span> and <span class="math inline">\(R'\)</span> are the two representations of the same block, but potentially of different images. <span class="math inline">\(R_{x,y}\)</span> is the token at spatial position <span class="math inline">\((x, y) \in \{1,...,w\}\times\{1,...,h\}\)</span> of representation <span class="math inline">\(R\)</span>.</p>
<p>Instead of cosine similarity, also other similarity measures or distance metrics can be used. Options include cosine similarity without normalization, i.e.&nbsp;the dot product, or cosine similarity with centering the representations first, i.e.&nbsp;subtracting the mean of each representation. When using distance metrics, their results need to be inverted to be used as a similarity measure, so that higher values indicate higher similarity. Common distance metrics are the Euclidean distance (<span class="math inline">\(L_2\)</span>), the Manhattan distance (<span class="math inline">\(L_1\)</span>), the Chebyshev distance (<span class="math inline">\(L_\infty\)</span>), or more generally the distance metrics given by the <span class="math inline">\(L_p\)</span> norm, defined as</p>
<p><span class="math display">\[ L_p(R_{x,y}, R'_{x',y'}) = \left| \sum_{i=1}^c (R_{x,y,i} - R'_{x',y',i})^p \right|^\frac{1}{p} \]</span></p>
<p>where <span class="math inline">\(p \in [1, \infty)\)</span> is the order of the norm.</p>
<p>Instead of calculating the similarity between the tokens of one representation, one can also concatenate multiple representations along the channel dimension. If the spatial representations shapes differ, we use nearest neighbor upsampling on the smaller representations. For this, tokens are duplicated until the representation shapes match. While this approach can combine the semantic information of the different representations, the resulting representations are often very large, which can be a disadvantage in resource-constrained environments.</p>
<p>Measuring the similarity between the representation tokens is the basis for several downstream tasks. In this thesis, we use cosine similarity as a basis for semantic correspondence and dense correspondence tasks, as described in <a href="#sec-methods-downstream-tasks" class="quarto-xref">Section&nbsp;2.4</a>.</p>
</section>
<section id="sec-methods-downstream-tasks" class="level2">
<h2 class="anchored" data-anchor-id="sec-methods-downstream-tasks">Downstream Tasks</h2>
<p>To evaluate the quality and properties of the extracted representations, we use several downstream tasks. These tasks either work on the similarity maps between representations, this includes semantic and dense correspondence, or using a linear probe on the representations, which is the case for classification and depth estimation.</p>
<section id="sec-methods-lpc" class="level3">
<h3 class="anchored" data-anchor-id="sec-methods-lpc">Linear Probe Classification</h3>
<p>A conceptually simple way to evaluate the extracted representations from the U-Net of diffusion models, is the usage of a linear probe classifier <span class="citation" data-cites="xiang2023denoising hudson2023soda mukhopadhyay2023diffusion">[<a href="#ref-xiang2023denoising" role="doc-biblioref">27</a>, <a href="#ref-mukhopadhyay2023diffusion" role="doc-biblioref">30</a>, <a href="#ref-hudson2023soda" role="doc-biblioref">31</a>]</span>. The key idea is to assess how well the representations encode semantic information, while keeping the classifier architecture minimal to focus on the quality of the representations themselves.</p>
<p>We use the datasets <a href="#sec-datasets-cifar"> CIFAR</a>-10 <span class="citation" data-cites="Krizhevsky2009LearningML">[<a href="#ref-Krizhevsky2009LearningML" role="doc-biblioref">83</a>]</span> with 10 classes, <a href="#sec-datasets-cifar"> CIFAR</a>-100 <span class="citation" data-cites="Krizhevsky2009LearningML">[<a href="#ref-Krizhevsky2009LearningML" role="doc-biblioref">83</a>]</span> with 100 classes, and Tiny-<a href="#sec-datasets-imagenet"> Imagenet</a> <span class="citation" data-cites="ILSVRC15">[<a href="#ref-ILSVRC15" role="doc-biblioref">84</a>]</span> with 200 classes. We upscale images to 512 px and extract representations from a given block of the U-Net. As the representations have both spatial and channel dimensions, we average over the spatial dimensions to obtain a single feature vector per image <span class="citation" data-cites="xiang2023denoising linhardt2024analysis">[<a href="#ref-xiang2023denoising" role="doc-biblioref">27</a>, <a href="#ref-linhardt2024analysis" role="doc-biblioref">72</a>]</span>.</p>
<p>We then train a linear layer that maps from these averaged representations to class probabilities. For training, we use cross-entropy loss and the Adam optimizer <span class="citation" data-cites="kingma2017adam">[<a href="#ref-kingma2017adam" role="doc-biblioref">93</a>]</span>. The linear layer consists of a weight matrix <span class="math inline">\(W\)</span> and bias vector <span class="math inline">\(b\)</span>, with the number of output neurons matching the number of classes in the dataset. The class prediction <span class="math inline">\(\hat{y}\)</span> for a representation <span class="math inline">\(R\)</span> is defined as follows:</p>
<p><span class="math display">\[ \hat{y} = \underset{i}{\text{argmax}} \left( \left( \frac{1}{w \cdot h} \sum_{x,y} R_{x,y} \right) \cdot W + b \right)_i \]</span></p>
<p>The advantage of a linear probe lies in its limited complexity - it can only learn linear combinations of the input features. This restriction means that if the classifier achieves good performance, the original representations must already encode the relevant semantic information in a linearly separable way. Conversely, if a linear classifier struggles despite sufficient training data, this suggests that the probed class concepts are either not well captured or are encoded in a way that requires more complex transformations to be useful for classification.</p>
</section>
<section id="sec-methods-sc" class="level3">
<h3 class="anchored" data-anchor-id="sec-methods-sc">Semantic Correspondence</h3>
<p>Semantic correspondence is the task of finding the corresponding keypoint in a target image for a given keypoint in a source image. For example, the left ear of a cat in a source image should be matched to the left ear of a cat in a target image. This task requires a semantic understanding of image content and object positions. We primarily use <a href="#sec-datasets-spair"> SPair-71k</a> <span class="citation" data-cites="min2019spair71k">[<a href="#ref-min2019spair71k" role="doc-biblioref">90</a>]</span> in our experiments, one of the most commonly used datasets for semantic correspondence.</p>
<p>To find corresponding points between two images, we first extract representations from both images at a specific block of the U-Net. For each source keypoint <span class="math inline">\((x_\text{src}, y_\text{src}) \in \{1,...,w\}\times\{1,...,h\}\)</span> in the source image’s representation <span class="math inline">\(R_\text{src} \in \mathbb{R}^{w \times h \times c}\)</span>, we then compute the cosine similarity with all spatial positions in the target image’s representation <span class="math inline">\(R_\text{trg} \in \mathbb{R}^{w \times h \times c}\)</span>. The position with the highest similarity score is predicted as the corresponding point <span class="math inline">\((\hat{x}_\text{trg}, \hat{y}_\text{trg}) \in \{1,...,w\}\times\{1,...,h\}\)</span>.</p>
<p><span class="math display">\[ (\hat{x}_\text{trg}, \hat{y}_\text{trg}) = \underset{(x, y)}{\text{argmax}}\ \frac{R_{\text{src},x_\text{src}, y_\text{src}} \cdot R_{\text{trg}, x, y}}{ ||R_{\text{src},x_\text{src}, y_\text{src}}|| \cdot ||R_{\text{trg}, x, y}|| } \]</span></p>
<p>As we only use argmax on the cosine similarity of the representations to determine the correspondences, the method can be considered unsupervised and zero-shot.</p>
<p>The performance in the semantic correspondence task is usually reported in terms of the percentage of correct keypoints (PCK). PCK measures the percentage of predicted keypoints <span class="math inline">\((\hat{x}_\text{trg}, \hat{y}_\text{trg})\)</span> with a distance to the target keypoints <span class="math inline">\((x_\text{trg}, y_\text{trg}) \in \mathbb{R}^{w \times h}\)</span> below a certain fraction <span class="math inline">\(\alpha\)</span> of the bounding box size <span class="math inline">\((w_\text{bbox}, h_\text{bbox}) \in \{1,...,w\}\times\{1,...,h\}\)</span> of the objects (PCK@<span class="math inline">\(\alpha_\text{bbox}\)</span>), typically with <span class="math inline">\(\alpha=0.1\)</span> <span class="citation" data-cites="zhang2023tale zhang2024telling tang2023emergent luo2023dhf hedlin2023unsupervised li2023sd4match">[<a href="#ref-tang2023emergent" role="doc-biblioref">3</a>, <a href="#ref-zhang2023tale" role="doc-biblioref">4</a>, <a href="#ref-zhang2024telling" role="doc-biblioref">18</a>, <a href="#ref-luo2023dhf" role="doc-biblioref">20</a>, <a href="#ref-hedlin2023unsupervised" role="doc-biblioref">21</a>, <a href="#ref-li2023sd4match" role="doc-biblioref">22</a>]</span>. Instead of the bounding box size, some works also report the PCK for a fraction of the image size (PCK@<span class="math inline">\(\alpha_\text{img}\)</span>) <span class="citation" data-cites="luo2023dhf li2023sd4match stracke2024clean">[<a href="#ref-luo2023dhf" role="doc-biblioref">20</a>, <a href="#ref-li2023sd4match" role="doc-biblioref">22</a>, <a href="#ref-stracke2024clean" role="doc-biblioref">23</a>]</span>.</p>
<p><span class="math display">\[ \text{PCK@}\alpha_\text{bbox} = \frac{1}{n} \sum_{i=1}^n \mathbb{1}\left( \left( (x_\text{trg} - \hat{x}_\text{trg})^2 + (y_\text{trg} - \hat{y}_\text{trg})^2 \right)^\frac{1}{2} \leq \alpha\cdot\text{max}(w_\text{bbox}, h_\text{bbox})\right) \]</span></p>
<p>We report PCK@0.1<span class="math inline">\(_\text{bbox}\)</span> for all our semantic correspondence experiments.</p>
<p>Note that in practice, the source and target images might have different sizes, and that the keypoints and bounding box sizes are given in pixels, which necessitates conversion between the pixel and representation space. This is achieved by linear scaling according to the ratio between the image and representation size.</p>
</section>
<section id="sec-methods-dc" class="level3">
<h3 class="anchored" data-anchor-id="sec-methods-dc">Dense Correspondence</h3>
<p>In dense correspondence, the task is to match not only specific keypoints, but rather larger regions or even the full content between two images. It can be considered a special case of semantic correspondence, where the keypoints are at every token position. Its applications are, for example, object tracking in videos and estimation of optical flow <span class="citation" data-cites="zhang2023tale zhang2025difftracker">[<a href="#ref-zhang2023tale" role="doc-biblioref">4</a>, <a href="#ref-zhang2025difftracker" role="doc-biblioref">94</a>]</span>. We use the dense correspondence task to evaluate failures and biases in the representations, see <a href="#sec-biases" class="quarto-xref">Section&nbsp;5</a>. Due to the dense matching, inconsistencies in the representations can be better visualized.</p>
<p>The correspondences are found using the same approach as for semantic correspondence, but instead of PCK, we use accuracy as performance metric, i.e.&nbsp;the percentage of correct correspondences. This setup is equivalent to semantic correspondence with keypoints at every token position and PCK@0.</p>
</section>
<section id="sec-methods-depth-estimation" class="level3">
<h3 class="anchored" data-anchor-id="sec-methods-depth-estimation">Depth Estimation</h3>
<p>Monocular depth estimation is the task of predicting depth values for each pixel in a single RGB image. This is a challenging task as it requires understanding the 3D structure of a scene from a single 2D view. The task has applications for example in robotics and scene understanding <span class="citation" data-cites="Silberman2012Indoor">[<a href="#ref-Silberman2012Indoor" role="doc-biblioref">89</a>]</span>.</p>
<p>We use the <a href="#sec-datasets-nyu-v2"> NYU Depth v2</a> dataset <span class="citation" data-cites="Silberman2012Indoor">[<a href="#ref-Silberman2012Indoor" role="doc-biblioref">89</a>]</span> to evaluate the depth estimation capabilities of SD representations. Following <span class="citation" data-cites="Chen2023BeyondSS">[<a href="#ref-Chen2023BeyondSS" role="doc-biblioref">32</a>]</span>, we train a simple linear probe on the representations to predict depth values. The linear probe maps from the representation channels to a single depth value for each spatial position. This allows us to assess how well depth information is encoded in different blocks of the model while keeping the probe’s complexity minimal.</p>
<p>We train the linear probe using the Huber loss, following <span class="citation" data-cites="Chen2023BeyondSS">[<a href="#ref-Chen2023BeyondSS" role="doc-biblioref">32</a>]</span>, with threshold <span class="math inline">\(\delta = 1\)</span>, also known as SmoothL1loss <span class="citation" data-cites="pytorch2023huberloss">[<a href="#ref-pytorch2023huberloss" role="doc-biblioref">95</a>]</span>. The Huber loss is defined as</p>
<p><span class="math display">\[ \mathcal{L}_i = \begin{cases} 0.5\cdot(d_i - \hat{d}_i)^2, &amp; \text{if } |d_i - \hat{d}_i| &lt; \delta \\ \delta \cdot (|d_i - \hat{d}_i| - 0.5 \cdot \delta), &amp; \text{otherwise} \end{cases} \]</span></p>
<p>where <span class="math inline">\(d_i\)</span> is the ground truth depth and <span class="math inline">\(\hat{d}_i\)</span> is the predicted depth at pixel <span class="math inline">\(i\)</span>.</p>
<p>To align with common practice in the depth estimation literature, we evaluate the performance using root mean squared error (RMSE) <span class="citation" data-cites="Chen2023BeyondSS Patni2024ECoDepth stracke2024clean">[<a href="#ref-stracke2024clean" role="doc-biblioref">23</a>, <a href="#ref-Chen2023BeyondSS" role="doc-biblioref">32</a>, <a href="#ref-Patni2024ECoDepth" role="doc-biblioref">33</a>]</span>. Lower RMSE values indicate better depth predictions:</p>
<p><span class="math display">\[ \text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n (d_i - \hat{d}_i)^2} \]</span></p>
</section>
</section>
<section id="sec-datasets" class="level2">
<h2 class="anchored" data-anchor-id="sec-datasets">Datasets</h2>
<p>For evaluating the representations and their similarities, we use multiple different datasets, depending on the task:</p>
<ul>
<li>classification: <a href="#sec-datasets-cifar"> CIFAR</a>, <a href="#sec-datasets-imagenet"> Imagenet</a></li>
<li>semantic correspondence: <a href="#sec-datasets-spair"> SPair-71k</a></li>
<li>depth estimation: <a href="#sec-datasets-nyu-v2"> NYU Depth v2</a></li>
<li>anomaly evaluation: <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a>, <a href="#sec-datasets-nyu-v2"> NYU Depth v2</a></li>
</ul>
<p>For some tasks we also use task specific (synthetic) datasets, which are further described in the respective sections.</p>
<section id="sec-datasets-cifar" class="level3 unlisted unnumbered">
<h3 class="unlisted unnumbered anchored" data-anchor-id="sec-datasets-cifar">CIFAR</h3>
<p>We utilize the CIFAR-10 and CIFAR-100 datasets by <span class="citation" data-cites="Krizhevsky2009LearningML">[<a href="#ref-Krizhevsky2009LearningML" role="doc-biblioref">83</a>]</span> for linear probe classification (see <a href="#sec-linear-probe-classification" class="quarto-xref">Section&nbsp;4.1</a>). The datasets contain 60,000 images (50,000 training, 10,000 test) of size <span class="math inline">\(32\times32\)</span> pixels with 10 and 100 classes respectively. We use the versions hosted on Hugging Face<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> <a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>.</p>
</section>
<section id="sec-datasets-imagenet" class="level3 unlisted unnumbered">
<h3 class="unlisted unnumbered anchored" data-anchor-id="sec-datasets-imagenet">Imagenet</h3>
<p>ImageNet by <span class="citation" data-cites="ILSVRC15">[<a href="#ref-ILSVRC15" role="doc-biblioref">84</a>]</span> is a large-scale image recognition dataset. A common variant is the imagenet-1k (ILSVRC) 2012 version hosted on Hugging Face<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>. It consists of 1000 classes and contains 1,281,167 training, 50,000 validation and 100,000 test images, which differ in resolution and aspect ratio.</p>
<p>Due to the large size of the dataset, we run our experiments on a smaller variant, Tiny ImageNet<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>, which contains 200 classes and 100,000 images of size <span class="math inline">\(64\times64\)</span> pixels. Each class has 500 images for training, 50 for validation, and 50 for testing.</p>
<p>For some tasks, we also utilize a custom subset of ImageNet, as described below, with fewer but larger images, compared to Tiny ImageNet.</p>
</section>
<section id="sec-datasets-imagenet-subset" class="level3 unlisted unnumbered">
<h3 class="unlisted unnumbered anchored" data-anchor-id="sec-datasets-imagenet-subset">ImageNet Subset</h3>
<p>For several of our statistical analyses of the SD representations, we use a small subset of ImageNet <span class="citation" data-cites="ILSVRC15">[<a href="#ref-ILSVRC15" role="doc-biblioref">84</a>]</span> (imagenet-1k variant from Hugging Face), containing 500 images of 5 different classes, i.e.&nbsp;100 images per class. The images are center-cropped and resized to <span class="math inline">\(512\times512\)</span> pixels. The size of the dataset is a trade-off between statistical significance and the computational complexity of running state-of-the-art diffusion models.</p>
<p>The original ImageNet ids and class names used in the subset are:</p>
<ul>
<li>235: German shepherd, German shepherd dog, German police dog, alsatian</li>
<li>242: boxer</li>
<li>282: tiger cat</li>
<li>717: pickup, pickup truck</li>
<li>980: volcano</li>
</ul>
</section>
<section id="sec-datasets-spair" class="level3 unlisted unnumbered">
<h3 class="unlisted unnumbered anchored" data-anchor-id="sec-datasets-spair">SPair-71k</h3>
<p>The SPair-71k dataset by <span class="citation" data-cites="min2019spair71k">[<a href="#ref-min2019spair71k" role="doc-biblioref">90</a>]</span> is a large-scale benchmark for evaluating semantic correspondence algorithms. It contains nearly 71,000 image pairs annotated with semantically meaningful correspondences and covers diverse categories, varying object scales and challenging viewpoint variations. The dataset is widely used for advancing research in visual correspondence and matching tasks.</p>
<p>For better usability, we created and published a dataset loading script that can be used with the Hugging Face <code>datasets</code> library<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>.</p>
<p>While working with the SPair-71k dataset, we found that the keypoints usually relate to a specific position on an object, but sometimes they are ambiguous. For example, for a keypoint on a rotational symmetric flowerpot, the semantically correct target position on a second flowerpot object is ambiguous. This effect could cause semantically correct predictions to be rejected. However, as this problem is model independent, we consider the impact to be negligible for our purposes of comparing models and representations.</p>
</section>
<section id="sec-datasets-nyu-v2" class="level3 unlisted unnumbered">
<h3 class="unlisted unnumbered anchored" data-anchor-id="sec-datasets-nyu-v2">NYU Depth v2</h3>
<p>The NYU Depth v2 dataset contains RGB and depth images of indoor scenes and was introduced by <span class="citation" data-cites="Silberman2012Indoor">[<a href="#ref-Silberman2012Indoor" role="doc-biblioref">89</a>]</span>. It contains 1449 densely labeled and curated pairs of RGB and depth images. There is also a larger variant with unlabeled pairs, however, we only use the densely labeled and curated pairs. The images were recorded using Microsoft Kinect cameras in different locations and cities. Similar as for the <a href="#sec-datasets-spair"> SPair-71k</a> dataset, we created and published a dataset loading script that can be used with the Hugging Face <code>datasets</code> library<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>.</p>
</section>
</section>
</section>
<section id="sec-representation-extraction-exploration" class="level1">
<h1>Representation Extraction and Exploration</h1>
<p>A significant part of the work for this thesis was dedicated to the exploration of the extracted representations, and the engineering work of developing and optimizing the representation extraction and exploration tools. Both the exploration and the tools provide the basis and inspiration for the analyses and results we present in <a href="#sec-downstream-tasks" class="quarto-xref">Section&nbsp;4</a> and <a href="#sec-biases" class="quarto-xref">Section&nbsp;5</a>. This chapter describes the <code>sdhelper</code> package, which provides a unified interface to extract representations from different SD models, and the <em>Representation Similarity Explorer</em>, which allows for interactive exploration of the extracted representations. Finally, we present results and insights of the exploration of the representation similarities, which lay the foundation for the following chapters.</p>
<section id="sec-sdhelper" class="level2">
<h2 class="anchored" data-anchor-id="sec-sdhelper"><code>sdhelper</code> Package</h2>
<p>We developed and published the <code>sdhelper</code> Python package to simplify representation extraction and analysis, offering a unified interface for various SD models<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>. In the background, it uses the Hugging Face <code>diffusers</code> library <span class="citation" data-cites="vonplaten2022diffusers">[<a href="#ref-vonplaten2022diffusers" role="doc-biblioref">57</a>]</span> and SD text-to-image pipeline, which provides a flexible interface for interacting with intermediate outputs <span class="citation" data-cites="hfsd">[<a href="#ref-hfsd" role="doc-biblioref">58</a>]</span>. Representations can be extracted either during image generation from noise or for existing images. We use the <code>sdhelper</code> package as a basis for most of our experiments on the representations.</p>
<p>When initializing an <code>SD</code> object with a model name, the <code>sdhelper</code> package loads the corresponding model from Hugging Face or the local cache. The <code>SD</code> object then allows to generate images, extract representations for existing images, and access model properties. The library also simplifies further processing of the representations, including the computation of similarities.</p>
<p>Example usage:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sdhelper <span class="im">import</span> SD</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># load model</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>sd <span class="op">=</span> SD(<span class="st">'SD-1.5'</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># generate image</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> sd(<span class="st">'a beautiful landscape'</span>).result_image</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># extract representations from the `up[1]` block at time step 50</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> sd.img2repr(img, extract_positions<span class="op">=</span>[<span class="st">'up_blocks[1]'</span>], step<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># compute similarity between all pairs of tokens in `r`</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>similarities <span class="op">=</span> r.cosine_similarity(r)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="sec-repr-sim-explorer" class="level2">
<h2 class="anchored" data-anchor-id="sec-repr-sim-explorer">Representation Similarity Explorer</h2>
<div id="cell-fig-repr-sim-explorer" class="cell" data-wrapfigure="R 0.5" data-execution_count="5">
<div class="cell-output cell-output-display">
<div id="fig-repr-sim-explorer" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-repr-sim-explorer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-repr-sim-explorer-output-1.png" width="310" height="566" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-repr-sim-explorer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Screenshot of the Representation Similarity Explorer (<a href="https://sd-similarities.jloos.de">sd-similarities.jloos.de</a>). The selected reference token is highlighted in orange (on the cat’s eye).
</figcaption>
</figure>
</div>
</div>
</div>
<p>To better understand the properties of the extracted representations and the similarities between their tokens, we developed the <em>Representation Similarity Explorer</em>. It is a tool allowing to interactively visualize the similarities between the tokens of different blocks and spatial positions for different images and models. For a screenshot of the tool, see <a href="#fig-repr-sim-explorer" class="quarto-xref">Figure&nbsp;3</a>. We provide a public web version at <a href="https://sd-similarities.jloos.de">sd-similarities.jloos.de</a> that includes precomputed representations for given images. For the upload and exploration of arbitrary images, running the tool locally is required<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>.</p>
<p>To use the tool for representation similarity analysis, open the link in a browser and select one or more images to analyze. Now, hover over the images or similarity maps to show the similarities to the token at the current cursor position. For a good first example of interesting semantic correspondences, select two images that contain a human or animal and hover over the position of an eye. For further exploration change the SD model, the block (position), the similarity measure, and the noise level.</p>
<p>The tool allows for uncomplicated exploration of the representation similarities and properties, which helps us to identify interesting patterns and directions for further analyses. For example, many of the biases described in <a href="#sec-biases" class="quarto-xref">Section&nbsp;5</a> were discovered using it. Available options include:</p>
<ul>
<li>custom image upload (locally)</li>
<li>various SD models</li>
<li>all blocks of the selected SD model</li>
<li>various similarity measures, including cosine similarity and <span class="math inline">\(L_p\)</span> norms</li>
<li>different noise levels (time steps)</li>
</ul>
<p>Architecturally, the representation similarity explorer is split into a Flask-based backend that computes the representations for uploaded images using the <code>sdhelper</code> package (see <a href="#sec-sdhelper" class="quarto-xref">Section&nbsp;3.1</a>) and a simple frontend built with HTML, CSS, and JavaScript. To improve the performance and interactivity, we developed an asynchronous webworker in Rust, that computes the shown similarities in the browser in real time.</p>
<p><span class="citation" data-cites="luo2023dhf">[<a href="#ref-luo2023dhf" role="doc-biblioref">20</a>]</span> provide a similar interactive demo, in the form of a Jupyter notebook, to showcase their semantic correspondence results. Compared to their demo, our tool is significantly more responsive and provides additional options for exploring the representation similarities.</p>
</section>
<section id="sec-similarities" class="level2">
<h2 class="anchored" data-anchor-id="sec-similarities">Similarity Exploration</h2>
<div id="cell-fig-similarities-example" class="cell" data-execution_count="6">
<div class="cell-output cell-output-display">
<div id="fig-similarities-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-similarities-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-similarities-example-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-similarities-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Cosine similarity maps over the spatial dimensions of selected representations of different SD models. Cosine similarity is relative to the token at position (12,17) in image 1, which is at the position of the cat’s eye. The similarities are computed separately for each model.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="cell-fig-similarities-example-blocks" class="cell" data-execution_count="7">
<div class="cell-output cell-output-display">
<div id="fig-similarities-example-blocks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-similarities-example-blocks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-similarities-example-blocks-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-similarities-example-blocks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Cosine similarity maps over the spatial dimensions of the blocks of SD-1.5. Cosine similarity is relative to the token at the position of the cat’s eye in image 1. The similarities are computed separately for each block.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="cell-fig-kmeans-norms-pca" class="cell" data-wrapfigure="R 0.5" data-execution_count="8">
<div class="cell-output cell-output-display">
<div id="fig-kmeans-norms-pca" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-kmeans-norms-pca-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-kmeans-norms-pca-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-kmeans-norms-pca-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Different visualizations of the <code>up[1]</code> representations of SD-1.5: K-means clustering with 6 clusters, L2 norms, and the first 3 principal components (PCs) as color channels.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Using the representation similarity explorer and <code>sdhelper</code> package, we explore the similarities between the representations of various images for the different SD models. The goal is to qualitatively compare the similarity behavior of the representations with our intuitions and expectations to find unexpected behavior and further investigate known properties.</p>
<p><strong>Cosine Similarity Maps.</strong> As can be seen in <a href="#fig-similarities-example" class="quarto-xref">Figure&nbsp;4</a>, the cosine similarity between representation tokens highlights semantic correspondences, in this case especially between the tokens at the positions of the eyes of the animals. Notably, this similarity is not only meaningful within the same image, but also across different images. These semantic similarities can be observed across all tested models, but some differences are visible. The representations used in <a href="#fig-similarities-example" class="quarto-xref">Figure&nbsp;4</a> are the output of the blocks that do not only look the most semantically meaningful, but which also perform best in semantic correspondence, as will be described in <a href="#sec-semantic-correspondence" class="quarto-xref">Section&nbsp;4.2</a>. In <a href="#fig-similarities-example-blocks" class="quarto-xref">Figure&nbsp;5</a>, we visualize the similarities over the different blocks of the SD-1.5 U-Net. The upper U-Net blocks (<code>conv-in</code>, <code>down[0]</code>, <code>up[3]</code>) are relatively noisy, and <code>conv-out</code> does not appear to be useful for representation extraction. Some lower blocks (<code>down[2]</code>, <code>down[3]</code>, <code>mid</code>) have such a low spatial resolution that the image content is not well recognizable in their similarity maps. Their cross-image similarities, however, appear to be meaningful. In the remaining similarity maps, the semantic content and correspondence to the reference token is visible. Thereby, the <code>up[1]</code> representations look particularly relevant, due to their good semantic similarities at a resolution that is high enough to recognize the image content.</p>
<p><strong>Alternative Visualizations.</strong> <a href="#fig-kmeans-norms-pca" class="quarto-xref">Figure&nbsp;6</a> visualizes the spatial correspondences of the representations using k-means clustering, norms, and principal component analysis. Similar to the cosine similarity maps from <a href="#fig-similarities-example" class="quarto-xref">Figure&nbsp;4</a>, the content of the image is reflected in the visualizations. They show that the representations contain spatially accurate and semantically meaningful information, that can be visualized in different ways. For example, both k-means clustering and the principal components separate between foreground and background. For the L2 norm map, the separation is less clear, but some structure is still visible, and the highest norm is at the position of the cat’s face, which might be considered as the most important part of the image. While this observation is very subjective, we find it to be a tendency which is relatively consistent across different images for SD-1.5, SD-2.1, and SD-Turbo. Additional visualizations of the representation norms can be found in <a href="#sec-appendix-repr-norms" class="quarto-xref">Section&nbsp;9.6</a>.</p>
<p>Overall, all tested models show semantically meaningful cosine similarity maps, but the meaningfulness highly depends on the block. Not only cosine similarity shows that the representations contain information about the image content, but so do different visualization methods, such as k-means clustering, L2 norm, and principal components. Following these qualitative observations, we will present more quantitative results in the following chapters.</p>
</section>
</section>
<section id="sec-downstream-tasks" class="level1">
<h1>Performance Evaluation on Downstream Tasks</h1>
<p>To evaluate the quality of the representations for downstream tasks, we use linear probe classification (see <a href="#sec-methods-lpc" class="quarto-xref">Section&nbsp;2.4.1</a>) and semantic correspondence (see <a href="#sec-methods-sc" class="quarto-xref">Section&nbsp;2.4.2</a>) on the datasets described in <a href="#sec-datasets" class="quarto-xref">Section&nbsp;2.5</a>. Liner probe classification is good for assessing the overall quality of the representations due to the spatial averaging. Semantic correspondence supplements this, as it additionally relies on accurate spatial correspondence between the representations and the image.</p>
<section id="sec-linear-probe-classification" class="level2">
<h2 class="anchored" data-anchor-id="sec-linear-probe-classification">Linear Probe Classification</h2>
<div id="tbl-lpc" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-lpc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Linear Probe Classification Results for different SD models and blocks on the <a href="#sec-datasets-cifar"> CIFAR</a>-10 (C10), <a href="#sec-datasets-cifar"> CIFAR</a>-100 (C100), and Tiny-<a href="#sec-datasets-imagenet"> Imagenet</a> (T-IN) datasets. Input images are resized to <span class="math inline">\(512\times512\)</span> pixels and representations are averaged over the spatial dimensions. SDXL and SDXL-Turbo do not have <code>down[3]</code> and <code>up[3]</code> blocks.
</figcaption>
<div aria-describedby="tbl-lpc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="747061e7" class="cell" data-execution_count="11">
<div class="cell-output cell-output-display">
<table class="caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<tbody>
<tr class="odd">
<td></td>
<td colspan="3" style="text-align: center; border-left: 1px solid black;">SD-1.5</td>
<td colspan="3" style="text-align: center; border-left: 1px solid black;">SD-2.1</td>
<td colspan="3" style="text-align: center; border-left: 1px solid black;">SD-Turbo</td>
<td colspan="3" style="text-align: center; border-left: 1px solid black;">SDXL</td>
<td colspan="3" style="text-align: center; border-left: 1px solid black;">SDXL-Turbo</td>
</tr>
<tr class="even">
<td></td>
<td style="text-align: center;">C10</td>
<td style="text-align: center;">C100</td>
<td style="text-align: center;">T-IN</td>
<td style="text-align: center;">C10</td>
<td style="text-align: center;">C100</td>
<td style="text-align: center;">T-IN</td>
<td style="text-align: center;">C10</td>
<td style="text-align: center;">C100</td>
<td style="text-align: center;">T-IN</td>
<td style="text-align: center;">C10</td>
<td style="text-align: center;">C100</td>
<td style="text-align: center;">T-IN</td>
<td style="text-align: center;">C10</td>
<td style="text-align: center;">C100</td>
<td style="text-align: center;">T-IN</td>
</tr>
<tr class="odd">
<td>conv-in</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.17335636017151024); border-left: 1px solid black;">.29</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.04722486898523106);">.08</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.03150309671272034);">.05</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.17210576464983324); border-left: 1px solid black;">.29</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.046808003811338726);">.08</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.02656026679371129);">.04</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.17752501191043354); border-left: 1px solid black;">.29</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.04865412101000476);">.08</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.0295378751786565);">.05</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.172879942829919); border-left: 1px solid black;">.29</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.04543830395426393);">.08</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.028049070986183894);">.05</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.17657217722725105); border-left: 1px solid black;">.29</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.044009051929490235);">.07</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.028465936160076227);">.05</td>
</tr>
<tr class="even">
<td>down[0]</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.34879704621248214); border-left: 1px solid black;">.58</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.18919723677941877);">.31</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.1432229633158647);">.24</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.35135778942353496); border-left: 1px solid black;">.58</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.19515245354930918);">.32</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.14852310624106718);">.25</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.34480705097665554); border-left: 1px solid black;">.57</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.190269175797999);">.32</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.15227489280609813);">.25</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.26911624583134824); border-left: 1px solid black;">.45</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.1151738923296808);">.19</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.0912339209147213);">.15</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.26786565030967124); border-left: 1px solid black;">.44</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.11553120533587422);">.19</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.0886136255359695);">.15</td>
</tr>
<tr class="odd">
<td>down[1]</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.3916150547879943); border-left: 1px solid black;">.65</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.23427822772748927);">.39</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.17448785135778944);">.29</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.3906026679371129); border-left: 1px solid black;">.65</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.2347546450690805);">.39</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.1668651738923297);">.28</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.4017984754645069); border-left: 1px solid black;">.67</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.24309194854692712);">.40</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.1840161981896141);">.31</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.3199737970462125); border-left: 1px solid black;">.53</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.14715340638399238);">.24</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.11630538351595997);">.19</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.32795378751786564); border-left: 1px solid black;">.54</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.15948070509766554);">.26</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.11833015721772272);">.20</td>
</tr>
<tr class="even">
<td>down[2]</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.45605050023820864); border-left: 1px solid black;">.76</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.29603382563125297);">.49</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.2039066222010481);">.34</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.4587898999523582); border-left: 1px solid black;"><strong>.76</strong></td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.2999047165316817);"><strong>.50</strong></td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.19503334921391138);"><strong>.32</strong></td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.4889828489757027); border-left: 1px solid black;"><strong>.81</strong></td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.3318842305859933);">.55</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.2305264411624583);">.38</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.47873987613149116); border-left: 1px solid black;"><strong>.79</strong></td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.32104573606479275);"><strong>.53</strong></td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.257444020962363);"><strong>.43</strong></td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.4925559790376369); border-left: 1px solid black;">.82</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.34224630776560266);"><strong>.57</strong></td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.28549309194854694);"><strong>.47</strong></td>
</tr>
<tr class="odd">
<td>down[3]</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.47201048118151495); border-left: 1px solid black;">.78</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.31121962839447354);">.52</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.2181991424487851);">.36</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.45265602667937116); border-left: 1px solid black;">.75</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.2876369699857075);">.48</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.17960933777989516);">.30</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.4795736064792758); border-left: 1px solid black;">.80</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.3278346831824678);">.54</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.22343973320628868);">.37</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.0); border-left: 1px solid black;">-</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.0);">-</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.0);">-</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.0); border-left: 1px solid black;">-</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.0);">-</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.0);">-</td>
</tr>
<tr class="even">
<td>mid</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.5); border-left: 1px solid black;"><strong>.83</strong></td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.34105526441162454);"><strong>.57</strong></td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.23005002382086706);"><strong>.38</strong></td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.4422343973320629); border-left: 1px solid black;">.73</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.27245116722248686);">.45</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.16823487374940446);">.28</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.47981181515007143); border-left: 1px solid black;">.80</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.3188423058599333);">.53</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.209564078132444);">.35</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.47391615054787994); border-left: 1px solid black;"><strong>.79</strong></td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.31324440209623633);">.52</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.24481896141019532);">.41</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.4975583611243449); border-left: 1px solid black;"><strong>.83</strong></td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.33825631252977606);">.56</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.282336827060505);"><strong>.47</strong></td>
</tr>
<tr class="odd">
<td>up[0]</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.47969271081467363); border-left: 1px solid black;">.80</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.30901619818961407);">.51</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.21266079085278702);">.35</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.4211529299666507); border-left: 1px solid black;">.70</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.24100762267746545);">.40</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.14387803716055264);">.24</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.48243211052882323); border-left: 1px solid black;">.80</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.3338494521200571);">.55</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.22695331110052405);">.38</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.4146021915197713); border-left: 1px solid black;">.69</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.23010957598856596);">.38</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.17079561696045736);">.28</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.46760362077179607); border-left: 1px solid black;">.78</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.28757741781800855);">.48</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.22814435445450212);">.38</td>
</tr>
<tr class="even">
<td>up[1]</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.4680204859456884); border-left: 1px solid black;">.78</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.30532396379228205);">.51</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.19640304907098616);">.33</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.4500357313006193); border-left: 1px solid black;">.75</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.29478323010957597);">.49</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.18645783706526917);">.31</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.4888637446403049); border-left: 1px solid black;"><strong>.81</strong></td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.339566460219152);"><strong>.56</strong></td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.2388637446403049);"><strong>.40</strong></td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.34385421629347307); border-left: 1px solid black;">.57</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.17383277751310147);">.29</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.13446879466412576);">.22</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.38744640304907096); border-left: 1px solid black;">.64</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.2119461648404002);">.35</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.16543592186755596);">.27</td>
</tr>
<tr class="odd">
<td>up[2]</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.4088256312529776); border-left: 1px solid black;">.68</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.24196045736064792);">.40</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.16192234397332061);">.27</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.3995354930919485); border-left: 1px solid black;">.66</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.23433777989518817);">.39</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.15983801810385898);">.27</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.43377798951881846); border-left: 1px solid black;">.72</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.2610171510242973);">.43</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.1851476893758933);">.31</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.2782277274892806); border-left: 1px solid black;">.46</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.12988327775131014);">.22</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.09337779895188184);">.16</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.297879942829919); border-left: 1px solid black;">.49</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.13262267746545972);">.22</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.10451405431157694);">.17</td>
</tr>
<tr class="even">
<td>up[3]</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.3361719866603144); border-left: 1px solid black;">.56</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.17359456884230584);">.29</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.1279180562172463);">.21</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.29686755597903763); border-left: 1px solid black;">.49</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.13035969509290138);">.22</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.09462839447355882);">.16</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.36195807527393997); border-left: 1px solid black;">.60</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.1954502143878037);">.32</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.14715340638399238);">.24</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.0); border-left: 1px solid black;">-</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.0);">-</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.0);">-</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.0); border-left: 1px solid black;">-</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.0);">-</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.0);">-</td>
</tr>
<tr class="odd">
<td>conv-out</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.06348261076703192); border-left: 1px solid black;">.11</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.006550738446879466);">.01</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.003989995235826584);">.01</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.13857789423535016); border-left: 1px solid black;">.23</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.03191996188661267);">.05</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.019711767508337304);">.03</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.11285135778942354); border-left: 1px solid black;">.19</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.011195807527393995);">.02</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.006669842782277275);">.01</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.08051453072891852); border-left: 1px solid black;">.13</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.009766555502620294);">.02</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.004228203906622201);">.01</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.11398284897570271); border-left: 1px solid black;">.19</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.022272510719390185);">.04</td>
<td style="text-align: center; background-color: rgb(0, 0, 255, 0.01095759885659838);">.02</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</figure>
</div>
<p><a href="#tbl-lpc" class="quarto-xref">Table&nbsp;2</a> presents linear probe classification results for <a href="#sec-datasets-cifar"> CIFAR</a>-10, <a href="#sec-datasets-cifar"> CIFAR</a>-100, and Tiny-<a href="#sec-datasets-imagenet"> Imagenet</a>. These results are for time step 50, which we found to perform the best for linear probe classification. They show that the representations contain information useful for classification that is usable by a simple linear probe. However, the results are far from perfect, with a maximum accuracy for SD-1.5 of 83% on <a href="#sec-datasets-cifar"> CIFAR</a>-10, 57% on <a href="#sec-datasets-cifar"> CIFAR</a>-100, and 38% on Tiny-<a href="#sec-datasets-imagenet"> Imagenet</a>. Other SD models achieve similar results. The lower blocks in the U-Net, in the center rows of <a href="#tbl-lpc" class="quarto-xref">Table&nbsp;2</a>, tend to achieve higher results than the upper blocks. Interestingly, which block achieves the highest results differs between the models. In the case of SD-1.5, the <code>mid</code> block achieves the highest results, while for SD-2.1, <code>down[2]</code>, <code>down[3]</code>, and <code>up[1]</code> achieve the best results. In SD-Turbo, all lower blocks from <code>down[2]</code> to <code>up[1]</code> perform similarly good. For SDXL, and SDXL-Turbo, <code>down[2]</code> and <code>mid</code> are the best suited blocks. Notably, there is no clear pattern favoring either down or up blocks, suggesting a more nuanced picture of the semantic information distribution across blocks. Interestingly, the results of the distilled models <span class="citation" data-cites="sauer2023adversarial">[<a href="#ref-sauer2023adversarial" role="doc-biblioref">51</a>]</span>, SD-Turbo and SDXL-Turbo, are slightly better than the results of their non-distilled counterparts, SD-2.1 and SDXL, respectively.</p>
<p>We can conclude from these results that the lower blocks in the U-Net contain more high level information useful for classification and that the different models only differ slightly in their performance.</p>
<p>Compared to supervised methods <span class="citation" data-cites="dosovitskiy2021an">[<a href="#ref-dosovitskiy2021an" role="doc-biblioref">96</a>]</span>, our results are relatively low. Potential reasons for the suboptimal performance could be the spatial averaging of the representations, which merges foreground and background tokens without distinction. For example, if an image contains more background than foreground, the averaged representation will likely be more biased towards the background. Additionally, the possibly non-linear nature of the learned features might also play a role. <span class="citation" data-cites="mukhopadhyay2023diffusion">[<a href="#ref-mukhopadhyay2023diffusion" role="doc-biblioref">30</a>]</span> indeed find that more complex mappings from the representations to the labels, e.g.&nbsp;CNN- or attention-based, can improve the performance significantly.</p>
<p>Another possible problem with the used datasets is the low resolution of the images. They are resized from 32 in the case of <a href="#sec-datasets-cifar"> CIFAR</a> and 64 in the case of Tiny-<a href="#sec-datasets-imagenet"> Imagenet</a> to a resolution of 512 pixels. The resulting blurring is likely out of distribution compared to the training data of the models. In fact, as discussed in <a href="#sec-texture-color-bias" class="quarto-xref">Section&nbsp;5.2</a> and visualized in <a href="#fig-appendix-texture-bias-dense-correspondence-blur" class="quarto-xref">Figure&nbsp;34</a>, blurring the input images can lead to a significant decrease in performance.</p>
</section>
<section id="sec-semantic-correspondence" class="level2">
<h2 class="anchored" data-anchor-id="sec-semantic-correspondence">Semantic Correspondence</h2>
<p>As described in <a href="#sec-methods-sc" class="quarto-xref">Section&nbsp;2.4.2</a>, we evaluate the performance of the representations for unsupervised zero-shot semantic correspondence on the <a href="#sec-datasets-spair"> SPair-71k</a> dataset.</p>
<div id="cell-fig-sc-pck-over-blocks-noise" class="cell" data-execution_count="12">
<div class="cell-output cell-output-display">
<div id="fig-sc-pck-over-blocks-noise" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sc-pck-over-blocks-noise-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-sc-pck-over-blocks-noise-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sc-pck-over-blocks-noise-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Percentage correct keypoints (PCK@0.1<span class="math inline">\(_{\text{bbox}}\)</span>) for semantic correspondence on <a href="#sec-datasets-spair"> SPair-71k</a> for different blocks and time steps.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="cell-fig-sc-pck-over-blocks-resolution" class="cell" data-execution_count="13">
<div class="cell-output cell-output-display">
<div id="fig-sc-pck-over-blocks-resolution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sc-pck-over-blocks-resolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-sc-pck-over-blocks-resolution-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sc-pck-over-blocks-resolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Percentage correct keypoints (PCK@0.1<span class="math inline">\(_{\text{bbox}}\)</span>) for semantic correspondence on <a href="#sec-datasets-spair"> SPair-71k</a> for different blocks and resolutions.
</figcaption>
</figure>
</div>
</div>
</div>
<p>We evaluate the performance of the representations over different blocks and time steps, as shown in <a href="#fig-sc-pck-over-blocks-noise" class="quarto-xref">Figure&nbsp;7</a>. SD-1.5, SD-2.1, and SD-Turbo achieve their best performance with the <code>up[1]</code> block, while SDXL and SDXL-Turbo achieve their best performance with the <code>up[0]</code> block, which are therefore the blocks we will focus on. Due to the differences in model architecture, these blocks have actually the same depth in the U-Net and the same spatial resolution for images of the same size. The optimal time step (noise level) differs between models, but values between 25 and 100 achieve near optimal performance for all models. This observation motivates our choice of time step 50 as default for our experiments. Higher input image resolutions can lead to higher PCK values, as shown in <a href="#fig-sc-pck-over-blocks-resolution" class="quarto-xref">Figure&nbsp;8</a>. For SDXL, higher resolutions visibly increase performance at least up to 1024 (in pixels) for the best block (<code>up[0]</code>). Among all other tested models, the optimal resolution is 768, however, for SD1.5, SD-Turbo and SDXL-Turbo, the difference to 512 is only around 2 PCK. A resolution of 1024 even leads to a decrease in performance compared to 768. As computational requirements increase quadratically with the resolution, we mostly use 512 for our experiments.</p>
<p>The different models achieve PCK@0.1<span class="math inline">\(_\text{bbox}\)</span> values of up to 48.89 for SD-1.5, 52.38 for SD-2.1, 54.80 for SD-Turbo, 39.80 for SDXL, and 40.95 for SDXL-Turbo, at our default time step 50 and using an image resolution of 768. These values are similar to the results of <span class="citation" data-cites="tang2023emergent">[<a href="#ref-tang2023emergent" role="doc-biblioref">3</a>]</span>, who use a similar method and report 52.8 PCK@0.1<span class="math inline">\(_\text{bbox}\)</span> for SD-2.1.</p>
<div id="cell-fig-spair-71k-baseline" class="cell" data-wrapfigure="R 0.4" data-execution_count="15">
<div class="cell-output cell-output-display">
<div id="fig-spair-71k-baseline" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-spair-71k-baseline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-spair-71k-baseline-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-spair-71k-baseline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Baseline percentage of correct keypoints (PCK) for semantic correspondence on the <a href="#sec-datasets-spair"> SPair-71k</a> dataset using random and same-position estimators, i.e.&nbsp;where the target keypoint is predicted to be random or equal to the source keypoint, respectively.
</figcaption>
</figure>
</div>
</div>
</div>
<p>To be able to estimate if representations are semantically meaningful at all, we compute the PCK for a random and a same-position estimator, see <a href="#fig-spair-71k-baseline" class="quarto-xref">Figure&nbsp;9</a>. If representations yield semantic correspondence results at a similar level as these trivial baselines, they are not semantically meaningful in this context. The random estimator randomly guesses a predicted keypoint with uniform distribution over the input image size. It correctly predicts 2.02% of the keypoints correctly at <span class="math inline">\(\alpha_\text{bbox}=0.1\)</span>, which can be considered the lowest baseline above which a semantic correspondence algorithm should perform. The same-position estimator always assumes that the target keypoint is the same as the source keypoint and correctly predicts 5.49% of the keypoints at <span class="math inline">\(\alpha_\text{bbox}=0.1\)</span>. This means that estimators should reach a PCK@<span class="math inline">\(0.1_\text{bbox}\)</span> of more than 5.49% to be considered potentially meaningful for semantic correspondence on the <a href="#sec-datasets-spair"> SPair-71k</a> dataset. This value is what we find most blocks to converge to for very high noise levels (high time steps), see <a href="#fig-sc-pck-over-blocks-noise" class="quarto-xref">Figure&nbsp;7</a>. Especially some upper blocks (<code>conv-in</code>, <code>down[0]</code>, <code>up[3]</code>, and <code>conv-out</code>) are so close to these baselines, that they can be considered unsuitable for semantic correspondence.</p>
<p>Interestingly, we find that the maximum semantic correspondence performance of the different models does not directly correspond to the image generation quality of the models. For example, images generated by SDXL are usually of higher quality than images generated by SD-1.5 or SD-2.1 <span class="citation" data-cites="podell2023sdxl">[<a href="#ref-podell2023sdxl" role="doc-biblioref">12</a>]</span>, but the semantic correspondence performance of SDXL is significantly lower. Similarly, the performances of the distilled models, SD-Turbo and SDXL-Turbo, are higher than that of their non-Turbo counterparts, SD-2.1 and SDXL, respectively, even though their image quality is lower in general <span class="citation" data-cites="sauer2023adversarial">[<a href="#ref-sauer2023adversarial" role="doc-biblioref">51</a>]</span>. This aligns with the equivalent findings for linear probe classification in <a href="#sec-linear-probe-classification" class="quarto-xref">Section&nbsp;4.1</a>. These observations seem to contradict the intuition that models with higher image quality might also have better representation quality. This might be due to architectural differences between the models, or due to improvements of non-semantic properties, such as texture and style. We leave a more detailed analysis of the relationship between image quality, model architecture, and representation quality for future work.</p>
<p>Overall, our results show that SD representations can be usable for semantic correspondence, which aligns with the results of related work <span class="citation" data-cites="tang2023emergent zhang2023tale">[<a href="#ref-tang2023emergent" role="doc-biblioref">3</a>, <a href="#ref-zhang2023tale" role="doc-biblioref">4</a>]</span>. The performance depends on the model, block, resolution, and time step. Using suboptimal settings can easily degrade the performance to the level of a trivial baseline (see <a href="#fig-sc-pck-over-blocks-noise" class="quarto-xref">Figure&nbsp;7</a> and <a href="#fig-spair-71k-baseline" class="quarto-xref">Figure&nbsp;9</a>).</p>
</section>
<section id="sec-sc-improvements" class="level2">
<h2 class="anchored" data-anchor-id="sec-sc-improvements">Improvements for Semantic Correspondence</h2>
<p>To investigate what augmentations and improvements can be applied to the representations or workflows using them, we perform a series of experiments aiming to improve the semantic correspondence performance on the <a href="#sec-datasets-spair"> SPair-71k</a> dataset. However, we find that most of the tested approaches result in only small improvements of 1-2 PCK@0.1<span class="math inline">\(_\text{bbox}\)</span>, no significant change, or even a decrease in performance. As this is not the main focus of this thesis, we do not run extensive hyperparameter searches or ablation studies for all tested approaches, but rather report trends and observations based on a few selected experiments.</p>
<ul>
<li><strong>Class prompt</strong>: Using the prompt “a photo of a {class}” during representation extraction might help the model to extract more suitable representations <span class="citation" data-cites="xu2023open li2023sd4match tang2023emergent zhang2023tale zhang2025three">[<a href="#ref-tang2023emergent" role="doc-biblioref">3</a>, <a href="#ref-zhang2023tale" role="doc-biblioref">4</a>, <a href="#ref-li2023sd4match" role="doc-biblioref">22</a>, <a href="#ref-zhang2025three" role="doc-biblioref">35</a>, <a href="#ref-xu2023open" role="doc-biblioref">97</a>]</span>. We find improvements of 1-2 PCK, compared to an empty prompt.</li>
<li><strong>Concatenation of representations</strong>: As described in <a href="#sec-representation-similarities" class="quarto-xref">Section&nbsp;2.3</a>, representations of different blocks can be concatenated along the channel dimension to combine information from different blocks <span class="citation" data-cites="zhang2023tale luo2023dhf ji2024diffusion baranchuk2022labelefficient">[<a href="#ref-zhang2023tale" role="doc-biblioref">4</a>, <a href="#ref-luo2023dhf" role="doc-biblioref">20</a>, <a href="#ref-baranchuk2022labelefficient" role="doc-biblioref">36</a>, <a href="#ref-ji2024diffusion" role="doc-biblioref">37</a>]</span>. Only specific combinations improve performance, for example, for SD-1.5, concatenating <code>up[0]</code> and <code>up[1]</code> seems to perform best and is about 1 PCK better than <code>up[1]</code> alone.</li>
<li><strong>Subtracting the representations of an empty image</strong>: As further explored in <a href="#sec-position-bias" class="quarto-xref">Section&nbsp;5.1</a>, we observe a position bias in the representations. To counteract this bias, we subtract the representations of an empty image, which thus cannot encode much semantic information, from the representations. This could remove the positional information, if it is encoded additively. However, it seems to disrupt the integrity of the representations and leads to a decrease in performance.</li>
<li><strong>Image shifting for sub-token-size accuracy</strong>: The observation of lower performance of <code>down[2]</code> to <code>up[0]</code> compared to <code>up[1]</code> in the semantic correspondence task, while the respective performance in the linear probe classification task is better, could be due to the low spatial resolution of these blocks. We shift the input images by fractions of the token size and extract representations for all shifted image variants to gain sub-token-size spatial accuracy. However, we observe artifacts introduced by the combination of different representations, and do not see any significant change in performance.</li>
<li><strong>Representation averaging</strong>: As the random noise introduced during the representation extraction might lead to degradation of representations, we hypothesize that averaging the representations over multiple representations of the same image with different noise seeds might improve representation quality. We observe improvements of around 1 PCK for when averaging over 10 representations, which is similar to the observations reported by <span class="citation" data-cites="tang2023emergent">[<a href="#ref-tang2023emergent" role="doc-biblioref">3</a>]</span>.</li>
<li><strong>Cosine similarity over multiple representations</strong>: Similar to <em>representation averaging</em>, but instead of averaging multiple representations, we compute the cosine similarity between multiple representations of the same image with different noise seeds and use the maximum similarity to determine the predicted keypoint. We observe similar improvements of around 1 PCK.</li>
<li><strong>Cosine similarity over multiple image scales</strong>: Similar to <em>cosine similarity over multiple representations</em>, but we compute the cosine similarity between representations of the same image at different scales. This is motivated by the observation that the objects in different images of <a href="#sec-datasets-spair"> SPair-71k</a> are often at different scales. The results vary heavily depending on the image scale used and can significantly degrade performance when including e.g.&nbsp;very small scales. In some cases the performance improves slightly, but a more extensive analysis would be recommended.</li>
<li><strong>Cosine similarity over image flips</strong>: We extract the representations of the same image of both the original and horizontally flipped version, compute the cosine similarities separately, and then average the cosine similarity maps before computing the argmax. We observe improvements of around 2 PCK.</li>
<li><strong>Image padding</strong>: As further explored in <a href="#sec-anomalies-corner" class="quarto-xref">Section&nbsp;5.3.1</a>, the corners and borders of the representations are sometimes less semantically meaningful. We also find that the semantic correspondence performance is around 10 PCK lower for keypoints near the borders of the image. We hypothesize that padding the images with zeros might improve the performance, as then no keypoints are near the borders anymore. However, we find mostly similar performance, but very slight improvements of less than 1 PCK in some cases.</li>
<li><strong>PCA dimensionality reduction</strong>: To increase quality of the representations, we perform principal component analysis (PCA) dimensionality reduction along the channel dimension, computed over all tokens and images. We try different numbers of components, but observe at most very small improvements of less than 1 PCK. We refer to <span class="citation" data-cites="zhang2023tale">[<a href="#ref-zhang2023tale" role="doc-biblioref">4</a>]</span> for more information.</li>
</ul>
<p>Overall, we find that some of the evaluated approaches lead to small improvements of a few PCK maximum on the semantic correspondence task, while others do not increase, or even decrease, performance. Several approaches that improve performance, also reduce the impact of noise, mostly though averaging multiple representations or their similarities. This suggests that when using multiple approaches at once, the improvements may not be additive. For many, a disadvantage are the additional computational requirements, mostly due to additional representations that need to be computed. Among these results, the most promising approaches are the usage of a <em>class prompt</em>, <em>concatenation of representations</em>, <em>representation averaging</em>, and <em>cosine similarity over image flips</em>. <em>Cosine similarity over multiple image scales</em> and <em>PCA dimensionality reduction</em> also show potential, but require further investigation.</p>
<p>For more complex workflows with good performance, we refer to <span class="citation" data-cites="zhang2023tale">[<a href="#ref-zhang2023tale" role="doc-biblioref">4</a>]</span>, with the improvements introduced by <span class="citation" data-cites="zhang2024telling">[<a href="#ref-zhang2024telling" role="doc-biblioref">18</a>]</span> and <span class="citation" data-cites="stracke2024clean">[<a href="#ref-stracke2024clean" role="doc-biblioref">23</a>]</span>. To the best of our knowledge, this combination sets the current state-of-the-art performance of 69.99 PCK@0.1<span class="math inline">\(_\text{bbox}\)</span> for unsupervised zero-shot semantic correspondence on <a href="#sec-datasets-spair"> SPair-71k</a>, as of December 2024.</p>
<!-- Could do: maybe add results when applying all these improvements -->
</section>
</section>
<section id="sec-biases" class="level1">
<h1>Biases in the Representations</h1>
<p>During our experiments and exploration of the representation similarities, we observe several unexpected patterns, or biases, in the representations. These biases in the representations include an encoding of position, different levels of abstraction over the different blocks, and groups of tokens with unusually high norm and non-semantic similarities. These biases and their implications are explored and discussed in the following sections.</p>
<section id="sec-position-bias" class="level2">
<h2 class="anchored" data-anchor-id="sec-position-bias">Position Bias</h2>
<div id="cell-fig-position-similarities-empty-image" class="cell" data-wrapfigure="R 0.5" data-execution_count="16">
<div class="cell-output cell-output-display">
<div id="fig-position-similarities-empty-image" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-position-similarities-empty-image-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-position-similarities-empty-image-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-position-similarities-empty-image-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Observed positional embeddings in SD-1.5 <code>mid</code> representations, which have a spatial shape of <span class="math inline">\(8\times8\)</span>. Each tile shows the cosine similarity between the corresponding reference location (x, y) and the rest of the representation, averaged over 100 empty images to reduce noise impact.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Position bias refers to the tendency of representation tokens to encode information about their spatial position in the image. It is one of the first properties we observed in the representations, where closeby tokens share an increased cosine similarity, even when no clear semantic connection can be observed. <a href="#fig-position-similarities-empty-image" class="quarto-xref">Figure&nbsp;10</a> visualizes this bias in the <code>mid</code> representations of empty images (RGB values of 0). It is clearly visible that each position has increased similarity to nearby positions. An exception are border and corner positions, where the similarity does not expand into the center region of the image. This behavior is further explored in <a href="#sec-anomalies-corner" class="quarto-xref">Section&nbsp;5.3.1</a>. Notably, position bias also extends across images, with representations at the same positions in different images showing higher cosine similarity between each other.</p>
<section id="linear-position-estimation" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="linear-position-estimation">Linear Position Estimation</h3>
<div id="cell-fig-position-classifier-accuracy" class="cell" data-execution_count="17">
<div class="cell-output cell-output-display">
<div id="fig-position-classifier-accuracy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-position-classifier-accuracy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-position-classifier-accuracy-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-position-classifier-accuracy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: Position estimation test accuracies on the output of the different blocks and layers of SD-1.5 for different resolutions on which the accuracy is evaluated. Two linear probe position estimators are trained on the representations of the <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a>. The classification estimator has outputs for each row and column of the representation, while the regression estimator has only the x and y coordinates as outputs.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The observation of similarities based on position means that spatial position is saliently encoded in the representation tokens. To quantify this observation, we explore how well this positional embedding can be linearly extracted. <a href="#fig-position-classifier-accuracy" class="quarto-xref">Figure&nbsp;11</a> shows the test accuracy of two different linear probe position estimators that take one representation token as input and predict the position of the token in the image. We train the linear models on 80% of the <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a> using the Adam optimizer <span class="citation" data-cites="kingma2017adam">[<a href="#ref-kingma2017adam" role="doc-biblioref">93</a>]</span> and evaluate them on the remaining 20%. The classification estimator treats the rows and columns of the representation as each as a separate set of classes from which it tries to choose the correct row and column. The regression estimator has two continuous outputs for the x and y coordinates of the token, which are then discretized to the nearest integer, i.e.&nbsp;position. It is visible that the classification estimator performs significantly better than the regression estimator, and reaches an accuracy of over 90% on the output of some layers. Both models have better performance in the representations of the lower blocks of the U-Net, i.e.&nbsp;<code>down[2]</code> to <code>up[1]</code>. There, the problem is also easier due to the smaller spatial resolution of the representations. But even when evaluating the upper blocks using lower resolutions, which is visualized by the shaded lines, the lower blocks still perform better. Therefore, we conclude both from this experiment and the qualitative observations, that the lower layers in the U-Net seem to have a more pronounced positional embedding.</p>
<p>For a qualitative visualization of the position estimation results of the models used for <a href="#fig-position-classifier-accuracy" class="quarto-xref">Figure&nbsp;11</a>, see <a href="#sec-appendix-position-bias" class="quarto-xref">Section&nbsp;9.3</a> (<a href="#fig-appendix-position-classifier-example" class="quarto-xref">Figure&nbsp;27</a>).</p>
</section>
<section id="different-image-sizes" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="different-image-sizes">Different Image Sizes</h3>
<div id="cell-fig-pos-embedding-aspect-ratios" class="cell" data-execution_count="18">
<div class="cell-output cell-output-display">
<div id="fig-pos-embedding-aspect-ratios" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pos-embedding-aspect-ratios-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-pos-embedding-aspect-ratios-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pos-embedding-aspect-ratios-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: Behavior of position embedding over different aspect ratios and image sizes for SD-1.5. The first row shows a color gradient for the default image size of 512×512, which is then transferred according to the highest cosine similarity between the representations of the respective block to the 3 different target resolutions.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Furthermore, we investigate the behavior of the position bias over different aspect ratios and image sizes in <a href="#fig-pos-embedding-aspect-ratios" class="quarto-xref">Figure&nbsp;12</a>. This is visualized on a colorwheel, which is an image containing a color gradient. It is used to show which regions of an image are mapped where. The shown mapping is created by searching the token in the source representation with the highest cosine similarity for each token in the target representation and transferring the pixels of the colorwheel accordingly. The ideal mapping would preserve the relative positions in the colorwheel as much as possible. As visible in the figure, for the higher blocks in the U-Net, especially <code>down[0]</code>, <code>up[2]</code>, and <code>up[3]</code>, the mapping is mostly random, i.e.&nbsp;the positional correspondences are not preserved. The lower blocks better preserve the positional information. This observation aligns well with the previous observations using the position estimation task in <a href="#fig-position-classifier-accuracy" class="quarto-xref">Figure&nbsp;11</a>. Interestingly, the mapping does not seem to just be linearly scaled depending on the resolution, but rather depends on the relative distances to the nearest border. Additionally, in all mappings, the borders tend to better preserve the color gradient, i.e.&nbsp;seem to have more reliable positional information.</p>
</section>
<section id="dense-correspondence" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="dense-correspondence">Dense Correspondence</h3>
<div id="cell-fig-dense-correspondence-flip" class="cell" data-execution_count="19">
<div class="cell-output cell-output-display">
<div id="fig-dense-correspondence-flip" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dense-correspondence-flip-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-dense-correspondence-flip-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dense-correspondence-flip-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: Dense Correspondence when flipping images. The representations are extracted for an image in both original and flipped/mirrored form and then each token of the flipped representation is matched to the token with the highest cosine similarity in the original representation. The emerging mapping is visualized on the original image pixels (column 1, 4), on a colorwheel (column 2, 5), and with a map of the error between the predicted mapping and the target mapping of flipping horizontally (column 3, 6). For more info and a figure containing all blocks, see <a href="#sec-appendix-position-bias" class="quarto-xref">Section&nbsp;9.3</a> (<a href="#fig-appendix-dense-correspondence-flip-all-blocks" class="quarto-xref">Figure&nbsp;28</a>).
</figcaption>
</figure>
</div>
</div>
</div>
<p>One downstream task, where the positional embedding is visible, is dense correspondence, where each point in one image should be mapped to another image. A simple variant of this task is matching images with their flipped version, as visualized in <a href="#fig-dense-correspondence-flip" class="quarto-xref">Figure&nbsp;13</a>, where we show the results for two example images. Especially in the <code>up[0]</code> row, we see regions of high errors, where the colorwheel mapping is exactly the same as the original, instead of being similar to the flipped version. This indicates that the mapping is dominated by the position embedding. Interestingly, this primarily occurs in parts of the image where the semantic content of the image in the error regions does not differ much between the original and flipped images. This is especially visible in the left example for <code>up[0]</code> in <a href="#fig-dense-correspondence-flip" class="quarto-xref">Figure&nbsp;13</a>, where the error mainly occurs in the region of the grass, which goes over the full width of the image and is thus semantically invariant to flipping. In the right example, most of the background is semantically invariant to flipping, and thus the regions with high error is much larger.</p>
<p>This effect also appears at the spatial locations showing a cat in the left example at all shown blocks. However, it is less visible in the error map due to the small distance between the left and right side of the cat. Here, the “erroneous” mapping does not only make sense from a positional embedding perspective, but also from a semantic one. The shown mapping preserves the cat’s orientation, i.e.&nbsp;the left side of the cat is mapped to the left side of the new flipped cat. This effect highlights the limitations of the dense correspondence task over flipped images. In this task, but also in other cases, what is semantically meaningful and desired can differ depending on the task and user.</p>
<p>In several places, there are single tokens with high error at the borders or corners, i.e.&nbsp;corner or border anomalies, an effect that we further investigate in <a href="#sec-anomalies-corner" class="quarto-xref">Section&nbsp;5.3.1</a>.</p>
</section>
<section id="semantic-correspondence" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="semantic-correspondence">Semantic Correspondence</h3>
<div class="cell" data-layout="[[-15,70,-15],[48, -4, 48]]" data-execution_count="20">
<div id="fig-sc-errors-by-relative-position" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sc-errors-by-relative-position-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 15.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-sc-errors-by-relative-position" style="flex-basis: 70.0%;justify-content: flex-start;">
<div id="fig-sc-errors-by-relative-position-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-sc-errors-by-relative-position-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-sc-errors-by-relative-position-output-1.svg" class="img-fluid figure-img" data-ref-parent="fig-sc-errors-by-relative-position">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-sc-errors-by-relative-position-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Sample of the synthetic dataset. The target similarities are relative to the source keypoint and used for predicting the target keypoint.
</figcaption>
</figure>
</div>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 15.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
</div>
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-sc-errors-by-relative-position" style="flex-basis: 48.0%;justify-content: flex-start;">
<div id="fig-sc-errors-by-relative-position-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-sc-errors-by-relative-position-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-sc-errors-by-relative-position-output-2.svg" class="img-fluid figure-img" data-ref-parent="fig-sc-errors-by-relative-position">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-sc-errors-by-relative-position-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Error rate over the relative position to the source keypoint for SD-1.5. The center (0, 0) is the location of the source keypoint.
</figcaption>
</figure>
</div>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 4.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-sc-errors-by-relative-position" style="flex-basis: 48.0%;justify-content: flex-start;">
<div id="fig-sc-errors-by-relative-position-3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-sc-errors-by-relative-position-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-sc-errors-by-relative-position-output-3.svg" class="img-fluid figure-img" data-ref-parent="fig-sc-errors-by-relative-position">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-sc-errors-by-relative-position-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) Error rate over the distance relative to the source keypoint for different models.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sc-errors-by-relative-position-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: Mean semantic correspondence error rate over the predicted position/distance relative to the source keypoint. A prediction is considered an error, if the distance between predicted and target keypoint is larger than 16 pixels, i.e.&nbsp;larger than the size of one representation token. We use the <code>up[1]</code> representations, except for the SDXL and SDXL-Turbo, where <code>up[0]</code> is used. We exclude very high distances from the plots due to low sample size. Extended plots and more context can be found in <a href="#sec-appendix-position-bias" class="quarto-xref">Section&nbsp;9.3</a> (<a href="#fig-appendix-sc-errors-by-relative-position-full" class="quarto-xref">Figure&nbsp;29</a>).
</figcaption>
</figure>
</div>
</div>
<p>We also investigate the impact of the positional embedding on the semantic correspondence task. For this, we create a synthetic dataset, where different foreground objects, specifically animal images with annotated keypoints, e.g.&nbsp;eyes or paws, are placed at different locations on a white background image. For an example, see <a href="#fig-sc-errors-by-relative-position-1" class="quarto-xref">Figure&nbsp;14 (a)</a>. The dataset consists of 11,623,140 semantic correspondence tasks on 2048 unique images (512 px), which include horizontally flipped versions of all images, to reduce position biases from the dataset side. Using cosine similarity on the representations of all synthetic images, we predict the target keypoint locations and compare them to the annotated locations. From our previous experiments, we have the hypothesis that the positional embedding might sometimes overrule the semantic content in the representations and therefore negatively influence the semantic correspondence task. <a href="#fig-sc-errors-by-relative-position" class="quarto-xref">Figure&nbsp;14</a> supports this hypothesis, because the error rate is visibly increased, when the predicted keypoint is close to the position of the source keypoint. This effect occurs for all models we tested, indicating that the positional embedding might be an inherent property of the representations of U-Net based diffusion models in general. More details for this experiment can be found in <a href="#sec-appendix-position-bias" class="quarto-xref">Section&nbsp;9.3</a> (<a href="#fig-appendix-sc-errors-by-relative-position-full" class="quarto-xref">Figure&nbsp;29</a>).</p>
<p>Furthermore, we investigate the impact of this error case when using non-synthetic data, i.e.&nbsp;the <a href="#sec-datasets-spair"> SPair-71k</a> dataset. We find significantly increased error rates for very short distances between the predicted and target keypoint, similar to <a href="#fig-sc-errors-by-relative-position-3" class="quarto-xref">Figure&nbsp;14 (c)</a>, for details see <a href="#fig-appendix-sc-errors-by-relative-position-spair-lines" class="quarto-xref">Figure&nbsp;31</a>. While this occurs for most blocks and time steps, the effect is not as clearly visible as in the synthetic data. Particularly, while the error rate map in <a href="#fig-sc-errors-by-relative-position-2" class="quarto-xref">Figure&nbsp;14 (b)</a> shows a clear increase near (0,0), this is mostly not really visible in the error rate maps for <a href="#sec-datasets-spair"> SPair-71k</a>, for details see <a href="#fig-appendix-sc-errors-by-relative-position-spair-maps" class="quarto-xref">Figure&nbsp;30</a>. We see two factors that might explain this: First, the spatial averaging used to create the error rate maps can obscure localized effects. Second, and more importantly, there appears to be a counteracting effect in real-world images - the error rates are actually lower in a broader region around the source keypoint. This lower error rate in the vicinity likely stems from the non-uniform distribution of keypoints and semantic content in natural images like those in <a href="#sec-datasets-spair"> SPair-71k</a>. There, related features tend to appear in similar regions, for example near the image center, rather than being randomly distributed as in our synthetic dataset.</p>
<p>This phenomenon hampers accurate quantification of the impact of the increased error rates due to the positional embedding for real world data. Furthermore, the behavior heavily depends on the block and on the dataset. For example, for semantic correspondence on <a href="#sec-datasets-spair"> SPair-71k</a>, using the <code>up[1]</code> representations of SD-1.5, we estimate an additional increase in total errors of about 0.027%, while we get an increase of 5.480% when using the <code>up[0]</code> representations.</p>
<!-- percentage of increased errors are calculated with `sum((error_rate[i]-error_rate[lowest_error_idx])*total_counts[i] for i in range(lowest_error_idx)) / total_counts.sum()` -->
<!-- I.e. it is the increase at low distances (before minimum error rate) in @fig-appendix-sc-errors-by-relative-position-spair-lines for time step 50 -->
<!-- see `semantic_correspondence/analyze_sc_hyper_results_step_over_blocks.ipynb` -->
</section>
<section id="concluding-remarks" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="concluding-remarks">Concluding Remarks</h3>
<p>The existence of position bias is to some degree surprising, as SD does not use spatial positional embeddings as part of the architectural design <span class="citation" data-cites="rombach2022highresolution CompVis2022StableDiffusionRepo">[<a href="#ref-rombach2022highresolution" role="doc-biblioref">1</a>, <a href="#ref-CompVis2022StableDiffusionRepo" role="doc-biblioref">98</a>]</span>. The observation that the positional information is relative to the nearest border (see <a href="#fig-pos-embedding-aspect-ratios" class="quarto-xref">Figure&nbsp;12</a>) gives us a clue about its origin. It is an indication that the position information may arise in the convolutional layers, which use 0-padding, and therefore can easily detect the image boundaries.</p>
<p>In conclusion, our experiments establish the existence of positional bias in the representations of SD models. The positional information is linearly extractable and primarily present in the lower blocks of the U-Net (see <a href="#fig-position-classifier-accuracy" class="quarto-xref">Figure&nbsp;11</a>). It affects both dense correspondence (see <a href="#fig-dense-correspondence-flip" class="quarto-xref">Figure&nbsp;13</a>) and semantic correspondence tasks (see <a href="#fig-sc-errors-by-relative-position" class="quarto-xref">Figure&nbsp;14</a>), however, the impact is hard to quantify for real world data and is limited to specific blocks. Therefore, whether this positional bias is relevant or negligible depends on the specific setup, block, and dataset.</p>
</section>
</section>
<section id="sec-texture-color-bias" class="level2">
<h2 class="anchored" data-anchor-id="sec-texture-color-bias">Texture and Color Bias</h2>
<div id="cell-fig-texture-color-bias-examples" class="cell" data-execution_count="21">
<div class="cell-output cell-output-display">
<div id="fig-texture-color-bias-examples" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-texture-color-bias-examples-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-texture-color-bias-examples-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-texture-color-bias-examples-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: Examples of texture and color bias in the dense correspondence task in selected representations (for all see <a href="#fig-appendix-texture-color-bias-examples-1" class="quarto-xref">Figure&nbsp;32</a>). For each representation token from the respective block for the target image, the most similar representation token from the source is selected using cosine similarity. This mapping is visualized by transferring the image pixels of the source image (columns 1, 3, 5) and pixels of the colorwheel (columns 2, 4, 6) from their source location to the location defined by the mapping (just as in <a href="#fig-dense-correspondence-flip" class="quarto-xref">Figure&nbsp;13</a>).
</figcaption>
</figure>
</div>
</div>
</div>
<p>Texture and color bias refers to the tendency of prioritizing texture and color over semantic information. As described before, the extracted representations of the SD U-Net carry such semantic information, however, this does not apply to all blocks in the same way. The upper blocks (<code>conv-in</code>, <code>down[0]</code>, <code>down[1]</code>, <code>up[2]</code>, <code>up[3]</code>) are biased towards low-level surface properties in the image, such as texture and color, while the lower blocks (<code>down[2]</code>, <code>down[3]</code>, <code>mid</code>, <code>up[0]</code>, <code>up[1]</code>) focus more on the abstract or semantic meaning <span class="citation" data-cites="zhang2023tale tang2023emergent">[<a href="#ref-tang2023emergent" role="doc-biblioref">3</a>, <a href="#ref-zhang2023tale" role="doc-biblioref">4</a>]</span>. The transition between the abstraction levels is gradual, i.e.&nbsp;there is no sharp separation between upper and lower blocks in terms of texture and color bias.</p>
<section id="dense-correspondence-1" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="dense-correspondence-1">Dense Correspondence</h3>
<p><a href="#fig-texture-color-bias-examples" class="quarto-xref">Figure&nbsp;15</a> shows some examples of this effect using the dense correspondence task. It visualizes the mapping defined by matching each representation token in the target image to the representation token with the highest cosine similarity in the source image. The pixels from the source image and colorwheel are then transferred accordingly to the target image. Among the blocks visualized in <a href="#fig-texture-color-bias-examples" class="quarto-xref">Figure&nbsp;15</a>, the <code>conv-in</code> block shows the strongest texture or color bias, as can be seen by the similar looks of the mapped image to the target image, while the colorwheel shows that the mapping does not well correspond to any semantic meaning. An example for this are the wall being mapped to represent the light regions of the cat (example 1), and the sky regions being mapped to represent the snow (example 2). For <code>up[0]</code>, in contrast, the mapping corresponds more to the semantic meaning. For example, the head regions of the source image are used to represent the head regions of the target image (all examples), and similar for e.g.&nbsp;feet and ground, even though color and texture might be different. Going further along the up blocks, the mapping of <code>up[1]</code> introduces some texture and color bias again, while mostly preserving the semantics. The mapping of <code>up[2]</code> is even more texture and color biased, but still more semantically meaningful as e.g.&nbsp;the <code>conv-in</code> mapping.</p>
<div class="cell" data-execution_count="22">
<div id="fig-color-bias-rgb-bgr-dc" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="22">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-color-bias-rgb-bgr-dc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div id="fig-color-bias-rgb-bgr-dc-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-color-bias-rgb-bgr-dc-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-color-bias-rgb-bgr-dc-output-1.svg" class="img-fluid figure-img" data-ref-parent="fig-color-bias-rgb-bgr-dc">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-color-bias-rgb-bgr-dc-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Example of the gradual color channel permutation
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div id="fig-color-bias-rgb-bgr-dc-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-color-bias-rgb-bgr-dc-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-color-bias-rgb-bgr-dc-output-2.svg" class="img-fluid figure-img" data-ref-parent="fig-color-bias-rgb-bgr-dc">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-color-bias-rgb-bgr-dc-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Accuracy, and change relative to the initial accuracy, for dense correspondence
</figcaption>
</figure>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-color-bias-rgb-bgr-dc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: Dense correspondence results on the <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a>, see (b), when permuting the RGB channels of the image from RGB to BGR, as visualized in (a). The ideal mapping (100% accuracy) would be if for each token in the target image, the token with the highest cosine similarity in the source image is at the same spatial location.
</figcaption>
</figure>
</div>
</div>
<p>We hypothesize that the texture and color bias might negatively impact downstream tasks like semantic and dense correspondence, as the model might incorrectly prioritize low-level perceptual similarities over semantic ones.</p>
<p>To quantify the color bias, we investigate the impact of degradations in color space by evaluating the dense correspondence accuracy of the task of matching the tokens between an image and the same image with permuted color channels, see <a href="#fig-color-bias-rgb-bgr-dc" class="quarto-xref">Figure&nbsp;16</a>. We measure the accuracy <span class="math inline">\(\text{acc}_i\)</span> for 10 interpolation steps <span class="math inline">\(i\in[0,1]\)</span>, and the relative change in accuracy <span class="math inline">\(\Delta\text{acc}_{i,\text{rel}}=\frac{\text{acc}_i-\text{acc}_0}{\text{acc}_0}\)</span>, when permuting the RGB channels of the image from RGB to BGR. The results are averaged over the 500 images of the <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a>. When looking at the relative change in accuracy, we find, as expected, that the performance of the upper blocks drops with increased color degradation, especially <code>conv-in</code>, and <code>up[3]</code>, but also <code>down[0]</code>, and <code>up[2]</code>. At the same time, the lower blocks do not show any significant change in accuracy. This shows that the upper blocks are easily confused by color changes. At interpolation step 0, one might expect perfect accuracy for all blocks, as the source and target images are exactly the same. However, as we use different noise seeds for source and target image, the dense correspondence accuracy might not be perfect. This setup is more relevant, as, in general, one can not expect the noise pattern to be related between the matching regions in real world dense correspondence tasks.</p>
<div class="cell" data-execution_count="23">
<div id="fig-texture-bias-texture-overlay-dc" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="23">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-texture-bias-texture-overlay-dc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div id="fig-texture-bias-texture-overlay-dc-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-texture-bias-texture-overlay-dc-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-texture-bias-texture-overlay-dc-output-1.svg" class="img-fluid figure-img" data-ref-parent="fig-texture-bias-texture-overlay-dc">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-texture-bias-texture-overlay-dc-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Example of the gradual addition of the texture overlay
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div id="fig-texture-bias-texture-overlay-dc-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-texture-bias-texture-overlay-dc-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-texture-bias-texture-overlay-dc-output-2.svg" class="img-fluid figure-img" data-ref-parent="fig-texture-bias-texture-overlay-dc">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-texture-bias-texture-overlay-dc-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Accuracy, and change relative to the initial accuracy, for dense correspondence
</figcaption>
</figure>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-texture-bias-texture-overlay-dc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17: Dense correspondence results on the <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a>, see (b), when overlaying a texture on the image, e.g.&nbsp;grass, as visualized in (a). The ideal mapping (100% accuracy) would be if for each token in the target image, the token with the highest cosine similarity in the source image is at the same spatial location.
</figcaption>
</figure>
</div>
</div>
<p>In <a href="#fig-texture-bias-texture-overlay-dc" class="quarto-xref">Figure&nbsp;17</a>, we evaluate the same setup as in <a href="#fig-color-bias-rgb-bgr-dc" class="quarto-xref">Figure&nbsp;16</a>, but instead of permuting the color channels, we overlay a texture on the image. This aims to change the texture perceived by the model, thus allowing to evaluate texture bias. We find that in this case, all blocks are somewhat impacted, but that the upper blocks are again more affected. We also evaluate this for other texture overlays, blurring, and noise, and find similar results for all these cases. More details can be found in <a href="#sec-appendix-texture-color-bias" class="quarto-xref">Section&nbsp;9.4</a> (<a href="#fig-appendix-texture-bias-dense-correspondence-blur" class="quarto-xref">Figure&nbsp;34</a> and <a href="#fig-appendix-texture-bias-dense-correspondence-noise" class="quarto-xref">Figure&nbsp;35</a>).</p>
<p>However, while these results show the impact of color and texture on the upper blocks, they alone do not show whether the lower blocks are actually more resistant to such changes. The reason is that in this dense correspondence task, the relatively good performance of the lower blocks could also be due to increased reliance on positional embedding, which we showed to be more pronounced in these blocks, see <a href="#sec-position-bias" class="quarto-xref">Section&nbsp;5.1</a>.</p>
</section>
<section id="semantic-correspondence-1" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="semantic-correspondence-1">Semantic Correspondence</h3>
<div id="cell-fig-texture-color-bias-sc" class="cell" data-execution_count="24">
<div class="cell-output cell-output-display">
<div id="fig-texture-color-bias-sc" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-texture-color-bias-sc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-texture-color-bias-sc-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-texture-color-bias-sc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18: Semantic correspondence percentage of correct keypoints (PCK) results on the <a href="#sec-datasets-spair"> SPair-71k</a> dataset, when permuting the RGB channels of the image (upper plots) and when overlaying a texture on the image (lower plots). The permutation of the RGB channels from RGB to BGR is the same as in <a href="#fig-color-bias-rgb-bgr-dc" class="quarto-xref">Figure&nbsp;16</a>, and the overlay of a grass texture is the same as in <a href="#fig-texture-bias-texture-overlay-dc" class="quarto-xref">Figure&nbsp;17</a>.
</figcaption>
</figure>
</div>
</div>
</div>
<p>To further investigate the texture and color bias and answer the question about the reason for the better performance of the lower blocks, we evaluate the semantic correspondence performance on <a href="#sec-datasets-spair"> SPair-71k</a> while applying the same color and texture changes as above. The results are shown in <a href="#fig-texture-color-bias-sc" class="quarto-xref">Figure&nbsp;18</a>. As discussed in <a href="#sec-semantic-correspondence" class="quarto-xref">Section&nbsp;4.2</a>, the semantic correspondence performance of many of the blocks is not much better than the trivial baseline for <a href="#sec-datasets-spair"> SPair-71k</a> (which is at around 5.5 PCK, see <a href="#sec-datasets" class="quarto-xref">Section&nbsp;2.5</a>). This means interpretation of these results is difficult for all blocks, except for <code>up[0]</code>, <code>up[1]</code>, and <code>up[2]</code>, where the initial performance is relatively high. For the permutation of the RGB channels, all blocks are only slightly impacted. While this does not mean much for the bad performing blocks, it shows that the three better performing blocks are quite robust against color channel permutation. The same goes for the texture overlay results, where the performance of the three blocks significantly decreases, but still remains significantly above the trivial baseline. With this, we can rule out the position bias as the reason for the previous good performance of the lower blocks in the dense correspondence task, because the positional embedding is not enough for good performance in the semantic correspondence task. Additionally, we observe that among the three better performing blocks, the relative decrease in performance increases with being higher in the U-Net. This aligns well the general observation of color and texture bias increasing towards the upper blocks.</p>
</section>
<section id="concluding-remarks-1" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="concluding-remarks-1">Concluding Remarks</h3>
<p>Neural style transfer might seem like another good path forward for investigating texture bias. However, we find that popular style transfer methods <span class="citation" data-cites="fofr2024styletransfer">[<a href="#ref-fofr2024styletransfer" role="doc-biblioref">99</a>]</span> also sometimes impact the semantic meaning of image regions or objects. Therefore, it is unsuitable for the semantic correspondence task, especially on the <a href="#sec-datasets-spair"> SPair-71k</a> dataset, where keypoints often depend on fine details. However, using other style transfer methods or with extensive manual hyperparameter tuning, neural style transfer might offer an interesting challenge for further investigating texture and color bias of SD representations.</p>
<p>In conclusion, we qualitatively find texture and color bias in the upper blocks of the SD U-Net and quantitatively show that it impacts the dense correspondence task. The evaluation on the semantic correspondence task is restricted by the unsuitability of many representations for semantic correspondence on the <a href="#sec-datasets-spair"> SPair-71k</a> dataset, but we show that lower blocks tend to be more resistant to texture and especially color changes.</p>
</section>
</section>
<section id="sec-anomalies" class="level2">
<h2 class="anchored" data-anchor-id="sec-anomalies">Anomalies</h2>
<p>When investigating the similarities of representations in the representation similarity explorer (see <a href="#sec-repr-sim-explorer" class="quarto-xref">Section&nbsp;3.2</a>), we observe different unexpected patterns. Some of these patterns are localized to only one or a few tokens and only appear in the representations of certain blocks or at certain spatial positions. They do not occur in all images, and their spatial position differs depending on the image. These anomalies can be separated into different categories:</p>
<ul>
<li><strong>corner and border anomalies</strong>: border and corner tokens with non-semantic similarities</li>
<li><strong>high-norm anomalies</strong>: localized groups of tokens with high norm and non-semantic similarities</li>
</ul>
<section id="sec-anomalies-corner" class="level3">
<h3 class="anchored" data-anchor-id="sec-anomalies-corner">Corner and Border Anomalies</h3>
<div id="cell-fig-cosine-similarity-corner" class="cell" data-wrapfigure="R 0.5" data-execution_count="25">
<div class="cell-output cell-output-display">
<div id="fig-cosine-similarity-corner" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cosine-similarity-corner-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-cosine-similarity-corner-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cosine-similarity-corner-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19: Cosine similarities between <code>mid</code> block representations of three images and the upper left corner token in the left example.
</figcaption>
</figure>
</div>
</div>
</div>
<p>One anomaly that we observe in the similarity maps is that the borders, and especially the corners, in most of the representations of the different blocks behave differently and show similarities that do not correspond to the semantic content of the image. <a href="#fig-cosine-similarity-corner" class="quarto-xref">Figure&nbsp;19</a> shows an example of such increased cosine similarities between corners and borders. Here, many of the corners and borders have an increased similarity towards the upper left corner of the first representation. These similarities between corners and borders sometimes correspond to semantic similarities in the images, but often do not.</p>
<div id="fig-border-corner-similarity" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-border-corner-similarity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div id="cell-fig-border-corner-similarity-over-blocks" class="cell quarto-layout-cell-subref quarto-layout-cell" data-execution_count="26" data-ref-parent="fig-border-corner-similarity" style="flex-basis: 57.1%;justify-content: flex-start;">
<div class="cell-output cell-output-display quarto-layout-cell-subref" data-ref-parent="fig-border-corner-similarity">
<div id="fig-border-corner-similarity-over-blocks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-border-corner-similarity-over-blocks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-border-corner-similarity-over-blocks-output-1.svg" class="img-fluid figure-img" data-ref-parent="fig-border-corner-similarity">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-border-corner-similarity-over-blocks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Average relative cosine similarities in the token groups containing the corners, the borders, or the other tokens. A value above 1 indicates increased similarities among a token group, when the corners/borders are already in the corners/borders during representation extraction.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 4.8%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-border-corner-similarity" style="flex-basis: 38.1%;justify-content: flex-start;">
<div id="fig-corner-similarities-method" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-corner-similarities-method-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="assets/figures/corner_similarities_method.jpg" class="img-fluid figure-img" data-ref-parent="fig-border-corner-similarity">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-corner-similarities-method-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Schematic visualization of the experiment.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-border-corner-similarity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;20: Visualization of increased corner similarities. (a) shows the similarities using cropped images (see (b), left path) relative to those where the representations are cropped instead (see (b), right path).
</figcaption>
</figure>
</div>
<p>A naive way of quantifying these observations would be to compare the mean similarity among the groups of tokens in the corners or border, and compare them to the mean similarity among the other tokens. This, however, does not take into account the non-independent nature of representation tokens. Natural images tend to have certain characteristics that influence the similarity values, such as foreground objects often being located in the center of an image. To remedy this, we compare the similarities of the representations at the same position in the image, but once where the image is cropped so that the token is the corner/border during representation extraction and once where it is not. The amount of pixels cropped is based on the corresponding size of one token, which differs depending on the block used for representation extraction. The similarities are calculated per image and averaged over the 500 samples in the <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a>. The results are shown in <a href="#fig-border-corner-similarity" class="quarto-xref">Figure&nbsp;20</a> show that borders have an relatively average cosine similarity between each other. The corners, however, show a significant increase in similarity between each other for the lower blocks (<code>down[2]</code>, <code>down[3]</code>, <code>mid</code>, <code>up[0]</code>). Interestingly, for <code>conv-in</code>, <code>up[2]</code>, and <code>up[3]</code> the similarity among the corners in decreased instead. In the representation similarity explorer (see <a href="#sec-repr-sim-explorer" class="quarto-xref">Section&nbsp;3.2</a>), we observe that the corner similarities are more spread out in the <code>up[1]</code>, <code>up[2]</code>, and <code>up[3]</code> representations, which might be an explanation for why the similarity is not increased there.</p>
<p>We also check the similarity of corners and borders for SD-2.1, SD-Turbo, SDXL, and SDXL-Turbo (see <a href="#sec-appendix-corner-anomalies" class="quarto-xref">Section&nbsp;9.5</a>). For SD-2.1 and SD-Turbo, the corner similarities drop sharply in the <code>up[0]</code> representations. For SDXL and SDXL-Turbo, the corner similarity increases only after the <code>down[1]</code> block. But despite these slight differences, the overall pattern is consistent across all tested models. This indicates that corner anomalies might be inherent to the used U-Net architecture, and not tied to a specific layer count, model size, training data, and training procedure.</p>
<p>Two hypotheses for the cause of these corner anomalies are that they are caused by the padding in the convolutions of the ResNet-layers, or that the models might consider them as less important regions, where non-semantic information can be “stored”, similar to the findings of <span class="citation" data-cites="darcet2024vision">[<a href="#ref-darcet2024vision" role="doc-biblioref">70</a>]</span>, where such behavior is observed in vision transformers. The impact of border anomalies on downstream tasks is further discussed in <a href="#sec-anomalies-impact" class="quarto-xref">Section&nbsp;5.3.3</a>.</p>
</section>
<section id="sec-anomalies-high-norm" class="level3">
<h3 class="anchored" data-anchor-id="sec-anomalies-high-norm">High-norm Anomalies</h3>
<!-- Could do: add paragraph headings -->
<p>While investigating the norms of the representation tokens, we notice two kinds of anomalies in the representations of different blocks. The two distinct types of anomalies are localized groups of neighboring tokens with highly increased norms. One type occurs in the <code>conv-in</code>, <code>down[0]</code>, and <code>up[3]</code> blocks, while the other type occurs in the <code>up[1]</code> and <code>up[2]</code> blocks.</p>
<div id="cell-fig-high-norm-anomalies-1" class="cell" data-wrapfigure="R 0.6" data-execution_count="28">
<div class="cell-output cell-output-display">
<div id="fig-high-norm-anomalies-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-high-norm-anomalies-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-high-norm-anomalies-1-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-high-norm-anomalies-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21: Example of the L2 norms of representation tokens for different time steps <span class="math inline">\(t\)</span> of SD-1.5 and SD-2.1 where a high-norm anomaly occurs in the <code>conv-in</code>, <code>down[0]</code>, and <code>up[3]</code> blocks.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The first kind of anomalies with a high norm occurs primarily in the representations of the <code>conv-in</code>, <code>down[0]</code>, and <code>up[3]</code> blocks of SD-1.5. For SD-2.1, we additionally observe them in <code>down[1]</code> and <code>conv-out</code>. As they are most pronounced in the <code>conv-in</code> representations, we call them <code>conv-in</code> anomalies. They consist of a group of roughly 1-9 neighboring tokens with considerably higher L2 norm than the rest of the tokens. The spatial position of these anomalies is robust over the different blocks and over a wide range of noise levels. Interestingly, these anomalies sometimes even occur at exactly the same spatial position in SD-1.5 and SD-2.1 for the same image. This is particularly surprising, as SD-1.5 and SD-2.1, while they share the same U-Net architecture, do not share any pretraining history. In <a href="#fig-high-norm-anomalies-1" class="quarto-xref">Figure&nbsp;21</a>, one such anomaly is visualized for different noise levels, blocks, and models.</p>
<div id="cell-fig-high-norm-anomalies-2" class="cell" data-execution_count="29">
<div class="cell-output cell-output-display">
<div id="fig-high-norm-anomalies-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-high-norm-anomalies-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-high-norm-anomalies-2-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-high-norm-anomalies-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;22: Example of the L2 norms of representation tokens for different time steps <span class="math inline">\(t\)</span> of SD-1.5 and SD-2.1 where high-norm anomalies occur in the <code>up[1]</code> and <code>up[2]</code> blocks.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The second kind of high-norm anomalies occur in the <code>up[1]</code> and <code>up[2]</code> representations of SD-1.5 and SD-2.1. Their occurence and spatial position does not seem to be connected to the <code>conv-in</code> anomalies and is also not consistent over different models. They tend to disappear for very low noise levels and are not always stable over different noise levels and noise seeds. See <a href="#fig-high-norm-anomalies-2" class="quarto-xref">Figure&nbsp;22</a> for an example over different noise levels, blocks, and models. These <code>up[1]</code> anomalies typically occur in the shape of 2 patches in the <code>up[1]</code> representations and only in every second row and column. These two properties are likely related to the spatial upscaling of the representations at the end of the up-blocks.</p>
<div id="cell-fig-high-norm-anomalies-searching" class="cell" data-wrapfigure="R 0.5" data-execution_count="30">
<div class="cell-output cell-output-display">
<div id="fig-high-norm-anomalies-searching" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-high-norm-anomalies-searching-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-high-norm-anomalies-searching-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-high-norm-anomalies-searching-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23: True positives, false positives, and false negatives over different thresholds when searching for high-norm anomalies in the <a href="#sec-datasets-nyu-v2"> NYU Depth v2</a> dataset using the cosine similarity to the mean <code>up[1]</code> anomaly token from the <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a>.
</figcaption>
</figure>
</div>
</div>
</div>
<p>We labelled the anomaly patches in the norm plots for <code>up[1]</code> in the <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a> using a custom build annotation tool and find that about 25% of all images have at least one such high-norm anomaly. Anomaly tokens have a very high cosine similarity of ~0.80 between each other, compared to an average cosine similarity of about 0.055 between all representation tokens in <code>up[1]</code>. Especially high is the cosine similarity between tokens at same position in different anomaly patches with a value of ~0.92 (e.g.&nbsp;between the upper left tokens in the 2 patches). This high similarity suggests that the anomalies could be systematically found using cosine similarity. We test this hypothesis by computing the cosine similarity between the mean <code>up[1]</code> anomaly token of the manually labelled anomalies in the <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a> and all <code>up[1]</code> representation tokens in the <a href="#sec-datasets-nyu-v2"> NYU Depth v2</a> dataset. All tokens with a cosine similarity higher than a threshold (e.g.&nbsp;0.8) are considered to be anomalies. As can be seen in <a href="#fig-high-norm-anomalies-searching" class="quarto-xref">Figure&nbsp;23</a>, for the right threshold, this simple search method achieves a high true positive rate and a low false negative rate, while the false positive rate is moderate. Notably, the false positives here do not necessarily indicate a failure of the method, but could also be due to missing labels caused by the manual labelling process. Lower thresholds lead to more false positives, while higher thresholds lead to more false negatives.</p>
<!-- Note: the 0.055 were taken over 20 images from the imagenet subset -->
<div id="cell-fig-high-norm-anomalies-over-layers" class="cell" data-execution_count="31">
<div class="cell-output cell-output-display">
<div id="fig-high-norm-anomalies-over-layers" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-high-norm-anomalies-over-layers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-high-norm-anomalies-over-layers-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-high-norm-anomalies-over-layers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;24: Mean L2 norms of high-norm anomaly tokens relative to the mean of all token norms per representation over the different layers of SD-1.5. We manually labeled these anomalies in the representation norm plots of the <code>conv-in</code> and <code>up[1]</code> blocks for the <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a>. The shaded areas show the standard deviation.
</figcaption>
</figure>
</div>
</div>
</div>
<p>As described above, both types of the high-norm anomalies occur in the same regions of nearby blocks and layers. To investigate this observation, we manually label the anomalies in the <code>conv-in</code> and <code>up[1]</code> representations of SD-1.5 and compare the norms at their positions to the mean representation norm in the respective block or layer. As can be seen in <a href="#fig-high-norm-anomalies-over-layers" class="quarto-xref">Figure&nbsp;24</a>, <code>conv-in</code> anomalies tend to have the highest relative norm in the <code>conv-in</code> representations, and continue to have a high but decreasing relative norm in the <code>down[0]</code> layers and the first layers of the <code>down[1]</code> block. In the lower blocks of the U-Net, i.e.&nbsp;<code>down[2]</code> to <code>up[2]</code>, the tokens at these positions tend to have average norms, but in the layers of the <code>up[3]</code> block, the relative norm is again increased. This pattern is plausible due to the skip connections in the U-Net architecture between the down- and up-blocks. The <code>up[1]</code> anomalies begin to have a high relative norm in the layers of the <code>up[1]</code> block, which holds until the end of the <code>up[2]</code> block. These findings confirm and quantify the observed presence of two distinct types of high-norm anomalies that each persist over multiple layers.</p>
<p>While the described high-norm anomalies occur regularly and are clearly distinguishable in the SD-1.5 and SD-2.1 representations, we do not find similar anomalies when inspecting SDXL and SDXL-Turbo.</p>
<p>The observed <code>up[1]</code> high-norm anomalies seem similar to the artifacts found by <span class="citation" data-cites="darcet2024vision">[<a href="#ref-darcet2024vision" role="doc-biblioref">70</a>]</span> in the attention maps of vision transformers such as DINOv2 <span class="citation" data-cites="oquab2024dinov">[<a href="#ref-oquab2024dinov" role="doc-biblioref">15</a>]</span>. An additional indication, that these anomalies might be of the same nature is that they seem to materialize in the output of the attention layers of the <code>up[1]</code> block, as can be seen in <a href="#fig-high-norm-anomalies-over-layers" class="quarto-xref">Figure&nbsp;24</a>. <span class="citation" data-cites="darcet2024vision">[<a href="#ref-darcet2024vision" role="doc-biblioref">70</a>]</span> propose to train vision transformers with additional register tokens to remedy the artifacts they found, which might also be possible for the attention layers of the SD U-Net. Due to the high computational cost of training near state-of-the-art diffusion models, we leave this approach for future work. Another potential approach that might be able to reduce these anomalies is <em>CleanDIFT</em> proposed by <span class="citation" data-cites="stracke2024clean">[<a href="#ref-stracke2024clean" role="doc-biblioref">23</a>]</span>, who suggest finetuning SD models to reduce noise in the representations and to allow extraction of high quality representations at time step 0. The observations visualized in <a href="#fig-high-norm-anomalies-2" class="quarto-xref">Figure&nbsp;22</a> suggest that this approach might remove the <code>up[1]</code> high-norm anomalies, as they are not present at time step 0.</p>
<p>Further information and findings on the statistics of the representation norms can be found in <a href="#sec-appendix-repr-norms" class="quarto-xref">Section&nbsp;9.6</a>.</p>
</section>
<section id="sec-anomalies-impact" class="level3">
<h3 class="anchored" data-anchor-id="sec-anomalies-impact">Impact of Anomalies</h3>
<p>Both border and high-norm anomalies introduce non-semantic similarities between tokens, which might impact the performance of downstream tasks. However, border anomalies are by definition at the borders of the image, which usually is part of the background. We also found qualitatively that the <code>up[1]</code> high-norm anomalies tend to be in background areas. The question is, whether these anomalies still have a measurable impact on the performance of downstream tasks. To investigate this, we evaluate the performance on a depth estimation task, which is a dense prediction task, where each token in the representation is relevant to the prediction. We presume that by using such a dense prediction task the impact of the anomalies can be observed more clearly, compared to a task there only selected tokens are relevant to the prediction, such as semantic correspondence on <a href="#sec-datasets-spair"> SPair-71k</a>. As the <code>conv-in</code> anomalies are only present in the upper blocks that are usually not used for representation extraction, we leave their evaluation for future work, and only focus on the corner/border and <code>up[1]</code> anomalies.</p>
<div id="cell-fig-depth-estimation-performance-over-layers" class="cell" data-execution_count="32">
<div class="cell-output cell-output-display">
<div id="fig-depth-estimation-performance-over-layers" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-depth-estimation-performance-over-layers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-depth-estimation-performance-over-layers-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-depth-estimation-performance-over-layers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;25: Root mean squared error (rmse) of linear probes on the representations of the test set of the <a href="#sec-datasets-nyu-v2"> NYU Depth v2</a> dataset (lower is better). The border, corner, and anomaly results use the same linear probes, but are only evaluated on the tokens that are at the spatial position of a border, corner, or <code>up[1]</code> anomaly, respectively.
</figcaption>
</figure>
</div>
</div>
</div>
<p>We evaluate the performance of the SD-1.5 representations for depth estimation on the <a href="#sec-datasets-nyu-v2"> NYU Depth v2</a> dataset <span class="citation" data-cites="Silberman2012Indoor">[<a href="#ref-Silberman2012Indoor" role="doc-biblioref">89</a>]</span> using a simple linear probe, as described in <a href="#sec-methods-depth-estimation" class="quarto-xref">Section&nbsp;2.4.4</a>. This linear probe was trained on 1159 and tested on 290 pairs of RGB and depth images, where the RGB images are the input and the depth images are the target. The results over the different blocks and layers of SD-1.5 are shown in <a href="#fig-depth-estimation-performance-over-layers" class="quarto-xref">Figure&nbsp;25</a>. The performance follows an overall similar trend as what was found by <span class="citation" data-cites="Chen2023BeyondSS">[<a href="#ref-Chen2023BeyondSS" role="doc-biblioref">32</a>]</span>, where the depth estimation performance tends to improve for the lower layers, peaks at the beginning of the <code>up</code>-blocks and then decreases again.</p>
<p>For the border tokens, the depth estimation results are mostly similar to the average performance of all tokens, except for <code>conv-in</code> the first <code>down</code>-blocks, where the performance is better. The performance for corner tokens follows a similar trend, except for heavily varying performance in the layers of the <code>down[0]</code> block and decreased performance in and around the <code>up[1]</code> block, where the performance over all tokens actually peaks. We find <code>up[1]</code> to be also the most useful block for representation extraction for other tasks, such as semantic correspondence (see <a href="#sec-semantic-correspondence" class="quarto-xref">Section&nbsp;4.2</a>). This decrease in performance at the <code>up[1]</code> block suggests that downstream tasks might have decreased performance in corner regions. Fortunately, image corners often contain less critical content for downstream tasks<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a>.</p>
<p>Interestingly, the performance of tokens at the position of <code>up[1]</code> high-norm anomalies is better across the board. This result is unexpected, as previous observations showed less semantic behavior for these anomaly tokens. However, the somewhat uniform improve in performance over all layers, while the high-norm anomalies only impact certain layers (see <a href="#fig-high-norm-anomalies-over-layers" class="quarto-xref">Figure&nbsp;24</a>), suggests that the performance increase is unrelated to the high-norm anomalies. Potentially, the anomalies tend to be located in areas where depth estimation is easier.</p>
<p>Overall, these results suggest that <code>up[1]</code> high-norm anomalies likely do not significantly impact depth estimation negatively. As the depth estimation was performed using a linear probe, which might be able to ignore the dimensions that cause the high norms, this result might not transfer to tasks without a linear probe. Investigating the impact of high-norm anomalies on other types of downstream tasks is therefore a potential direction for further research.</p>
</section>
</section>
</section>
<section id="sec-discussion" class="level1">
<h1>Discussion</h1>
<p>In this thesis, we set out to investigate representation similarity in latent diffusion models, to evaluate their performance on downstream tasks, and to identify relevant properties of the representations. We confirm that SD representations are useful for downstream tasks, and identify a position bias, a texture and color bias, and high-norm anomalies. To the best of our knowledge, our findings for the position bias and the high-norm anomalies are novel and not reported in the literature. In the following, we summarize and discuss our results, point out limitations, and suggest directions for future work.</p>
<section id="sec-discussion-results" class="level2">
<h2 class="anchored" data-anchor-id="sec-discussion-results">Summary of Results</h2>
<p><strong>Position Bias.</strong> We find a position bias (see <a href="#sec-position-bias" class="quarto-xref">Section&nbsp;5.1</a>) in the lower U-Net blocks, which is linearly extractable. The origin of the position bias in the SD representations is unclear, as SD does not use spatial positional embeddings as part of the architecture <span class="citation" data-cites="rombach2022highresolution CompVis2022StableDiffusionRepo">[<a href="#ref-rombach2022highresolution" role="doc-biblioref">1</a>, <a href="#ref-CompVis2022StableDiffusionRepo" role="doc-biblioref">98</a>]</span>. However, we observe that the position information seems to be relative to the nearest border. This suggests that it might arise in the convolutional layers, which use 0-padding and therefore can detect the image boundaries. While some position-related issues of SD representations are described and addressed in the literature <span class="citation" data-cites="zhang2024telling">[<a href="#ref-zhang2024telling" role="doc-biblioref">18</a>]</span>, to the best of our knowledge, we are the first to describe and analyze this bias more broadly. Even though the position bias may be beneficial in tasks where the absolute position of objects in the image is relevant, it can lead to errors in other tasks by outweighing semantic similarity. We show that it has negative impact on the semantic correspondence performance on the <a href="#sec-datasets-spair"> SPair-71k</a> dataset, which however heavily differs depending on the block. For <code>up[1]</code>, which is the most relevant block for semantic correspondence, the performance degradation is negligible, but for other tasks, where other blocks are preferred, the impact of the position bias might be significant.</p>
<p><strong>Texture and Color Bias.</strong> While the lower blocks encode more abstract meaning, the upper blocks encode more low-level features, such as texture and color (see <a href="#sec-texture-color-bias" class="quarto-xref">Section&nbsp;5.2</a>). We specifically investigate the biases towards changes in texture and color of the different blocks and find that, especially for color, there is a strong difference between the blocks. While the lower blocks seem very robust against color changes, the upper blocks, especially <code>conv-in</code> and <code>up[3]</code> (in the case of SD-1.5), are easily disturbed. The results of our texture-related experiments tend to paint a similar picture, but are less conclusive. We primarily use a simple texture overlay, while more sophisticated texture degradation methods could likely yield more conclusive results. Overall, our results confirm the observations of previous works that different blocks of the U-Net encode different levels of abstraction <span class="citation" data-cites="zhang2023tale tang2023emergent">[<a href="#ref-tang2023emergent" role="doc-biblioref">3</a>, <a href="#ref-zhang2023tale" role="doc-biblioref">4</a>]</span>.</p>
<p><strong>Anomalies.</strong> We find both corner and high-norm anomalies in the representation norms and cosine similarity maps (see <a href="#sec-anomalies" class="quarto-xref">Section&nbsp;5.3</a>). The corner tokens in the representations tend to have an increased similarity between each other, which often is not related to any semantic similarity. We find that the performance of corner tokens for depth estimation is decreased, at least for the <code>up[1]</code> block. However, their impact on downstream tasks is likely limited, as corners are often less critical to solving a task. The origin of the corner anomalies is unclear, but could be related to usage of padded convolutions in the U-Net architecture, and/or to the position bias. Additionally to the anomalous behavior of corner tokens, we find two types of anomalies with high norms in different blocks. Both types mostly consist of multiple neighboring tokens with an anomalously high norm, and both have a high cosine similarity among each other, similar to the corner anomalies. The first type is found primarily in the <code>conv-in</code> block, but also appears in the layers of the <code>down[0]</code> and <code>up[3]</code> blocks, and is robust over different time steps and surprisingly sometimes even over different models. Its relevance for downstream tasks is limited, as these blocks are usually not used. Similar to the corner anomalies, the origin of this anomaly type is unclear. One hypothesis is that they may be connected to some unintuitive behavior of the SD VAE. The second type of high-norm anomalies is found primarily in the <code>up[1]</code> block, but also appears in the layers of the <code>up[2]</code> block. The L2 norm of these anomalies tends to increase in the attention layers of the <code>up[1]</code> block, which is an indication that they might be related to the artifact tokens found by <span class="citation" data-cites="darcet2024vision">[<a href="#ref-darcet2024vision" role="doc-biblioref">70</a>]</span> in the representations of vision transformers. Interestingly, the cosine similarity between these anomalies is so high, that it can be used to identify new <code>up[1]</code> anomalies with a high accuracy. When evaluating their impact on depth estimation on the <a href="#sec-datasets-nyu-v2"> NYU Depth v2</a> dataset, we do not find any degradation in performance. While we chose depth estimation due to its dense prediction nature that might make the impact of anomalies more visible, the usage of a linear probe for this task might nullify any negative impact of these anomalies. While this suggests that tasks using trained networks on top of the representations may not be significantly impacted by these anomalies, their effect on tasks that directly use the representations, such as dense correspondence through similarity matching, remains a possibility.</p>
<p><strong>Performance Evaluation.</strong> While we focussed on the properties and biases of the representations, we also evaluated their general performance for linear probe classification and in more detail for semantic correspondence, see <a href="#sec-downstream-tasks" class="quarto-xref">Section&nbsp;4</a>. For linear probe classification, we find results of 76 - 83% maximum accuracy for the different models on <a href="#sec-datasets-cifar"> CIFAR</a>-10, 50 - 57% on <a href="#sec-datasets-cifar"> CIFAR</a>-100, and 32 - 47% on Tiny-<a href="#sec-datasets-imagenet"> Imagenet</a>. We find the best performance at or near the <code>mid</code> block, depending on the model. For semantic correspondence on the <a href="#sec-datasets-spair"> SPair-71k</a> dataset, we find maximum PCK@0.1<span class="math inline">\(_\text{bbox}\)</span> values of 39.85 - 54.80 for the different models. The best blocks here are the <code>up[1]</code> block for SD-1.5, SD-2.1, and SD-Turbo, and <code>up[0]</code> for SDXL and SDXL-Turbo. Surprisingly, for semantic correspondence, SDXL and SDXL-Turbo perform significantly worse than the others, even though they are newer models with higher image generation quality <span class="citation" data-cites="podell2023sdxl sauer2023adversarial">[<a href="#ref-podell2023sdxl" role="doc-biblioref">12</a>, <a href="#ref-sauer2023adversarial" role="doc-biblioref">51</a>]</span>. Interestingly, the semantic correspondence performance of the Turbo variants (SD-Turbo, SDXL-Turbo) <span class="citation" data-cites="sauer2023adversarial">[<a href="#ref-sauer2023adversarial" role="doc-biblioref">51</a>]</span> is better than the respective base models (SD-2.1, SDXL). We find time step (noise level) 50 out of 1000 to be a good choice for representation extraction across all evaluated models. The choice of the time step is highly debated topic in the literature, with both higher and lower values being suggested (see <a href="#sec-related-work" class="quarto-xref">Section&nbsp;1.1</a>). While our finding is not unusual, a higher time step seems more common <span class="citation" data-cites="zhang2023tale tang2023emergent">[<a href="#ref-tang2023emergent" role="doc-biblioref">3</a>, <a href="#ref-zhang2023tale" role="doc-biblioref">4</a>]</span>.</p>
<p><strong>Performance Improvements.</strong> We also investigated methods of improving the performance using the representations for semantic correspondence on the <a href="#sec-datasets-spair"> SPair-71k</a> dataset, see <a href="#sec-sc-improvements" class="quarto-xref">Section&nbsp;4.3</a>. Some of these ideas are inspired by the found biases, while others are more general approaches or inspired by the literature. We find several methods that slightly improve the performance by up to 1-2 PCK. Especially methods that reduce noise in the representations, such as representation averaging, tend to help. In general, the more complex approaches suggested in the literature remain more promising <span class="citation" data-cites="zhang2023tale zhang2024telling luo2023dhf stracke2024clean">[<a href="#ref-zhang2023tale" role="doc-biblioref">4</a>, <a href="#ref-zhang2024telling" role="doc-biblioref">18</a>, <a href="#ref-luo2023dhf" role="doc-biblioref">20</a>, <a href="#ref-stracke2024clean" role="doc-biblioref">23</a>]</span>. Our results of only relatively small improvements indicate that before implementing further optimization methods on top of the representations, it is important to first determine the most performant choice of block(s) and time step for representation extraction.</p>
<!-- Could do: join the literature comparison with the performance evaluation -->
<p><strong>Comparison with Literature.</strong> Our results for semantic correspondence on <a href="#sec-datasets-spair"> SPair-71k</a> without further enhancements tend to fall in line with existing literature using SD representations <span class="citation" data-cites="tang2023emergent">[<a href="#ref-tang2023emergent" role="doc-biblioref">3</a>]</span>. Other works vary in how they extract representations and many additionally employ further optimizations and methods to improve the performance. <span class="citation" data-cites="hedlin2023unsupervised">[<a href="#ref-hedlin2023unsupervised" role="doc-biblioref">21</a>]</span> do semantic correspondence using the attention maps in the U-Net of SD-1.4 reporting a PCK@0.1<span class="math inline">\(_\text{bbox}\)</span> of 45.4, which is lower than our results. <span class="citation" data-cites="tang2023emergent">[<a href="#ref-tang2023emergent" role="doc-biblioref">3</a>]</span> use a relatively simple and mostly similar approach to ours, but include e.g.&nbsp;representation averaging (as discussed in <a href="#sec-sc-improvements" class="quarto-xref">Section&nbsp;4.3</a>), and report 52.8 PCK@0.1<span class="math inline">\(_\text{bbox}\)</span> - very similar to our results. Improvements to this are reported, for example, by <span class="citation" data-cites="zhang2023tale">[<a href="#ref-zhang2023tale" role="doc-biblioref">4</a>]</span>, who fuse SD and DINO representations and achieve up to 63.73 PCK@0.1<span class="math inline">\(_\text{bbox}\)</span>. <span class="citation" data-cites="zhang2024telling">[<a href="#ref-zhang2024telling" role="doc-biblioref">18</a>]</span> further improve on this by introducing a test-time adaptive pose alignment strategy to achieve 68.64 PCK@0.1<span class="math inline">\(_\text{bbox}\)</span>. <span class="citation" data-cites="stracke2024clean">[<a href="#ref-stracke2024clean" role="doc-biblioref">23</a>]</span> propose a fine-tuning method for SD to improve the quality of the representations resulting in further improvements by 1-2 PCK.</p>
<p>When comparing our results with non-diffusion foundation models for representation learning, SD performs well, but not strictly better than other approaches. For DINO (v1, ViT-S/8) <span class="citation" data-cites="tang2023emergent">[<a href="#ref-tang2023emergent" role="doc-biblioref">3</a>]</span>, the performance on semantic correspondence on <a href="#sec-datasets-spair"> SPair-71k</a> is reported to be relatively low at 33.3 PCK@0.1<span class="math inline">\(_\text{bbox}\)</span> <span class="citation" data-cites="zhang2023tale">[<a href="#ref-zhang2023tale" role="doc-biblioref">4</a>]</span>. For DINOv2 (ViT-B/14) <span class="citation" data-cites="oquab2024dinov">[<a href="#ref-oquab2024dinov" role="doc-biblioref">15</a>]</span>, it is at 55.6 PCK@0.1<span class="math inline">\(_\text{bbox}\)</span> <span class="citation" data-cites="zhang2023tale">[<a href="#ref-zhang2023tale" role="doc-biblioref">4</a>]</span>, i.e.&nbsp;slighly higher than our results for SD.</p>
<p><strong>Tool Development.</strong> The tools developed in the course of this thesis have been instrumental in identifying and quantifying the biases and anomalies (see <a href="#sec-representation-extraction-exploration" class="quarto-xref">Section&nbsp;3</a>). Both the <code>sdhelper</code> package allowing for faster implementation of experiments and the <em>Representation Similarity Explorer</em> for interactive exploration of representation similarities proved to be invaluable tools.</p>
<!-- 
Could add:
* maybe shortly mention SD2.1 `up[0]` behaving weirdly
* maybe mention advantage in terms of interpretability when using representation similarities, because one can look at the similarity maps to understand what the model is doing
 -->
</section>
<section id="sec-limitations" class="level2">
<h2 class="anchored" data-anchor-id="sec-limitations">Limitations</h2>
<p>While we believe our findings contribute meaningfully to understanding representation extraction from latent diffusion models, there are several important limitations to consider. In general, the field of using diffusion models for representation extraction is very young, and about half of our references were published in 2024 or are only available as preprints, and thus are potentially not as rigorously evaluated.</p>
<p><strong>Representation Extraction.</strong> We use a default time step (50) for representation extraction, which could cause us to miss time step dependent properties or improvements. For example, <span class="citation" data-cites="tang2023emergent">[<a href="#ref-tang2023emergent" role="doc-biblioref">3</a>]</span> find that higher time steps relate to more semantic representations, while lower time steps relate to more low-level bias. Additionally, a non-zero time step inherently adds noise to the latent image, which can reduce the information available for representation extraction, especially fine-grained details <span class="citation" data-cites="stracke2024clean">[<a href="#ref-stracke2024clean" role="doc-biblioref">23</a>]</span>. Approaches against this include averaging over multiple representations <span class="citation" data-cites="tang2023emergent">[<a href="#ref-tang2023emergent" role="doc-biblioref">3</a>]</span> or using specific finetuning methods <span class="citation" data-cites="stracke2024clean">[<a href="#ref-stracke2024clean" role="doc-biblioref">23</a>]</span>. An additional limitation that might prevent us from fully utilizing the representations is that we use an empty prompt. As indicated in <a href="#sec-related-work" class="quarto-xref">Section&nbsp;1.1</a> and <a href="#sec-sc-improvements" class="quarto-xref">Section&nbsp;4.3</a>, using relevant text prompt conditioning during representation extraction can improve the performance on downstream tasks <span class="citation" data-cites="xu2023open li2023sd4match tang2023emergent zhang2023tale zhang2025three">[<a href="#ref-tang2023emergent" role="doc-biblioref">3</a>, <a href="#ref-zhang2023tale" role="doc-biblioref">4</a>, <a href="#ref-li2023sd4match" role="doc-biblioref">22</a>, <a href="#ref-zhang2025three" role="doc-biblioref">35</a>, <a href="#ref-xu2023open" role="doc-biblioref">97</a>]</span>, however, a relevant text prompt is not always available. Furthermore, the spatial resolution of the representations is relatively low, especially for the lower blocks, as described in <a href="#sec-methods-representation-extraction" class="quarto-xref">Section&nbsp;2.2</a>. This limits the performance on downstream tasks that rely on spatially accurate predictions, such as semantic correspondence.</p>
<p><strong>Downstream Tasks.</strong> To utilize the representations for downstream tasks, we either use cosine similarity or linear probes. While cosine similarity is commonly used <span class="citation" data-cites="tang2023emergent zhang2023tale luo2023dhf">[<a href="#ref-tang2023emergent" role="doc-biblioref">3</a>, <a href="#ref-zhang2023tale" role="doc-biblioref">4</a>, <a href="#ref-luo2023dhf" role="doc-biblioref">20</a>]</span>, e.g. <span class="citation" data-cites="steck2024cosinesimilarity">[<a href="#ref-steck2024cosinesimilarity" role="doc-biblioref">100</a>]</span> caution against “blindly using cosine similarity” for embeddings, as it can yield meaningless similarities in certain cases. Other similarity measures might be more suitable for certain tasks or models. For example, for SD-3, a diffusion transformer discussed in <a href="#sec-future-work" class="quarto-xref">Section&nbsp;6.3</a>, our preliminary observations indicate that plain cosine similarity does not work well. While the usage of linear probes might be a good starting point for evaluating the representations, more sophisticated methods can further improve the performance, as described in <a href="#sec-related-work" class="quarto-xref">Section&nbsp;1.1</a>. A general difficulty of evaluating the impact of biases in the representations is the existence of image composition biases, such as foreground objects typically being located in the center of the image. This relationship between image content and spatial positions adds an additional potential origin for observed spatial differences in the representations, and therefore complicates the evaluation of representation biases.</p>
<p><strong>Computational Constraints.</strong> Due to time and resource constraints, we were not able to perform all experiments on all SD models and over all potentially interesting hyperparameter settings. This might have prevented us from finding more conclusive results and additional insights. Especially the focus on only a few U-Net based SD models limits the generalizability of our findings.</p>
</section>
<section id="sec-future-work" class="level2">
<h2 class="anchored" data-anchor-id="sec-future-work">Future Work</h2>
<p>There are several directions for future work. First of all, based on the intuition of the image generation capabilities of diffusion models hinting at semantic representations, the relationship between image and representation quality could be further investigated.</p>
<section id="other-tasks" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="other-tasks">Other Tasks</h3>
<p>In this thesis, we evaluate the representations and their biases on semantic correspondence, dense correspondence, depth estimation, and linear probe classification. As described in <a href="#sec-related-work" class="quarto-xref">Section&nbsp;1.1</a>, existing works also explore the suitability of diffusion model representations for other tasks, such as semantic segmentation <span class="citation" data-cites="baranchuk2022labelefficient ji2024diffusion couairon2024zeroshot zhao2023unleashing tian2024diffuse zhang2025three couairon2024diffcutcatalyzingzeroshotsemantic Yang2023Diffusion">[<a href="#ref-zhao2023unleashing" role="doc-biblioref">34</a>, <a href="#ref-zhang2025three" role="doc-biblioref">35</a>, <a href="#ref-baranchuk2022labelefficient" role="doc-biblioref">36</a>, <a href="#ref-ji2024diffusion" role="doc-biblioref">37</a>, <a href="#ref-couairon2024zeroshot" role="doc-biblioref">38</a>, <a href="#ref-tian2024diffuse" role="doc-biblioref">39</a>, <a href="#ref-couairon2024diffcutcatalyzingzeroshotsemantic" role="doc-biblioref">40</a>, <a href="#ref-Yang2023Diffusion" role="doc-biblioref">41</a>]</span>, robot control <span class="citation" data-cites="gupta2024pretrained shridhar2024generativeimageactionmodels tsagkas2024clickgraspzeroshotprecise">[<a href="#ref-gupta2024pretrained" role="doc-biblioref">42</a>, <a href="#ref-shridhar2024generativeimageactionmodels" role="doc-biblioref">43</a>, <a href="#ref-tsagkas2024clickgraspzeroshotprecise" role="doc-biblioref">44</a>]</span>, modification of the image generation process <span class="citation" data-cites="park2023unsupervised jeong2024trainingfree haas2023discovering gambashidze2024aligningdiffusionmodelsnoiseconditioned hudson2023soda park2023understanding">[<a href="#ref-hudson2023soda" role="doc-biblioref">31</a>, <a href="#ref-park2023understanding" role="doc-biblioref">47</a>, <a href="#ref-park2023unsupervised" role="doc-biblioref">75</a>, <a href="#ref-jeong2024trainingfree" role="doc-biblioref">76</a>, <a href="#ref-haas2023discovering" role="doc-biblioref">77</a>, <a href="#ref-gambashidze2024aligningdiffusionmodelsnoiseconditioned" role="doc-biblioref">78</a>]</span>, 3D scene understanding <span class="citation" data-cites="Man2024Lexicon3DPV">[<a href="#ref-Man2024Lexicon3DPV" role="doc-biblioref">79</a>]</span>, and surface normal estimation <span class="citation" data-cites="Ke2024RepurposingDI Lee2024ExploitingDI xu2024diffusionmodelstrainedlarge ye2024stablenormalreducingdiffusionvariance">[<a href="#ref-ye2024stablenormalreducingdiffusionvariance" role="doc-biblioref">45</a>, <a href="#ref-Ke2024RepurposingDI" role="doc-biblioref">80</a>, <a href="#ref-Lee2024ExploitingDI" role="doc-biblioref">81</a>, <a href="#ref-xu2024diffusionmodelstrainedlarge" role="doc-biblioref">82</a>]</span>. Creating a comprehensive overview of the suitability of diffusion model representations for all these tasks and a comparison between different models and alternatives such as DINOv2 <span class="citation" data-cites="oquab2024dinov">[<a href="#ref-oquab2024dinov" role="doc-biblioref">15</a>]</span> is a promising direction for future work, and may give insights into which architectural decisions and training methods lead to useful representations.</p>
<p>Compared to this thesis, many of these existing works already employ more complex methods to extract and use the representations for the tasks. However, comparing and improving these methods and developing new ones can lead to further improvements and new insights about the representations. Particularly interesting seem the directions of <span class="citation" data-cites="zhang2024telling">[<a href="#ref-zhang2024telling" role="doc-biblioref">18</a>]</span>, who add test-time adaptive pose alignment to a combination of SD and DINO representations, and <span class="citation" data-cites="stracke2024clean">[<a href="#ref-stracke2024clean" role="doc-biblioref">23</a>]</span>, who remove the requirement of adding noise during representation extraction.</p>
</section>
<section id="biases" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="biases">Biases</h3>
<p>Our findings about the biases and anomalies in the representations are based on both qualitative and quantitative analysis. However, especially further quantitative experiments could help to better understand the biases and anomalies, and their implications for representation extraction and downstream tasks. For the position bias, we found correspondences between absolute positions, but the analysis could be extended to relative positional information. For the texture and color bias, our analyses could be extended by evaluating the robustness to style transfer over different blocks. For the high-norm anomalies, their origin, relevance for the model, and impact on downstream tasks are interesting directions for future research. Especially noteworthy is the potential connection to the artifacts found by <span class="citation" data-cites="darcet2024vision">[<a href="#ref-darcet2024vision" role="doc-biblioref">70</a>]</span> in vision transformers.</p>
<p>In general, our experiments could benefit from being extended to the output of the attention and ResNet layers, and potentially also to the attention scores inside the attention layers <span class="citation" data-cites="zhao2023unleashing hedlin2023unsupervised">[<a href="#ref-hedlin2023unsupervised" role="doc-biblioref">21</a>, <a href="#ref-zhao2023unleashing" role="doc-biblioref">34</a>]</span>. Moreover, extending the experiments and the analysis of the impact of the biases and anomalies on downstream tasks to additional models, tasks, and datasets would strengthen the generalizability of our findings. Especially for the position bias and the anomalies, an exploration into their origins may be insightful. As a further step, mitigating negative effects of the biases and anomalies on downstream tasks could be an important direction for future investigation.</p>
</section>
<section id="diffusion-transformer" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="diffusion-transformer">Diffusion Transformer</h3>
<div id="cell-fig-sd3-example" class="cell" data-wrapfigure="R 0.55" data-execution_count="33">
<div class="cell-output cell-output-display">
<div id="fig-sd3-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sd3-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-sd3-example-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sd3-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;26: Representation L2 norms and cosine similarity for SD-3-medium at transformer block 12. Cosine similarity is relative to the token at position (24,34) in image 1 (cat’s eye).
</figcaption>
</figure>
</div>
</div>
</div>
<p>During the course of this thesis, the state-of-the-art in open-source image generation shifted from the U-Net based models to diffusion transformers <span class="citation" data-cites="peebles2023scalable">[<a href="#ref-peebles2023scalable" role="doc-biblioref">55</a>]</span>. Stability AI released SD-3 in February 2024 <span class="citation" data-cites="esser2024scaling">[<a href="#ref-esser2024scaling" role="doc-biblioref">56</a>]</span> and later SD-3.5 <span class="citation" data-cites="stabilityai2024sd35">[<a href="#ref-stabilityai2024sd35" role="doc-biblioref">59</a>]</span> and a Turbo variant <span class="citation" data-cites="sauer2024fasthighresolutionimagesynthesis">[<a href="#ref-sauer2024fasthighresolutionimagesynthesis" role="doc-biblioref">101</a>]</span>. The SD-3 models primarily improve prompt adherence and text support, which is achieved using a transformer-only denoising model that more deeply integrates the text embedding into the model architecture <span class="citation" data-cites="esser2024scaling">[<a href="#ref-esser2024scaling" role="doc-biblioref">56</a>]</span>.</p>
<p>Additionally, Black Forest Labs released the Flux.1 family of models in August 2024, which not only set a new state-of-the-art in open-source image generation, but also rival the best closed-source models such as Midjourney v6.0 or Ideogram v2.0 <span class="citation" data-cites="blackforestlabs2024fluxRepo blackforestlabs2024fluxAnnouncement">[<a href="#ref-blackforestlabs2024fluxAnnouncement" role="doc-biblioref">14</a>, <a href="#ref-blackforestlabs2024fluxRepo" role="doc-biblioref">60</a>]</span>. This indicates that now art generated even by open-source diffusion models is not distinguishable as AI-generated anymore for general users <span class="citation" data-cites="ha2024organicdiffuseddistinguishhuman">[<a href="#ref-ha2024organicdiffuseddistinguishhuman" role="doc-biblioref">53</a>]</span>.</p>
<p>We analyze SD-3-medium and Flux.1-schnell to get a first glimpse into their potential for representation extraction. Both models have a similar general architecture and are latent diffusion models just as the previous SD models, but switch for the denoising process from a U-Net to a transformer-based model. SD-3-medium has 24 transformer blocks with 1536 channels and a spatial resolution of 64 at the default image size of 1024. Flux.1-schnell has 19 transformer blocks with 3072 channels and the same spatial resolution of 64 at an image size of 1024. The representations can be similarly extracted and analyzed as for the U-Net based SD models. One difference is that for the diffusion transformer models, the representations for all blocks have the same spatial and channel dimensions.</p>
<p>Preliminary results for downstream tasks, such as semantic correspondence, show relatively low performance when using the same settings as for the U-Net based models. In the <em>Representation Similarity Explorer</em> (see <a href="#sec-repr-sim-explorer" class="quarto-xref">Section&nbsp;3.2</a>) other similarity measures than cosine similarity, for example centered cosine similarity, or an L2 based similarity, seem to result in more semantically meaningful similarity maps. But as visible in <a href="#fig-sd3-example" class="quarto-xref">Figure&nbsp;26</a>, even the cosine similarity maps can be semantically meaningful. Therefore, investigating the suitability of diffusion transformers for representation extraction seems like a promising direction for future research.</p>
</section>
</section>
</section>
<section id="sec-conclusion" class="level1">
<h1>Conclusion</h1>
<p>This thesis investigated the representation similarity in latent diffusion models, specifically SD, aiming to assess the suitability for downstream tasks and to uncover inherent properties and biases. Our findings confirm the potential of SD models as a source of useful visual representations, but simultaneously reveal limitations arising from a position bias, a texture and color bias, and high-norm anomalies. To the best of our knowledge, our investigation of the position bias and the high-norm anomalies constitute the main novelties of our work, contributing new insights on the usage of SD models for downstream tasks.</p>
<p>We found that the representations of SD are not solely driven by semantic content. First, we found a <strong>positional bias</strong>, primarily in the lower blocks of the SD U-Net. They encode linearly extractable positional information that is relative to the image borders. It can negatively impact tasks like semantic correspondence by overshadowing semantic similarities, but in other cases might also be helpful when the absolute spatial position of objects is relevant. Second, we investigated a <strong>texture and color bias</strong>, and found that the upper blocks are more sensitive to low level visual features, such as texture and color. In contrast, the lower blocks capture more semantically meaningful information. Third, we uncovered several types of spatially-localized <strong>anomalies</strong>, which appear in the representations of some images. Similarities for corner tokens, and to some degree border tokens, are often less semantically meaningful than for other positions. Furthermore, we observe groups of tokens with a very high L2 norm in the upper blocks, primarily <code>conv-in</code>, with unclear origins. Similarly, in the lower blocks (especially <code>up[1]</code>), which are more relevant for representation extraction, we observe a second type of high-norm anomalies. There, the cosine similarity between the anomalous tokens is so high, that new anomalous tokens can be detected with it. These <code>up[1]</code> anomalies might be related to artifacts found in vision transformers <span class="citation" data-cites="darcet2024vision">[<a href="#ref-darcet2024vision" role="doc-biblioref">70</a>]</span>. We showed that these biases and anomalies affect downstream tasks in some cases, but the impact varies depending on the model, task, data, and hyperparameters.</p>
<p>We demonstrated the utility of SD representations in linear probe classification, and primarily semantic correspondence, while also emphasizing the need for careful selection of the U-Net block and time step. Notably, the distilled Turbo models <span class="citation" data-cites="sauer2023adversarial">[<a href="#ref-sauer2023adversarial" role="doc-biblioref">51</a>]</span> seem to provide slightly better representations, indicating that training methods can further improve representation quality. However, the more recent SDXL models surprisingly show a significant decrease in semantic correspondence performance compared to SD-1.5 and SD-2.1, which challenges the idea that better image generation capabilities indicate more useful representations.</p>
<p>In summary, our results demonstrate that SD models learn representations that contain more than just semantic information. Future research should focus on how the identified biases and anomalies can be mitigated and if they generalize to other models, including new architectures like diffusion transformers. This will not only advance the field of representation learning but also promote more reliable and robust use of diffusion model representations in downstream tasks.</p>
</section>
<section id="sec-acknowledgements" class="level1">
<h1>Acknowledgements</h1>
<p>Many thanks to Lorenz for the great supervision, countless meetings, and helpful feedback.</p>
<p>Thank you to Prof.&nbsp;Dr.&nbsp;Klaus-Robert Müller and Prof.&nbsp;Dr.&nbsp;Grégoire Montavon for the opportunity to work on this thesis. Also thanks to the Machine Learning Group at TU Berlin in general for providing a great environment for research. Thanks to friends, family, and especially Jule, for all the support, motivation, and helpful discussions along the way.</p>
</section>
<section id="references" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body" data-entry-spacing="0" role="list">
<div id="ref-rombach2022highresolution" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, <span>“High-resolution image synthesis with latent diffusion models,”</span> in <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)</em>, 2022, pp. 10684–10695. doi: <a href="https://doi.org/10.1109/CVPR52688.2022.01042">10.1109/CVPR52688.2022.01042</a>.</div>
</div>
<div id="ref-ronneberger2015unet" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">O. Ronneberger, P. Fischer, and T. Brox, <span>“U-net: Convolutional networks for biomedical image segmentation,”</span> in <em>Medical image computing and computer-assisted intervention – MICCAI 2015</em>, Springer International Publishing, 2015, pp. 234–241. doi: <a href="https://doi.org/10.1007/978-3-319-24574-4_28">10.1007/978-3-319-24574-4_28</a>.</div>
</div>
<div id="ref-tang2023emergent" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">L. Tang, M. Jia, Q. Wang, C. P. Phoo, and B. Hariharan, <span>“Emergent correspondence from image diffusion,”</span> in <em>Thirty-seventh conference on neural information processing systems</em>, 2023. Available: <a href="https://openreview.net/forum?id=ypOiXjdfnU">https://openreview.net/forum?id=ypOiXjdfnU</a></div>
</div>
<div id="ref-zhang2023tale" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">J. Zhang <em>et al.</em>, <span>“A tale of two features: Stable diffusion complements DINO for zero-shot semantic correspondence,”</span> <em>Advances in Neural Information Processing Systems</em>, vol. 36, pp. 45533–45547, 2023, Available: <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/8e9bdc23f169a05ea9b72ccef4574551-Paper-Conference.pdf">https://proceedings.neurips.cc/paper_files/paper/2023/file/8e9bdc23f169a05ea9b72ccef4574551-Paper-Conference.pdf</a></div>
</div>
<div id="ref-pmlr-v37-sohl-dickstein15" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli, <span>“Deep unsupervised learning using nonequilibrium thermodynamics,”</span> in <em>Proceedings of the 32nd international conference on machine learning</em>, in Proceedings of machine learning research, vol. 37. PMLR, 2015, pp. 2256–2265. Available: <a href="https://proceedings.mlr.press/v37/sohl-dickstein15.html">https://proceedings.mlr.press/v37/sohl-dickstein15.html</a></div>
</div>
<div id="ref-ho2020denoising" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">J. Ho, A. Jain, and P. Abbeel, <span>“Denoising diffusion probabilistic models,”</span> <em>Advances in Neural Information Processing Systems</em>, vol. 33, pp. 6840–6851, 2020, Available: <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf</a></div>
</div>
<div id="ref-dhariwal2021diffusion" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">P. Dhariwal and A. Nichol, <span>“Diffusion models beat GANs on image synthesis,”</span> in <em>Advances in neural information processing systems</em>, Curran Associates, Inc., 2021, pp. 8780–8794. Available: <a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf</a></div>
</div>
<div id="ref-videoworldsimulators2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">T. Brooks <em>et al.</em>, <span>“Video generation models as world simulators,”</span> 2024, Available: <a href="https://openai.com/research/video-generation-models-as-world-simulators">https://openai.com/research/video-generation-models-as-world-simulators</a></div>
</div>
<div id="ref-kong2021diffwave" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro, <span>“DiffWave: A versatile diffusion model for audio synthesis,”</span> in <em>International conference on learning representations</em>, 2021. Available: <a href="https://openreview.net/forum?id=a-xFK8Ymz5J">https://openreview.net/forum?id=a-xFK8Ymz5J</a></div>
</div>
<div id="ref-singh2023codefusion" class="csl-entry" role="listitem">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">M. Singh, J. Cambronero, S. Gulwani, V. Le, G. Verbruggen, and C. Negreanu, <span>“CODEFUSION: A pre-trained diffusion model for code generation,”</span> in <em>EMNLP 2023</em>, 2023. Available: <a href="https://www.microsoft.com/en-us/research/publication/codefusion-a-pre-trained-diffusion-model-for-code-generation/">https://www.microsoft.com/en-us/research/publication/codefusion-a-pre-trained-diffusion-model-for-code-generation/</a></div>
</div>
<div id="ref-nichol2022glidephotorealisticimagegeneration" class="csl-entry" role="listitem">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">A. Nichol <em>et al.</em>, <span>“GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models.”</span> 2022. Available: <a href="https://arxiv.org/abs/2112.10741">https://arxiv.org/abs/2112.10741</a></div>
</div>
<div id="ref-podell2023sdxl" class="csl-entry" role="listitem">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">D. Podell <em>et al.</em>, <span>“SDXL: Improving latent diffusion models for high-resolution image synthesis.”</span> 2023. Available: <a href="https://arxiv.org/abs/2307.01952">https://arxiv.org/abs/2307.01952</a></div>
</div>
<div id="ref-betker2023improving" class="csl-entry" role="listitem">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">J. Betker <em>et al.</em>, <span>“Improving image generation with better captions,”</span> 2023, Available: <a href="https://cdn.openai.com/papers/dall-e-3.pdf">https://cdn.openai.com/papers/dall-e-3.pdf</a></div>
</div>
<div id="ref-blackforestlabs2024fluxAnnouncement" class="csl-entry" role="listitem">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline">Black Forest Labs, <span>“Announcing black forest labs.”</span> 2024. Available: <a href="https://blackforestlabs.ai/announcing-black-forest-labs/">https://blackforestlabs.ai/announcing-black-forest-labs/</a></div>
</div>
<div id="ref-oquab2024dinov" class="csl-entry" role="listitem">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline">M. Oquab <em>et al.</em>, <span>“<span>DINO</span>v2: Learning robust visual features without supervision,”</span> <em>Transactions on Machine Learning Research</em>, 2024, Available: <a href="https://openreview.net/forum?id=a68SUt6zFt">https://openreview.net/forum?id=a68SUt6zFt</a></div>
</div>
<div id="ref-radford2021learning" class="csl-entry" role="listitem">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline">A. Radford <em>et al.</em>, <span>“Learning transferable visual models from natural language supervision,”</span> in <em>Proceedings of the 38th international conference on machine learning</em>, in Proceedings of machine learning research, vol. 139. PMLR, 2021, pp. 8748–8763. Available: <a href="https://proceedings.mlr.press/v139/radford21a.html">https://proceedings.mlr.press/v139/radford21a.html</a></div>
</div>
<div id="ref-artificialanalysis2024quality" class="csl-entry" role="listitem">
<div class="csl-left-margin">[17] </div><div class="csl-right-inline">Artificial Analysis, <span>“Text to image AI model &amp; provider leaderboard - quality ELO.”</span> 2024. Available: <a href="https://artificialanalysis.ai/text-to-image#quality">https://artificialanalysis.ai/text-to-image#quality</a></div>
</div>
<div id="ref-zhang2024telling" class="csl-entry" role="listitem">
<div class="csl-left-margin">[18] </div><div class="csl-right-inline">J. Zhang <em>et al.</em>, <span>“Telling left from right: Identifying geometry-aware semantic correspondence,”</span> in <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)</em>, 2024, pp. 3076–3085. Available: <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Telling_Left_from_Right_Identifying_Geometry-Aware_Semantic_Correspondence_CVPR_2024_paper.html">https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Telling_Left_from_Right_Identifying_Geometry-Aware_Semantic_Correspondence_CVPR_2024_paper.html</a></div>
</div>
<div id="ref-banani2024probing" class="csl-entry" role="listitem">
<div class="csl-left-margin">[19] </div><div class="csl-right-inline">M. El Banani <em>et al.</em>, <span>“Probing the 3D awareness of visual foundation models,”</span> in <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)</em>, 2024, pp. 21795–21806. Available: <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Banani_Probing_the_3D_Awareness_of_Visual_Foundation_Models_CVPR_2024_paper.html">https://openaccess.thecvf.com/content/CVPR2024/html/Banani_Probing_the_3D_Awareness_of_Visual_Foundation_Models_CVPR_2024_paper.html</a></div>
</div>
<div id="ref-luo2023dhf" class="csl-entry" role="listitem">
<div class="csl-left-margin">[20] </div><div class="csl-right-inline">G. Luo, L. Dunlap, D. H. Park, A. Holynski, and T. Darrell, <span>“Diffusion hyperfeatures: Searching through time and space for semantic correspondence,”</span> in <em>Advances in neural information processing systems</em>, 2023. Available: <a href="https://openreview.net/forum?id=Vm1zeYqwdc">https://openreview.net/forum?id=Vm1zeYqwdc</a></div>
</div>
<div id="ref-hedlin2023unsupervised" class="csl-entry" role="listitem">
<div class="csl-left-margin">[21] </div><div class="csl-right-inline">E. Hedlin <em>et al.</em>, <span>“Unsupervised semantic correspondence using stable diffusion,”</span> <em>Advances in Neural Information Processing Systems</em>, vol. 36, pp. 8266–8279, 2023, Available: <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/1a074a28c3a6f2056562d00649ae6416-Paper-Conference.pdf">https://proceedings.neurips.cc/paper_files/paper/2023/file/1a074a28c3a6f2056562d00649ae6416-Paper-Conference.pdf</a></div>
</div>
<div id="ref-li2023sd4match" class="csl-entry" role="listitem">
<div class="csl-left-margin">[22] </div><div class="csl-right-inline">X. Li, J. Lu, K. Han, and V. A. Prisacariu, <span>“SD4Match: Learning to prompt stable diffusion model for semantic matching,”</span> in <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)</em>, 2024, pp. 27558–27568. Available: <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_SD4Match_Learning_to_Prompt_Stable_Diffusion_Model_for_Semantic_Matching_CVPR_2024_paper.html">https://openaccess.thecvf.com/content/CVPR2024/html/Li_SD4Match_Learning_to_Prompt_Stable_Diffusion_Model_for_Semantic_Matching_CVPR_2024_paper.html</a></div>
</div>
<div id="ref-stracke2024clean" class="csl-entry" role="listitem">
<div class="csl-left-margin">[23] </div><div class="csl-right-inline">N. Stracke, S. A. Baumann, K. Bauer, F. Fundel, and B. Ommer, <span>“CleanDIFT: Diffusion features without noise.”</span> 2024. Available: <a href="https://arxiv.org/abs/2412.03439">https://arxiv.org/abs/2412.03439</a></div>
</div>
<div id="ref-fundel2024distillationdiffusionfeaturessemantic" class="csl-entry" role="listitem">
<div class="csl-left-margin">[24] </div><div class="csl-right-inline">F. Fundel, J. Schusterbauer, V. T. Hu, and B. Ommer, <span>“Distillation of diffusion features for semantic correspondence.”</span> 2024. Available: <a href="https://arxiv.org/abs/2412.03512">https://arxiv.org/abs/2412.03512</a></div>
</div>
<div id="ref-mariotti2024improving" class="csl-entry" role="listitem">
<div class="csl-left-margin">[25] </div><div class="csl-right-inline">O. Mariotti, O. M. Aodha, and H. Bilen, <span>“Improving semantic correspondence with viewpoint-guided spherical maps,”</span> in <em>2024 IEEE/CVF conference on computer vision and pattern recognition (CVPR)</em>, 2024, pp. 19521–19530. doi: <a href="https://doi.org/10.1109/CVPR52733.2024.01846">10.1109/CVPR52733.2024.01846</a>.</div>
</div>
<div id="ref-kim2025matchme" class="csl-entry" role="listitem">
<div class="csl-left-margin">[26] </div><div class="csl-right-inline">J. Kim, B. Heo, S. Yun, S. Kim, and D. Han, <span>“Match me if you can: Semi-supervised semantic correspondence learning with unpaired images,”</span> in <em>Computer vision – ACCV 2024</em>, Springer Nature Singapore, 2025, pp. 462–479. Available: <a href="https://link.springer.com/chapter/10.1007/978-981-96-0960-4_28">https://link.springer.com/chapter/10.1007/978-981-96-0960-4_28</a></div>
</div>
<div id="ref-xiang2023denoising" class="csl-entry" role="listitem">
<div class="csl-left-margin">[27] </div><div class="csl-right-inline">W. Xiang, H. Yang, D. Huang, and Y. Wang, <span>“Denoising diffusion autoencoders are unified self-supervised learners,”</span> in <em>Proceedings of the IEEE/CVF international conference on computer vision (ICCV)</em>, 2023, pp. 15802–15812. doi: <a href="https://doi.org/10.1109/ICCV51070.2023.01448">10.1109/ICCV51070.2023.01448</a>.</div>
</div>
<div id="ref-clark2024text" class="csl-entry" role="listitem">
<div class="csl-left-margin">[28] </div><div class="csl-right-inline">K. Clark and P. Jaini, <span>“Text-to-image diffusion models are zero shot classifiers,”</span> <em>Advances in Neural Information Processing Systems</em>, vol. 36, pp. 58921–58937, 2023, Available: <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/b87bdcf963cad3d0b265fcb78ae7d11e-Paper-Conference.pdf">https://proceedings.neurips.cc/paper_files/paper/2023/file/b87bdcf963cad3d0b265fcb78ae7d11e-Paper-Conference.pdf</a></div>
</div>
<div id="ref-li2023diffusion" class="csl-entry" role="listitem">
<div class="csl-left-margin">[29] </div><div class="csl-right-inline">A. C. Li, M. Prabhudesai, S. Duggal, E. Brown, and D. Pathak, <span>“Your diffusion model is secretly a zero-shot classifier,”</span> in <em>Proceedings of the IEEE/CVF international conference on computer vision (ICCV)</em>, 2023, pp. 2206–2217. Available: <a href="https://diffusion-classifier.github.io/static/docs/DiffusionClassifier.pdf">https://diffusion-classifier.github.io/static/docs/DiffusionClassifier.pdf</a></div>
</div>
<div id="ref-mukhopadhyay2023diffusion" class="csl-entry" role="listitem">
<div class="csl-left-margin">[30] </div><div class="csl-right-inline">S. Mukhopadhyay <em>et al.</em>, <span>“Diffusion models beat GANs on image classification.”</span> 2023. Available: <a href="https://arxiv.org/abs/2307.08702">https://arxiv.org/abs/2307.08702</a></div>
</div>
<div id="ref-hudson2023soda" class="csl-entry" role="listitem">
<div class="csl-left-margin">[31] </div><div class="csl-right-inline">D. A. Hudson <em>et al.</em>, <span>“SODA: Bottleneck diffusion models for representation learning,”</span> in <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)</em>, 2024, pp. 23115–23127.</div>
</div>
<div id="ref-Chen2023BeyondSS" class="csl-entry" role="listitem">
<div class="csl-left-margin">[32] </div><div class="csl-right-inline">Y. Chen, F. Vi’egas, and M. Wattenberg, <span>“Beyond surface statistics: Scene representations in a latent diffusion model.”</span> 2023. Available: <a href="https://arxiv.org/abs/2306.05720">https://arxiv.org/abs/2306.05720</a></div>
</div>
<div id="ref-Patni2024ECoDepth" class="csl-entry" role="listitem">
<div class="csl-left-margin">[33] </div><div class="csl-right-inline">S. Patni, A. Agarwal, and C. Arora, <span>“ECoDepth: Effective conditioning of diffusion models for monocular depth estimation,”</span> in <em>2024 IEEE/CVF conference on computer vision and pattern recognition (CVPR)</em>, 2024, pp. 28285–28295. doi: <a href="https://doi.org/10.1109/CVPR52733.2024.02672">10.1109/CVPR52733.2024.02672</a>.</div>
</div>
<div id="ref-zhao2023unleashing" class="csl-entry" role="listitem">
<div class="csl-left-margin">[34] </div><div class="csl-right-inline">W. Zhao, Y. Rao, Z. Liu, B. Liu, J. Zhou, and J. Lu, <span>“Unleashing text-to-image diffusion models for visual perception,”</span> in <em>2023 IEEE/CVF international conference on computer vision (ICCV)</em>, IEEE Computer Society, 2023, pp. 5706–5716. doi: <a href="https://doi.org/10.1109/ICCV51070.2023.00527">10.1109/ICCV51070.2023.00527</a>.</div>
</div>
<div id="ref-zhang2025three" class="csl-entry" role="listitem">
<div class="csl-left-margin">[35] </div><div class="csl-right-inline">M. Zhang, G. Song, X. Shi, Y. Liu, and H. Li, <span>“Three things we need to know about transferring stable diffusion to visual dense prediction tasks,”</span> in <em>Computer vision – ECCV 2024</em>, Springer Nature Switzerland, 2025, pp. 128–145. Available: <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05837.pdf">https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05837.pdf</a></div>
</div>
<div id="ref-baranchuk2022labelefficient" class="csl-entry" role="listitem">
<div class="csl-left-margin">[36] </div><div class="csl-right-inline">D. Baranchuk, I. Rubachev, A. Voynov, V. Khrulkov, and A. Babenko, <span>“Label-efficient semantic segmentation with diffusion models.”</span> 2022. Available: <a href="https://arxiv.org/abs/2112.03126">https://arxiv.org/abs/2112.03126</a></div>
</div>
<div id="ref-ji2024diffusion" class="csl-entry" role="listitem">
<div class="csl-left-margin">[37] </div><div class="csl-right-inline">Y. Ji, B. He, C. Qu, Z. Tan, C. Qin, and L. Wu, <span>“Diffusion features to bridge domain gap for semantic segmentation.”</span> 2024. Available: <a href="https://arxiv.org/abs/2406.00777">https://arxiv.org/abs/2406.00777</a></div>
</div>
<div id="ref-couairon2024zeroshot" class="csl-entry" role="listitem">
<div class="csl-left-margin">[38] </div><div class="csl-right-inline">P. Couairon, M. Shukor, J.-E. Haugeard, M. Cord, and N. Thome, <span>“Zero-shot image segmentation via recursive normalized cut on diffusion features.”</span> 2024. Available: <a href="https://arxiv.org/abs/2406.02842">https://arxiv.org/abs/2406.02842</a></div>
</div>
<div id="ref-tian2024diffuse" class="csl-entry" role="listitem">
<div class="csl-left-margin">[39] </div><div class="csl-right-inline">J. Tian, L. Aggarwal, A. Colaco, Z. Kira, and M. Gonzalez-Franco, <span>“<span class="nocase">Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion</span>,”</span> in <em>2024 IEEE/CVF conference on computer vision and pattern recognition (CVPR)</em>, IEEE Computer Society, 2024, pp. 3554–3563. doi: <a href="https://doi.org/10.1109/CVPR52733.2024.00341">10.1109/CVPR52733.2024.00341</a>.</div>
</div>
<div id="ref-couairon2024diffcutcatalyzingzeroshotsemantic" class="csl-entry" role="listitem">
<div class="csl-left-margin">[40] </div><div class="csl-right-inline">P. Couairon, M. Shukor, J.-E. Haugeard, M. Cord, and N. Thome, <span>“DiffCut: Catalyzing zero-shot semantic segmentation with diffusion features and recursive normalized cut,”</span> in <em>The thirty-eighth annual conference on neural information processing systems</em>, 2024. Available: <a href="https://openreview.net/forum?id=N0xNf9Qqmc">https://openreview.net/forum?id=N0xNf9Qqmc</a></div>
</div>
<div id="ref-Yang2023Diffusion" class="csl-entry" role="listitem">
<div class="csl-left-margin">[41] </div><div class="csl-right-inline">X. Yang and X. Wang, <span>“Diffusion model as representation learner,”</span> in <em>2023 IEEE/CVF international conference on computer vision (ICCV)</em>, 2023, pp. 18892–18903. doi: <a href="https://doi.org/10.1109/ICCV51070.2023.01736">10.1109/ICCV51070.2023.01736</a>.</div>
</div>
<div id="ref-gupta2024pretrained" class="csl-entry" role="listitem">
<div class="csl-left-margin">[42] </div><div class="csl-right-inline">G. Gupta <em>et al.</em>, <span>“Pre-trained text-to-image diffusion models are versatile representation learners for control.”</span> 2024. Available: <a href="https://arxiv.org/abs/2405.05852">https://arxiv.org/abs/2405.05852</a></div>
</div>
<div id="ref-shridhar2024generativeimageactionmodels" class="csl-entry" role="listitem">
<div class="csl-left-margin">[43] </div><div class="csl-right-inline">M. Shridhar, Y. L. Lo, and S. James, <span>“Generative image as action models.”</span> 2024. Available: <a href="https://arxiv.org/abs/2407.07875">https://arxiv.org/abs/2407.07875</a></div>
</div>
<div id="ref-tsagkas2024clickgraspzeroshotprecise" class="csl-entry" role="listitem">
<div class="csl-left-margin">[44] </div><div class="csl-right-inline">N. Tsagkas, J. Rome, S. Ramamoorthy, O. M. Aodha, and C. X. Lu, <span>“Click to grasp: Zero-shot precise manipulation via visual diffusion descriptors.”</span> 2024. Available: <a href="https://arxiv.org/abs/2403.14526">https://arxiv.org/abs/2403.14526</a></div>
</div>
<div id="ref-ye2024stablenormalreducingdiffusionvariance" class="csl-entry" role="listitem">
<div class="csl-left-margin">[45] </div><div class="csl-right-inline">C. Ye <em>et al.</em>, <span>“StableNormal: Reducing diffusion variance for stable and sharp normal,”</span> <em>ACM Trans. Graph.</em>, vol. 43, 2024, doi: <a href="https://doi.org/10.1145/3687971">10.1145/3687971</a>.</div>
</div>
<div id="ref-de2024genziqa" class="csl-entry" role="listitem">
<div class="csl-left-margin">[46] </div><div class="csl-right-inline">D. De, S. Mitra, and R. Soundararajan, <span>“GenzIQA: Generalized image quality assessment using prompt-guided latent diffusion models.”</span> 2024. Available: <a href="https://arxiv.org/abs/2406.04654">https://arxiv.org/abs/2406.04654</a></div>
</div>
<div id="ref-park2023understanding" class="csl-entry" role="listitem">
<div class="csl-left-margin">[47] </div><div class="csl-right-inline">Y.-H. Park, M. Kwon, J. Choi, J. Jo, and Y. Uh, <span>“Understanding the latent space of diffusion models through the lens of riemannian geometry,”</span> <em>Advances in Neural Information Processing Systems</em>, vol. 36, pp. 24129–24142, 2023, Available: <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/4bfcebedf7a2967c410b64670f27f904-Paper-Conference.pdf">https://proceedings.neurips.cc/paper_files/paper/2023/file/4bfcebedf7a2967c410b64670f27f904-Paper-Conference.pdf</a></div>
</div>
<div id="ref-jaini2024intriguing" class="csl-entry" role="listitem">
<div class="csl-left-margin">[48] </div><div class="csl-right-inline">P. Jaini, K. Clark, and R. Geirhos, <span>“Intriguing properties of generative classifiers.”</span> 2024. Available: <a href="https://arxiv.org/abs/2309.16779">https://arxiv.org/abs/2309.16779</a></div>
</div>
<div id="ref-fuest2024diffusionmodelsrepresentationlearning" class="csl-entry" role="listitem">
<div class="csl-left-margin">[49] </div><div class="csl-right-inline">M. Fuest, P. Ma, M. Gui, J. S. Fischer, V. T. Hu, and B. Ommer, <span>“Diffusion models and representation learning: A survey.”</span> 2024. Available: <a href="https://arxiv.org/abs/2407.00783">https://arxiv.org/abs/2407.00783</a></div>
</div>
<div id="ref-goodfellow2020generative" class="csl-entry" role="listitem">
<div class="csl-left-margin">[50] </div><div class="csl-right-inline">I. Goodfellow <em>et al.</em>, <span>“Generative adversarial networks,”</span> <em>Commun. ACM</em>, vol. 63, pp. 139–144, 2020, doi: <a href="https://doi.org/10.1145/3422622">10.1145/3422622</a>.</div>
</div>
<div id="ref-sauer2023adversarial" class="csl-entry" role="listitem">
<div class="csl-left-margin">[51] </div><div class="csl-right-inline">A. Sauer, D. Lorenz, A. Blattmann, and R. Rombach, <span>“Adversarial diffusion distillation,”</span> in <em>Computer vision – ECCV 2024</em>, Springer Nature Switzerland, 2025, pp. 87–103. Available: <a href="https://link.springer.com/chapter/10.1007/978-3-031-73016-0_6">https://link.springer.com/chapter/10.1007/978-3-031-73016-0_6</a></div>
</div>
<div id="ref-lin2024sdxllightning" class="csl-entry" role="listitem">
<div class="csl-left-margin">[52] </div><div class="csl-right-inline">S. Lin, A. Wang, and X. Yang, <span>“SDXL-lightning: Progressive adversarial diffusion distillation.”</span> 2024. Available: <a href="https://arxiv.org/abs/2402.13929">https://arxiv.org/abs/2402.13929</a></div>
</div>
<div id="ref-ha2024organicdiffuseddistinguishhuman" class="csl-entry" role="listitem">
<div class="csl-left-margin">[53] </div><div class="csl-right-inline">A. Y. J. Ha <em>et al.</em>, <span>“Organic or diffused: Can we distinguish human art from AI-generated images?”</span> in <em>Proceedings of the 2024 on ACM SIGSAC conference on computer and communications security</em>, in CCS ’24. Association for Computing Machinery, 2024, pp. 4822–4836. doi: <a href="https://doi.org/10.1145/3658644.3670306">10.1145/3658644.3670306</a>.</div>
</div>
<div id="ref-lipman2023flow" class="csl-entry" role="listitem">
<div class="csl-left-margin">[54] </div><div class="csl-right-inline">Y. Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le, <span>“Flow matching for generative modeling,”</span> in <em>The eleventh international conference on learning representations</em>, 2023. Available: <a href="https://openreview.net/forum?id=PqvMRDCJT9t">https://openreview.net/forum?id=PqvMRDCJT9t</a></div>
</div>
<div id="ref-peebles2023scalable" class="csl-entry" role="listitem">
<div class="csl-left-margin">[55] </div><div class="csl-right-inline">W. Peebles and S. Xie, <span>“Scalable diffusion models with transformers,”</span> in <em>2023 IEEE/CVF international conference on computer vision (ICCV)</em>, 2023, pp. 4172–4182. doi: <a href="https://doi.org/10.1109/ICCV51070.2023.00387">10.1109/ICCV51070.2023.00387</a>.</div>
</div>
<div id="ref-esser2024scaling" class="csl-entry" role="listitem">
<div class="csl-left-margin">[56] </div><div class="csl-right-inline">P. Esser <em>et al.</em>, <span>“Scaling rectified flow transformers for high-resolution image synthesis,”</span> in <em>Forty-first international conference on machine learning</em>, 2024. Available: <a href="https://openreview.net/forum?id=FPnUhsQJ5B">https://openreview.net/forum?id=FPnUhsQJ5B</a></div>
</div>
<div id="ref-vonplaten2022diffusers" class="csl-entry" role="listitem">
<div class="csl-left-margin">[57] </div><div class="csl-right-inline">P. von Platen <em>et al.</em>, <span>“Diffusers: State-of-the-art diffusion models.”</span> GitHub, 2022. Available: <a href="https://github.com/huggingface/diffusers">https://github.com/huggingface/diffusers</a></div>
</div>
<div id="ref-hfsd" class="csl-entry" role="listitem">
<div class="csl-left-margin">[58] </div><div class="csl-right-inline">Hugging-Face, <span>“Stable diffusion pipelines.”</span> 2024. Available: <a href="https://huggingface.co/docs/diffusers/en/api/pipelines/stable_diffusion/overview">https://huggingface.co/docs/diffusers/en/api/pipelines/stable_diffusion/overview</a></div>
</div>
<div id="ref-stabilityai2024sd35" class="csl-entry" role="listitem">
<div class="csl-left-margin">[59] </div><div class="csl-right-inline">Stability AI, <span>“Introducing stable diffusion 3.5.”</span> 2024. Available: <a href="https://stability.ai/news/introducing-stable-diffusion-3-5">https://stability.ai/news/introducing-stable-diffusion-3-5</a></div>
</div>
<div id="ref-blackforestlabs2024fluxRepo" class="csl-entry" role="listitem">
<div class="csl-left-margin">[60] </div><div class="csl-right-inline">Black Forest Labs, <span>“Flux.”</span> GitHub, 2024. Available: <a href="https://github.com/black-forest-labs/flux">https://github.com/black-forest-labs/flux</a></div>
</div>
<div id="ref-yu2024representationalignmentgenerationtraining" class="csl-entry" role="listitem">
<div class="csl-left-margin">[61] </div><div class="csl-right-inline">S. Yu <em>et al.</em>, <span>“Representation alignment for generation: Training diffusion transformers is easier than you think.”</span> 2024. Available: <a href="https://arxiv.org/abs/2410.06940">https://arxiv.org/abs/2410.06940</a></div>
</div>
<div id="ref-he2022maskedautoencoders" class="csl-entry" role="listitem">
<div class="csl-left-margin">[62] </div><div class="csl-right-inline">K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, <span>“Masked autoencoders are scalable vision learners,”</span> in <em>2022 IEEE/CVF conference on computer vision and pattern recognition (CVPR)</em>, 2022, pp. 15979–15988. doi: <a href="https://doi.org/10.1109/CVPR52688.2022.01553">10.1109/CVPR52688.2022.01553</a>.</div>
</div>
<div id="ref-gidaris2018unsupervisedrepresentationlearningpredicting" class="csl-entry" role="listitem">
<div class="csl-left-margin">[63] </div><div class="csl-right-inline">S. Gidaris, P. Singh, and N. Komodakis, <span>“Unsupervised representation learning by predicting image rotations.”</span> 2018. Available: <a href="https://arxiv.org/abs/1803.07728">https://arxiv.org/abs/1803.07728</a></div>
</div>
<div id="ref-misra2020selfsupervised" class="csl-entry" role="listitem">
<div class="csl-left-margin">[64] </div><div class="csl-right-inline">I. Misra and L. van der Maaten, <span>“Self-supervised learning of pretext-invariant representations,”</span> in <em>2020 IEEE/CVF conference on computer vision and pattern recognition (CVPR)</em>, 2020, pp. 6706–6716. doi: <a href="https://doi.org/10.1109/CVPR42600.2020.00674">10.1109/CVPR42600.2020.00674</a>.</div>
</div>
<div id="ref-noroozi2016unsupervised" class="csl-entry" role="listitem">
<div class="csl-left-margin">[65] </div><div class="csl-right-inline">M. Noroozi and P. Favaro, <span>“Unsupervised learning of visual representations by solving jigsaw puzzles,”</span> in <em>Computer vision – ECCV 2016</em>, Springer International Publishing, 2016, pp. 69–84. Available: <a href="https://link.springer.com/chapter/10.1007/978-3-319-46466-4_5">https://link.springer.com/chapter/10.1007/978-3-319-46466-4_5</a></div>
</div>
<div id="ref-doersch2015unsupervised" class="csl-entry" role="listitem">
<div class="csl-left-margin">[66] </div><div class="csl-right-inline">C. Doersch, A. Gupta, and A. A. Efros, <span>“Unsupervised visual representation learning by context prediction,”</span> in <em>2015 IEEE international conference on computer vision (ICCV)</em>, 2015, pp. 1422–1430. doi: <a href="https://doi.org/10.1109/ICCV.2015.167">10.1109/ICCV.2015.167</a>.</div>
</div>
<div id="ref-caron2021emerging" class="csl-entry" role="listitem">
<div class="csl-left-margin">[67] </div><div class="csl-right-inline">M. Caron <em>et al.</em>, <span>“Emerging properties in self-supervised vision transformers,”</span> in <em>2021 IEEE/CVF international conference on computer vision (ICCV)</em>, 2021, pp. 9630–9640. doi: <a href="https://doi.org/10.1109/ICCV48922.2021.00951">10.1109/ICCV48922.2021.00951</a>.</div>
</div>
<div id="ref-chen2021exploring" class="csl-entry" role="listitem">
<div class="csl-left-margin">[68] </div><div class="csl-right-inline">X. Chen and K. He, <span>“Exploring simple siamese representation learning,”</span> in <em>2021 IEEE/CVF conference on computer vision and pattern recognition (CVPR)</em>, 2021, pp. 15745–15753. doi: <a href="https://doi.org/10.1109/CVPR46437.2021.01549">10.1109/CVPR46437.2021.01549</a>.</div>
</div>
<div id="ref-chen2016infogan" class="csl-entry" role="listitem">
<div class="csl-left-margin">[69] </div><div class="csl-right-inline">X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel, <span>“InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets,”</span> in <em>Advances in neural information processing systems</em>, Curran Associates, Inc., 2016. Available: <a href="https://proceedings.neurips.cc/paper_files/paper/2016/file/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2016/file/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Paper.pdf</a></div>
</div>
<div id="ref-darcet2024vision" class="csl-entry" role="listitem">
<div class="csl-left-margin">[70] </div><div class="csl-right-inline">T. Darcet, M. Oquab, J. Mairal, and P. Bojanowski, <span>“Vision transformers need registers,”</span> in <em>The twelfth international conference on learning representations</em>, 2024. Available: <a href="https://openreview.net/forum?id=2dnO3LLiJ1">https://openreview.net/forum?id=2dnO3LLiJ1</a></div>
</div>
<div id="ref-chen2024deconstructingdenoisingdiffusionmodels" class="csl-entry" role="listitem">
<div class="csl-left-margin">[71] </div><div class="csl-right-inline">X. Chen, Z. Liu, S. Xie, and K. He, <span>“Deconstructing denoising diffusion models for self-supervised learning.”</span> 2024. Available: <a href="https://arxiv.org/abs/2401.14404">https://arxiv.org/abs/2401.14404</a></div>
</div>
<div id="ref-linhardt2024analysis" class="csl-entry" role="listitem">
<div class="csl-left-margin">[72] </div><div class="csl-right-inline">L. Linhardt, M. Morik, S. Bender, and N. E. Borras, <span>“An analysis of human alignment of latent diffusion models.”</span> 2024. Available: <a href="https://arxiv.org/abs/2403.08469">https://arxiv.org/abs/2403.08469</a></div>
</div>
<div id="ref-wang2024diffusionmodels3dvision" class="csl-entry" role="listitem">
<div class="csl-left-margin">[73] </div><div class="csl-right-inline">Z. Wang, D. Li, and R. Jiang, <span>“Diffusion models in 3D vision: A survey.”</span> 2024. Available: <a href="https://arxiv.org/abs/2410.04738">https://arxiv.org/abs/2410.04738</a></div>
</div>
<div id="ref-kwon2023diffusion" class="csl-entry" role="listitem">
<div class="csl-left-margin">[74] </div><div class="csl-right-inline">M. Kwon, J. Jeong, and Y. Uh, <span>“Diffusion models already have a semantic latent space.”</span> 2023. Available: <a href="https://arxiv.org/abs/2210.10960">https://arxiv.org/abs/2210.10960</a></div>
</div>
<div id="ref-park2023unsupervised" class="csl-entry" role="listitem">
<div class="csl-left-margin">[75] </div><div class="csl-right-inline">Y.-H. Park, M. Kwon, J. Jo, and Y. Uh, <span>“Unsupervised discovery of semantic latent directions in diffusion models.”</span> 2023. Available: <a href="https://arxiv.org/abs/2302.12469">https://arxiv.org/abs/2302.12469</a></div>
</div>
<div id="ref-jeong2024trainingfree" class="csl-entry" role="listitem">
<div class="csl-left-margin">[76] </div><div class="csl-right-inline">J. Jeong, M. Kwon, and Y. Uh, <span>“Training-free content injection using h-space in diffusion models,”</span> in <em>Proceedings of the IEEE/CVF winter conference on applications of computer vision</em>, 2024, pp. 5151–5161. doi: <a href="https://doi.org/10.1109/WACV57701.2024.00507">10.1109/WACV57701.2024.00507</a>.</div>
</div>
<div id="ref-haas2023discovering" class="csl-entry" role="listitem">
<div class="csl-left-margin">[77] </div><div class="csl-right-inline">R. Haas, I. Huberman-Spiegelglas, R. Mulayoff, S. Graßhof, S. S. Brandt, and T. Michaeli, <span>“Discovering interpretable directions in the semantic latent space of diffusion models,”</span> in <em>2024 IEEE 18th international conference on automatic face and gesture recognition (FG)</em>, 2024, pp. 1–9. doi: <a href="https://doi.org/10.1109/FG59268.2024.10581912">10.1109/FG59268.2024.10581912</a>.</div>
</div>
<div id="ref-gambashidze2024aligningdiffusionmodelsnoiseconditioned" class="csl-entry" role="listitem">
<div class="csl-left-margin">[78] </div><div class="csl-right-inline">A. Gambashidze, A. Kulikov, Y. Sosnin, and I. Makarov, <span>“Aligning diffusion models with noise-conditioned perception.”</span> 2024. Available: <a href="https://arxiv.org/abs/2406.17636">https://arxiv.org/abs/2406.17636</a></div>
</div>
<div id="ref-Man2024Lexicon3DPV" class="csl-entry" role="listitem">
<div class="csl-left-margin">[79] </div><div class="csl-right-inline">Y. Man, S. Zheng, Z. Bao, M. Hebert, L. Gui, and Y.-X. Wang, <span>“Lexicon3D: Probing visual foundation models for complex 3D scene understanding.”</span> 2024. Available: <a href="https://arxiv.org/abs/2409.03757">https://arxiv.org/abs/2409.03757</a></div>
</div>
<div id="ref-Ke2024RepurposingDI" class="csl-entry" role="listitem">
<div class="csl-left-margin">[80] </div><div class="csl-right-inline">B. Ke, A. Obukhov, S. Huang, N. Metzger, R. C. Daudt, and K. Schindler, <span>“Repurposing diffusion-based image generators for monocular depth estimation,”</span> in <em>2024 IEEE/CVF conference on computer vision and pattern recognition (CVPR)</em>, 2024, pp. 9492–9502. doi: <a href="https://doi.org/10.1109/CVPR52733.2024.00907">10.1109/CVPR52733.2024.00907</a>.</div>
</div>
<div id="ref-Lee2024ExploitingDI" class="csl-entry" role="listitem">
<div class="csl-left-margin">[81] </div><div class="csl-right-inline">H.-Y. Lee, H.-Y. Tseng, H.-Y. Lee, and M.-H. Yang, <span>“Exploiting diffusion prior for generalizable dense prediction,”</span> in <em>2024 IEEE/CVF conference on computer vision and pattern recognition (CVPR)</em>, 2024, pp. 7861–7871. doi: <a href="https://doi.org/10.1109/CVPR52733.2024.00751">10.1109/CVPR52733.2024.00751</a>.</div>
</div>
<div id="ref-xu2024diffusionmodelstrainedlarge" class="csl-entry" role="listitem">
<div class="csl-left-margin">[82] </div><div class="csl-right-inline">G. Xu <em>et al.</em>, <span>“Diffusion models trained with large data are transferable visual models.”</span> 2024. Available: <a href="https://arxiv.org/abs/2403.06090">https://arxiv.org/abs/2403.06090</a></div>
</div>
<div id="ref-Krizhevsky2009LearningML" class="csl-entry" role="listitem">
<div class="csl-left-margin">[83] </div><div class="csl-right-inline">A. Krizhevsky, <span>“Learning multiple layers of features from tiny images,”</span> Technical Report, 2009. Available: <a href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf">https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf</a></div>
</div>
<div id="ref-ILSVRC15" class="csl-entry" role="listitem">
<div class="csl-left-margin">[84] </div><div class="csl-right-inline">O. Russakovsky <em>et al.</em>, <span>“<span>ImageNet Large Scale Visual Recognition Challenge</span>,”</span> <em>International Journal of Computer Vision (IJCV)</em>, vol. 115, pp. 211–252, 2015, doi: <a href="https://doi.org/10.1007/s11263-015-0816-y">10.1007/s11263-015-0816-y</a>.</div>
</div>
<div id="ref-He2024LotusDV" class="csl-entry" role="listitem">
<div class="csl-left-margin">[85] </div><div class="csl-right-inline">J. He <em>et al.</em>, <span>“Lotus: Diffusion-based visual foundation model for high-quality dense prediction,”</span> 2024. Available: <a href="https://arxiv.org/abs/2409.18124">https://arxiv.org/abs/2409.18124</a></div>
</div>
<div id="ref-fu2024geowizard" class="csl-entry" role="listitem">
<div class="csl-left-margin">[86] </div><div class="csl-right-inline">X. Fu <em>et al.</em>, <span>“GeoWizard: Unleashing the diffusion priors for 3D geometry estimation from a single image,”</span> in <em>Computer vision – ECCV 2024</em>, Springer Nature Switzerland, 2025, pp. 241–258. Available: <a href="https://link.springer.com/chapter/10.1007/978-3-031-72670-5_14">https://link.springer.com/chapter/10.1007/978-3-031-72670-5_14</a></div>
</div>
<div id="ref-zhang2024betterdepthplugandplaydiffusionrefiner" class="csl-entry" role="listitem">
<div class="csl-left-margin">[87] </div><div class="csl-right-inline">X. Zhang <em>et al.</em>, <span>“BetterDepth: Plug-and-play diffusion refiner for zero-shot monocular depth estimation.”</span> 2024. Available: <a href="https://arxiv.org/abs/2407.17952">https://arxiv.org/abs/2407.17952</a></div>
</div>
<div id="ref-zhang2024atlantis" class="csl-entry" role="listitem">
<div class="csl-left-margin">[88] </div><div class="csl-right-inline">F. Zhang, S. You, Y. Li, and Y. Fu, <span>“Atlantis: Enabling underwater depth estimation with stable diffusion,”</span> in <em>2024 IEEE/CVF conference on computer vision and pattern recognition (CVPR)</em>, 2024, pp. 11852–11861. doi: <a href="https://doi.org/10.1109/CVPR52733.2024.01126">10.1109/CVPR52733.2024.01126</a>.</div>
</div>
<div id="ref-Silberman2012Indoor" class="csl-entry" role="listitem">
<div class="csl-left-margin">[89] </div><div class="csl-right-inline">N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, <span>“Indoor segmentation and support inference from RGBD images,”</span> in <em>Computer vision – ECCV 2012</em>, Springer Berlin Heidelberg, 2012, pp. 746–760. Available: <a href="https://link.springer.com/chapter/10.1007/978-3-642-33715-4_54">https://link.springer.com/chapter/10.1007/978-3-642-33715-4_54</a></div>
</div>
<div id="ref-min2019spair71k" class="csl-entry" role="listitem">
<div class="csl-left-margin">[90] </div><div class="csl-right-inline">J. Min, J. Lee, J. Ponce, and M. Cho, <span>“SPair-71k: A large-scale benchmark for semantic correspondence.”</span> 2019. Available: <a href="https://arxiv.org/abs/1908.10543">https://arxiv.org/abs/1908.10543</a></div>
</div>
<div id="ref-amir2022deepvitfeaturesdense" class="csl-entry" role="listitem">
<div class="csl-left-margin">[91] </div><div class="csl-right-inline">S. Amir, Y. Gandelsman, S. Bagon, and T. Dekel, <span>“Deep ViT features as dense visual descriptors.”</span> 2022. Available: <a href="https://arxiv.org/abs/2112.05814">https://arxiv.org/abs/2112.05814</a></div>
</div>
<div id="ref-muttenthaler2023human" class="csl-entry" role="listitem">
<div class="csl-left-margin">[92] </div><div class="csl-right-inline">L. Muttenthaler, J. Dippel, L. Linhardt, R. A. Vandermeulen, and S. Kornblith, <span>“Human alignment of neural network representations.”</span> 2023. Available: <a href="https://arxiv.org/abs/2211.01201">https://arxiv.org/abs/2211.01201</a></div>
</div>
<div id="ref-kingma2017adam" class="csl-entry" role="listitem">
<div class="csl-left-margin">[93] </div><div class="csl-right-inline">D. P. Kingma and J. Ba, <span>“Adam: A method for stochastic optimization.”</span> 2017. Available: <a href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a></div>
</div>
<div id="ref-zhang2025difftracker" class="csl-entry" role="listitem">
<div class="csl-left-margin">[94] </div><div class="csl-right-inline">Z. Zhang, L. Xu, D. Peng, H. Rahmani, and J. Liu, <span>“Diff-tracker: Text-to-image diffusion models are unsupervised trackers,”</span> in <em>Computer vision – ECCV 2024</em>, Springer Nature Switzerland, 2025, pp. 319–337. Available: <a href="https://link.springer.com/chapter/10.1007/978-3-031-73390-1_19">https://link.springer.com/chapter/10.1007/978-3-031-73390-1_19</a></div>
</div>
<div id="ref-pytorch2023huberloss" class="csl-entry" role="listitem">
<div class="csl-left-margin">[95] </div><div class="csl-right-inline">PyTorch Contributors, <span>“HuberLoss.”</span> PyTorch Docs, 2023. Available: <a href="https://pytorch.org/docs/2.5/generated/torch.nn.HuberLoss.html">https://pytorch.org/docs/2.5/generated/torch.nn.HuberLoss.html</a></div>
</div>
<div id="ref-dosovitskiy2021an" class="csl-entry" role="listitem">
<div class="csl-left-margin">[96] </div><div class="csl-right-inline">A. Dosovitskiy <em>et al.</em>, <span>“An image is worth 16x16 words: Transformers for image recognition at scale,”</span> in <em>International conference on learning representations</em>, 2021. Available: <a href="https://openreview.net/forum?id=YicbFdNTTy">https://openreview.net/forum?id=YicbFdNTTy</a></div>
</div>
<div id="ref-xu2023open" class="csl-entry" role="listitem">
<div class="csl-left-margin">[97] </div><div class="csl-right-inline">J. Xu, S. Liu, A. Vahdat, W. Byeon, X. Wang, and S. De Mello, <span>“Open-vocabulary panoptic segmentation with text-to-image diffusion models,”</span> in <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)</em>, 2023, pp. 2955–2966. Available: <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Open-Vocabulary_Panoptic_Segmentation_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf">https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Open-Vocabulary_Panoptic_Segmentation_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf</a></div>
</div>
<div id="ref-CompVis2022StableDiffusionRepo" class="csl-entry" role="listitem">
<div class="csl-left-margin">[98] </div><div class="csl-right-inline">CompVis, <span>“Stable diffusion.”</span> GitHub, 2022. Available: <a href="https://github.com/CompVis/stable-diffusion">https://github.com/CompVis/stable-diffusion</a></div>
</div>
<div id="ref-fofr2024styletransfer" class="csl-entry" role="listitem">
<div class="csl-left-margin">[99] </div><div class="csl-right-inline">fofr, <span>“Style-transfer.”</span> Replicate, 2024. Available: <a href="https://replicate.com/fofr/style-transfer">https://replicate.com/fofr/style-transfer</a></div>
</div>
<div id="ref-steck2024cosinesimilarity" class="csl-entry" role="listitem">
<div class="csl-left-margin">[100] </div><div class="csl-right-inline">H. Steck, C. Ekanadham, and N. Kallus, <span>“Is cosine-similarity of embeddings really about similarity?”</span> in <em>Companion proceedings of the ACM web conference 2024</em>, in WWW ’24. Association for Computing Machinery, 2024, pp. 887–890. doi: <a href="https://doi.org/10.1145/3589335.3651526">10.1145/3589335.3651526</a>.</div>
</div>
<div id="ref-sauer2024fasthighresolutionimagesynthesis" class="csl-entry" role="listitem">
<div class="csl-left-margin">[101] </div><div class="csl-right-inline">A. Sauer, F. Boesel, T. Dockhorn, A. Blattmann, P. Esser, and R. Rombach, <span>“Fast high-resolution image synthesis with latent adversarial diffusion distillation,”</span> in <em>SIGGRAPH asia 2024 conference papers</em>, in SA ’24. Association for Computing Machinery, 2024. doi: <a href="https://doi.org/10.1145/3680528.3687625">10.1145/3680528.3687625</a>.</div>
</div>
<div id="ref-StabilityAI2022StableDiffusionRepo" class="csl-entry" role="listitem">
<div class="csl-left-margin">[102] </div><div class="csl-right-inline">Stability AI, <span>“Stable diffusion.”</span> GitHub, 2022. Available: <a href="https://github.com/Stability-AI/stablediffusion">https://github.com/Stability-AI/stablediffusion</a></div>
</div>
<div id="ref-salimans2022progressivedistillationfastsampling" class="csl-entry" role="listitem">
<div class="csl-left-margin">[103] </div><div class="csl-right-inline">T. Salimans and J. Ho, <span>“Progressive distillation for fast sampling of diffusion models.”</span> 2022. Available: <a href="https://arxiv.org/abs/2202.00512">https://arxiv.org/abs/2202.00512</a></div>
</div>
</div>
<p> </p>
</section>
<section id="sec-appendix" class="level1">
<h1>Appendix</h1>
<section id="sec-appendix-experimental-setup" class="level2">
<h2 class="anchored" data-anchor-id="sec-appendix-experimental-setup">Experimental Setup</h2>
<p>For all experiments, we use SD-1.5 as default model, except when stated otherwise. The default time step for representation extraction is 50 out of 1000 steps (see <a href="#sec-methods-representation-extraction" class="quarto-xref">Section&nbsp;2.2</a>). For semantic and dense correspondence, we use cosine similarity as default measure for determining the best match. We always use our <code>sdhelper</code> library (see <a href="#sec-sdhelper" class="quarto-xref">Section&nbsp;3.1</a>) for extracting the representations, which uses the Hugging Face <code>diffusers</code> <span class="citation" data-cites="vonplaten2022diffusers">[<a href="#ref-vonplaten2022diffusers" role="doc-biblioref">57</a>]</span> library and the model weights available on the Hugging Face model hub. We conducted most experiments on a single Nvidia RTX 3090, however, more demanding experiments were run on the Hydra cluster of the TU Berlin<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a>.</p>
<section id="public-code" class="level3">
<h3 class="anchored" data-anchor-id="public-code">Public Code</h3>
<p>We publish the code for some of our experiments, tools, and scripts as open-source software:</p>
<ul>
<li>Thesis document and experiments: <a href="https://github.com/JonasLoos/thesis">github.com/JonasLoos/thesis</a></li>
<li>Representation Similarity Explorer: <a href="https://github.com/JonasLoos/sd_representation_similarity_explorer">github.com/JonasLoos/sd_representation_similarity_explorer</a></li>
<li><code>sdhelper</code> library: <a href="https://github.com/JonasLoos/sdhelper">github.com/JonasLoos/sdhelper</a></li>
<li><a href="#sec-datasets-spair"> SPair-71k</a> dataset loading script: <a href="https://huggingface.co/datasets/0jl/SPair-71k">huggingface.co/datasets/0jl/SPair-71k</a></li>
<li><a href="#sec-datasets-nyu-v2"> NYU Depth v2</a> dataset loading script: <a href="https://huggingface.co/datasets/0jl/NYUv2">huggingface.co/datasets/0jl/NYUv2</a></li>
</ul>
</section>
<section id="experiment-parameters-for-reproducibility" class="level3">
<h3 class="anchored" data-anchor-id="experiment-parameters-for-reproducibility">Experiment Parameters for Reproducibility</h3>
<p>To facilitate reproducibility of our results, we provide additional details on the experimental setup.</p>
<p>For representation extraction in our experiments, we used SD-1.5 and time step 50, except if stated otherwise.</p>
<ul>
<li><a href="#tbl-repr-shapes" class="quarto-xref">Table&nbsp;1</a>: data: images of the specified default sizes;</li>
<li><a href="#fig-noise-levels" class="quarto-xref">Figure&nbsp;2</a>: data: one manually chosen image of size 512 that is generated with Flux.1-schnell; the VAE of SD-1.5 is used; linear interpolation between the original image and standard Gaussian noise;</li>
<li><a href="#fig-similarities-example" class="quarto-xref">Figure&nbsp;4</a>: data: three manually chosen images of size 512 generated with Flux.1-schnell;</li>
<li><a href="#fig-kmeans-norms-pca" class="quarto-xref">Figure&nbsp;6</a>: data: one manually chosen image of size 512 generated with Flux.1-schnell;</li>
<li><a href="#tbl-lpc" class="quarto-xref">Table&nbsp;2</a>: data: <a href="#sec-datasets-cifar"> CIFAR</a>-10, <a href="#sec-datasets-cifar"> CIFAR</a>-100, and Tiny-<a href="#sec-datasets-imagenet"> Imagenet</a>; linear probe training: 80 epochs, batch size 128, data shuffle, optimizer Adam <span class="citation" data-cites="kingma2017adam">[<a href="#ref-kingma2017adam" role="doc-biblioref">93</a>]</span>, learning rate <span class="math inline">\(10^{-3}\)</span>;</li>
<li><a href="#fig-sc-pck-over-blocks-noise" class="quarto-xref">Figure&nbsp;7</a>: data: <a href="#sec-datasets-spair"> SPair-71k</a> (test split - 88328 correspondences); input images are scaled so that the larger side is 512px; time steps: <span class="math inline">\([0, 10, 25, 50, 75, 100, 150, 200, 300, 500, 800]\)</span>;</li>
<li><a href="#fig-position-similarities-empty-image" class="quarto-xref">Figure&nbsp;10</a>: data: 100 empty (black) images of size 512; block: <code>mid</code>;</li>
<li><a href="#fig-position-classifier-accuracy" class="quarto-xref">Figure&nbsp;11</a>: data: <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a> (500 images, 400 train, 100 test); Batch size: 512; 5 epochs; optimizer Adam<span class="citation" data-cites="kingma2017adam">[<a href="#ref-kingma2017adam" role="doc-biblioref">93</a>]</span>; learning rate <span class="math inline">\(10^{-3}\)</span>; loss: cross-entropy for classification, MSE for regression; samples were randomly permutated each epoch;</li>
<li><a href="#fig-pos-embedding-aspect-ratios" class="quarto-xref">Figure&nbsp;12</a>: data: One empty (black) image per resolution and block;</li>
<li><a href="#fig-dense-correspondence-flip" class="quarto-xref">Figure&nbsp;13</a>: data: two manually chosen images of size 512 that are generated with Flux.1-schnell; The transferred image tiles in the “(mapped) image” columns are individually flipped, so that a perfect correspondence results in the flipped image;</li>
<li><a href="#fig-sc-errors-by-relative-position" class="quarto-xref">Figure&nbsp;14</a>: data: custom synthetic dataset with 11623140 correspondences over 2048 images of size 512 (see <a href="#sec-position-bias" class="quarto-xref">Section&nbsp;5.1</a>);</li>
<li><a href="#fig-texture-color-bias-examples" class="quarto-xref">Figure&nbsp;15</a>: data: four manually chosen images of size 512 that are generated with Flux.1-schnell;</li>
<li><a href="#fig-color-bias-rgb-bgr-dc" class="quarto-xref">Figure&nbsp;16</a>: data: <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a>; results are averaged over the 500 images; for representation extraction, we used different seeds for the source and target images; number of interpolation steps: 10;</li>
<li><a href="#fig-texture-bias-texture-overlay-dc" class="quarto-xref">Figure&nbsp;17</a>: same as <a href="#fig-texture-bias-texture-overlay-dc" class="quarto-xref">Figure&nbsp;17</a>, except for the following; overlay texture is generated with SD-1.5 and the same for all images;</li>
<li><a href="#fig-texture-color-bias-sc" class="quarto-xref">Figure&nbsp;18</a>: data: <a href="#sec-datasets-spair"> SPair-71k</a> (test split - 88328 correspondences); input images are scaled so that the larger side is 512px;</li>
<li><a href="#fig-cosine-similarity-corner" class="quarto-xref">Figure&nbsp;19</a>: data: three manually chosen images of size 512 that are generated with Flux.1-schnell; block: <code>mid</code>;</li>
<li><a href="#fig-border-corner-similarity" class="quarto-xref">Figure&nbsp;20</a>: data: <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a>; results are averaged over the 500 images;</li>
<li><a href="#fig-high-norm-anomalies-1" class="quarto-xref">Figure&nbsp;21</a>: data: one manually chosen image of size 512 that is generated with Flux.1-schnell; L2 norm along the channel dimension;</li>
<li><a href="#fig-high-norm-anomalies-2" class="quarto-xref">Figure&nbsp;22</a>: data: one manually chosen image of size 512 that is generated with Flux.1-schnell; L2 norm along the channel dimension;</li>
<li><a href="#fig-high-norm-anomalies-searching" class="quarto-xref">Figure&nbsp;23</a>: data: <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a> and <a href="#sec-datasets-nyu-v2"> NYU Depth v2</a>; anomalies were labeled manually by a human annotator, always with size 2 tokens; We used the same noise seed during data labeling and anomaly searching;</li>
<li><a href="#fig-high-norm-anomalies-over-layers" class="quarto-xref">Figure&nbsp;24</a>: data: <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a>; anomaly labeling as in <a href="#fig-high-norm-anomalies-searching" class="quarto-xref">Figure&nbsp;23</a>; L2 norm along the channel dimension; results are averaged over the 500 images;</li>
<li><a href="#fig-depth-estimation-performance-over-layers" class="quarto-xref">Figure&nbsp;25</a>: data: <a href="#sec-datasets-nyu-v2"> NYU Depth v2</a> (training on train split, evaluation on test split); linear probe training: 10000 steps with random batches, optimizer Adam <span class="citation" data-cites="kingma2017adam">[<a href="#ref-kingma2017adam" role="doc-biblioref">93</a>]</span>, learning rate <span class="math inline">\(10^{-3}\)</span>, batch size 64, Huber loss <span class="citation" data-cites="pytorch2023huberloss">[<a href="#ref-pytorch2023huberloss" role="doc-biblioref">95</a>]</span> (following <span class="citation" data-cites="Chen2023BeyondSS">[<a href="#ref-Chen2023BeyondSS" role="doc-biblioref">32</a>]</span>, with <span class="math inline">\(\delta = 1\)</span>) (i.e.&nbsp;different loss functions for training and evaluation);</li>
<li><a href="#fig-sd3-example" class="quarto-xref">Figure&nbsp;26</a>: data: two manually chosen images of size 1024 that are generated with Flux.1-schnell; L2 norm along the channel dimension;</li>
<li><a href="#fig-appendix-position-classifier-example" class="quarto-xref">Figure&nbsp;27</a>: data: <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a> (500 images, 400 train, 100 test); linear probe training: 5 epochs, batch size 512, optimizer Adam <span class="citation" data-cites="kingma2017adam">[<a href="#ref-kingma2017adam" role="doc-biblioref">93</a>]</span>, learning rate <span class="math inline">\(10^{-3}\)</span>; samples were randomly permutated each epoch;</li>
<li><a href="#fig-appendix-dense-correspondence-flip-all-blocks" class="quarto-xref">Figure&nbsp;28</a>: data: two manually chosen images of size 512 that are generated with Flux.1-schnell; The transferred image tiles in the “(mapped) image” columns are individually flipped, so that a perfect correspondence results in the flipped image; the error is calculated as the percentile of possible error distances, so that the error distribution for a random estimator is independent of position;</li>
<li><a href="#fig-appendix-sc-errors-by-relative-position-full" class="quarto-xref">Figure&nbsp;29</a>: same as <a href="#fig-sc-errors-by-relative-position" class="quarto-xref">Figure&nbsp;14</a>;</li>
<li><a href="#fig-appendix-sc-errors-by-relative-position-spair-maps" class="quarto-xref">Figure&nbsp;30</a>: data: <a href="#sec-datasets-spair"> SPair-71k</a> (test split - 88328 correspondences); input images are scaled so that the larger side is 512px; error rate is averaged over 32 pixel tiles; Only the relative positions <span class="math inline">\([-255, ..., 256]^2 \subset [-511, ..., 512]^2\)</span> are shown;</li>
<li><a href="#fig-appendix-sc-errors-by-relative-position-spair-lines" class="quarto-xref">Figure&nbsp;31</a>: data: <a href="#sec-datasets-spair"> SPair-71k</a> (test split - 88328 correspondences); input images are scaled so that the larger side is 512px; error rate is averaged over (smoothed) bins with quadratically increasing width (to counteract the decreased sample density for larger distances);</li>
<li><a href="#fig-appendix-texture-color-bias-examples-1" class="quarto-xref">Figure&nbsp;32</a>: same as <a href="#fig-texture-color-bias-examples" class="quarto-xref">Figure&nbsp;15</a></li>
<li><a href="#fig-appendix-texture-color-bias-examples-2" class="quarto-xref">Figure&nbsp;33</a>: same as <a href="#fig-texture-color-bias-examples" class="quarto-xref">Figure&nbsp;15</a></li>
<li><a href="#fig-appendix-texture-bias-dense-correspondence-blur" class="quarto-xref">Figure&nbsp;34</a>: same as <a href="#fig-texture-bias-texture-overlay-dc" class="quarto-xref">Figure&nbsp;17</a>, except for the following; the interpolation steps correspond to Gaussian blurring with a kernel size between 0 and 16px;</li>
<li><a href="#fig-appendix-texture-bias-dense-correspondence-noise" class="quarto-xref">Figure&nbsp;35</a>: same as <a href="#fig-texture-bias-texture-overlay-dc" class="quarto-xref">Figure&nbsp;17</a>, except for the following; the interpolation steps are between the original image and uniformly random noise, where interpolation step 1 corresponds to half image and half noise;</li>
<li><a href="#fig-appendix-texture-color-bias-dense-correspondence-with-offset" class="quarto-xref">Figure&nbsp;36</a>: same as <a href="#fig-color-bias-rgb-bgr-dc" class="quarto-xref">Figure&nbsp;16</a> and <a href="#fig-texture-bias-texture-overlay-dc" class="quarto-xref">Figure&nbsp;17</a>, except for the following; The input images are upscaled from 512 to 768, and the source image is cropped from (0, 0) to (512, 512), while the target image is cropped from (256, 256) to (768, 768); The dense correspondence is calculated only for the overlapping region;</li>
<li><a href="#fig-appendix-border-corner-similarity" class="quarto-xref">Figure&nbsp;37</a>: same as <a href="#fig-border-corner-similarity" class="quarto-xref">Figure&nbsp;20</a>;</li>
<li><a href="#fig-appendix-spatial-norm" class="quarto-xref">Figure&nbsp;38</a>: data: <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a>;</li>
<li><a href="#fig-appendix-histogram-norm-similarity" class="quarto-xref">Figure&nbsp;39</a>: data: <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a>; Due to the large number of tokens per representation for <code>conv-in</code>, <code>down[0]</code>, <code>up[1]</code>, <code>up[2]</code>, <code>up[3]</code>, and <code>conv-out</code>, only a random subset of the pairs is shown for these blocks. Each histogram still shows at least <span class="math inline">\(10^9\)</span> pairs;</li>
<li><a href="#fig-appendix-norm-norm-scatter" class="quarto-xref">Figure&nbsp;40</a>: data: <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a>;</li>
</ul>
</section>
</section>
<section id="sec-appendix-sd-models" class="level2">
<h2 class="anchored" data-anchor-id="sec-appendix-sd-models">Stable Diffusion Models</h2>
<p>The SD series has attracted interest among researchers, artists, hobbyists, and commercial users, which lead to a large variety of different third-party finetunes and variants. This section lists and shortly describes primarily official SD variants and checkpoints. The following information is taken from <span class="citation" data-cites="CompVis2022StableDiffusionRepo">[<a href="#ref-CompVis2022StableDiffusionRepo" role="doc-biblioref">98</a>]</span>, <span class="citation" data-cites="StabilityAI2022StableDiffusionRepo">[<a href="#ref-StabilityAI2022StableDiffusionRepo" role="doc-biblioref">102</a>]</span>, and the respective referenced model cards on Hugging Face.</p>
<ul>
<li><strong>SD-1.1</strong>: 237k steps at resolution 256 on laion2B-en. 194k steps at resolution 512 on laion-high-resolution, released by CompVis.<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> Initial release of SD as proposed by <span class="citation" data-cites="rombach2022highresolution">[<a href="#ref-rombach2022highresolution" role="doc-biblioref">1</a>]</span>.</li>
<li><strong>SD-1.2</strong>: Resumed from SD-1.1. 515k steps at resolution 512 on laion-aesthetics v2 5+ with additional filtering. Released by CompVis.<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a></li>
<li><strong>SD-1.3</strong>: Resumed from SD-1.2. 195k steps at resolution 512 on “laion-aesthetics v2 5+” and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. Released by CompVis.<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a></li>
<li><strong>SD-1.4</strong>: Resumed from SD-1.2. 225k steps at resolution 512 on “laion-aesthetics v2 5+” and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. Released by CompVis.<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a></li>
<li><strong>SD-1.5</strong>: Resumed from SD-1.2. 595k steps at resolution 512 on “laion-aesthetics v2 5+” and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. Released by Runway.<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a> This checkpoint is the latest of the original SD series and received the most attention from the community.</li>
<li><strong>SD-1.6</strong>: Announced by Stability AI, but not available as open-source.<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a></li>
<li><strong>SD-2.0-base</strong>: Same number of parameters in the U-Net as SD-1.5, but uses OpenCLIP-ViT/H as the text encoder and is trained from scratch 550k steps at resolution 256 on a subset of LAION-5B. Then it is further trained for 850k steps at resolution 512. Released by Stability AI.<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a></li>
<li><strong>SD-2.0</strong>: Resumed from SD-2.0-base. 150k steps using a v-objective <span class="citation" data-cites="salimans2022progressivedistillationfastsampling">[<a href="#ref-salimans2022progressivedistillationfastsampling" role="doc-biblioref">103</a>]</span> on the same dataset. Resumed for another 140k steps on 768 images. Released by Stability AI.<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a></li>
<li><strong>SD-2.1</strong>: Resumed from SD-2.0. 55k steps of the same dataset (with <code>punsafe=0.1</code>), and then fine-tuned for another 155k extra steps (<code>with punsafe=0.98</code>). Released by Stability AI.<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a></li>
<li><strong>SD-Turbo</strong>: Distilled version of SD-2.1 using adversarial diffusion distillation <span class="citation" data-cites="sauer2023adversarial">[<a href="#ref-sauer2023adversarial" role="doc-biblioref">51</a>]</span> to generate images with only 1-4 steps required. Released by Stability AI.<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a></li>
<li><strong>SDXL</strong>: Updated architecture with more parameters and a second text-encoder <span class="citation" data-cites="podell2023sdxl">[<a href="#ref-podell2023sdxl" role="doc-biblioref">12</a>]</span>. The base model can be used standalone or in an ensemble of experts, where an additional refinement model is used to improve image quality. Released by Stability AI.<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a></li>
<li><strong>SDXL-Turbo</strong>: Distilled version of SDXL using adversarial diffusion distillation <span class="citation" data-cites="sauer2023adversarial">[<a href="#ref-sauer2023adversarial" role="doc-biblioref">51</a>]</span> to generate images with only 1-4 steps required. Released by Stability AI.<a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a></li>
<li><strong>SDXL-Lightning</strong>: Alternative distilled version of SDXL based on <span class="citation" data-cites="lin2024sdxllightning">[<a href="#ref-lin2024sdxllightning" role="doc-biblioref">52</a>]</span> to generate images with only 1-8 steps required. Compared with SDXL-Turbo, it has improved image size and quality. Released by ByteDance.<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a> The finetuning and release of this model is independent of Stability AI, so it is not part of the original SD series, but has nonetheless received a lot of attention.</li>
</ul>
</section>
<section id="sec-appendix-position-bias" class="level2">
<h2 class="anchored" data-anchor-id="sec-appendix-position-bias">Position Bias</h2>
<p>This sections provides additional visualizations and results for the position bias (see <a href="#sec-position-bias" class="quarto-xref">Section&nbsp;5.1</a>).</p>
<div id="cell-fig-appendix-position-classifier-example" class="cell" data-execution_count="34">
<div class="cell-output cell-output-display">
<div id="fig-appendix-position-classifier-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-appendix-position-classifier-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-appendix-position-classifier-example-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-appendix-position-classifier-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;27: Position estimation examples for two linear probe position estimators.
</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Position Estimators Example.</strong> <a href="#fig-appendix-position-classifier-example" class="quarto-xref">Figure&nbsp;27</a> visualizes exemplarily the position estimation results for two linear probe position estimators are trained for 5 epochs on the SD-1.5 representations of 80% of the <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a> using Adam <span class="citation" data-cites="kingma2017adam">[<a href="#ref-kingma2017adam" role="doc-biblioref">93</a>]</span>. The classification estimator has outputs for each row and column of the representation and is trained using cross-entropy loss. The regression estimator has x and y coordinates as outputs and is trained using MSE loss. The figure shows the position estimation results for one image of the test set. For this example, we use the first image of the test set and estimate the position of each token using the two estimators. Depending on the block, the estimation result is more or less accurate. The ideal position estimation would be equal to the displayed true positions. It is visible that the position estimates for the lower blocks of the U-Net are closer to the true position, while the position estimates for the higher blocks are of worse quality. Additionally, the classification estimator is able to estimate the position more accurately than the regression estimator.</p>
<div id="cell-fig-appendix-dense-correspondence-flip-all-blocks" class="cell" data-execution_count="35">
<div class="cell-output cell-output-display">
<div id="fig-appendix-dense-correspondence-flip-all-blocks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-appendix-dense-correspondence-flip-all-blocks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-appendix-dense-correspondence-flip-all-blocks-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-appendix-dense-correspondence-flip-all-blocks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;28: Visualization of examples for dense correspondence and the occurring error when images are flipped.
</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Dense Correspondence Examples over all Blocks.</strong> <a href="#fig-appendix-dense-correspondence-flip-all-blocks" class="quarto-xref">Figure&nbsp;28</a> visualizes dense correspondence results for SD-1.5 over all block outputs. The representations are extracted for an image in both original and flipped/mirrored form and then each token of the flipped representation is matched to the token with the highest cosine similarity in the original representation. The first column visualizes the mapping using the pixels of the original image, the second column shows the mapping on an image with color gradients, and the third column shows the error of the mapping. Given the space of all spatial positions <span class="math inline">\(P = \{1,...,w\}\times\{1,...,h\}\)</span>, the error is computed by comparing the Euclidean distance between the original position <span class="math inline">\(p_\text{src}\in P\)</span> of the mapped tokens to the position in the original image that corresponds to the predicted position <span class="math inline">\(p_\text{pred}\in P\)</span> in the flipped image. This error is then compared to all possible errors at the current position to compute the error percentile <span class="math inline">\(E\)</span>:</p>
<p><span class="math display">\[E = \frac{|\{p' \mid ||\text{flip}(p_\text{pred})-p_\text{src}|| \geq ||p'-p_\text{src}||,\ p'\in P\}|}{w\cdot h}\]</span></p>
<p>Using the error percentile as measure takes into account that the distribution of what errors are possible depends on the position in the image. This error calculation also applies to <a href="#fig-dense-correspondence-flip" class="quarto-xref">Figure&nbsp;13</a>. As can be seen by looking at the error maps and colorwheel of <a href="#fig-appendix-dense-correspondence-flip-all-blocks" class="quarto-xref">Figure&nbsp;28</a>, many errors are due to the representation tokens being mapped to their original absolute position, instead of to the desired flipped position. For example, this is well visible in the right image for <code>up[0]</code>, where many tokens at the left and right sides of the image are mapped to the original position. In the left image, this only really occurs in the region of the grass, where the semantics between left and right are ambiguous. For semantically different regions, the semantics seem to dominate over the positional embedding for determining the mapping. Important to note is that while the usage of absolute positional information is not desired in this task, the preservation of the position in the case of semantic ambiguity might be useful in other tasks.</p>
<div class="cell" data-layout="[[40,-5,40], [40,-5,40]]" data-execution_count="36">
<div id="fig-appendix-sc-errors-by-relative-position-full" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-appendix-sc-errors-by-relative-position-full-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-appendix-sc-errors-by-relative-position-full" style="flex-basis: 47.1%;justify-content: flex-start;">
<div id="fig-appendix-sc-errors-by-relative-position-full-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-appendix-sc-errors-by-relative-position-full-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-appendix-sc-errors-by-relative-position-full-output-1.svg" class="img-fluid figure-img" data-ref-parent="fig-appendix-sc-errors-by-relative-position-full">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-appendix-sc-errors-by-relative-position-full-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Error rate over the relative position to the source keypoint for SD-1.5. The center (0,0) is the location of the source keypoint.
</figcaption>
</figure>
</div>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 5.9%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-appendix-sc-errors-by-relative-position-full" style="flex-basis: 47.1%;justify-content: flex-start;">
<div id="fig-appendix-sc-errors-by-relative-position-full-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-appendix-sc-errors-by-relative-position-full-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-appendix-sc-errors-by-relative-position-full-output-2.svg" class="img-fluid figure-img" data-ref-parent="fig-appendix-sc-errors-by-relative-position-full">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-appendix-sc-errors-by-relative-position-full-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Error rate over the distance relative to the source keypoint for different models. The size of the encoded images is 512×512 px.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-appendix-sc-errors-by-relative-position-full" style="flex-basis: 47.1%;justify-content: flex-start;">
<div id="fig-appendix-sc-errors-by-relative-position-full-3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-appendix-sc-errors-by-relative-position-full-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-appendix-sc-errors-by-relative-position-full-output-3.svg" class="img-fluid figure-img" data-ref-parent="fig-appendix-sc-errors-by-relative-position-full">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-appendix-sc-errors-by-relative-position-full-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) Number of samples per pixel over the relative position to the source keypoint for SD-1.5.
</figcaption>
</figure>
</div>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 5.9%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-appendix-sc-errors-by-relative-position-full" style="flex-basis: 47.1%;justify-content: flex-start;">
<div id="fig-appendix-sc-errors-by-relative-position-full-4" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-appendix-sc-errors-by-relative-position-full-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-appendix-sc-errors-by-relative-position-full-output-4.svg" class="img-fluid figure-img" data-ref-parent="fig-appendix-sc-errors-by-relative-position-full">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-appendix-sc-errors-by-relative-position-full-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(d) Number of samples per pixel over the distance relative to the source keypoint for different models.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-appendix-sc-errors-by-relative-position-full-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29: Extended plots for the semantic correspondence error rate on our synthetic dataset (see <a href="#sec-position-bias" class="quarto-xref">Section&nbsp;5.1</a>) over the predicted position/distance relative to the source keypoint and the respective number of samples per pixel. The <code>up[1]</code> representations are used, except for the SDXL and SDXL-Turbo, where <code>up[0]</code> is used.
</figcaption>
</figure>
</div>
</div>
<p><strong>Semantic Correspondence on Synthetic Data.</strong> <a href="#fig-appendix-sc-errors-by-relative-position-full" class="quarto-xref">Figure&nbsp;29</a> shows the extended plots and additional information for <a href="#fig-sc-errors-by-relative-position" class="quarto-xref">Figure&nbsp;14</a>. In <a href="#fig-appendix-sc-errors-by-relative-position-full-1" class="quarto-xref">Figure&nbsp;29 (a)</a>, also the outer regions of the error rate map are shown, where the number of samples is low or zero, as visualized in <a href="#fig-appendix-sc-errors-by-relative-position-full-3" class="quarto-xref">Figure&nbsp;29 (c)</a>. Interestingly, the error rate on the very top of the error map in <a href="#fig-appendix-sc-errors-by-relative-position-full-1" class="quarto-xref">Figure&nbsp;29 (a)</a> is very high, which means that relatively often, source keypoints at the bottom of the image are erroneously mapped to the top of the target image. A potential reason for this could be the corner and border anomalies discussed in <a href="#sec-anomalies-corner" class="quarto-xref">Section&nbsp;5.3.1</a>. Due to the low number of samples in the outer regions with high distances, a few border anomalies could already cause this high error rate. Furthermore, just as in <a href="#fig-sc-errors-by-relative-position" class="quarto-xref">Figure&nbsp;14</a>, we see an increase in error rate at the center, where the predicted keypoint is erroneously mapped to the source keypoint. In <a href="#fig-appendix-sc-errors-by-relative-position-full-2" class="quarto-xref">Figure&nbsp;29 (b)</a>, we see the same information for different models plotted over the distance relative to the source keypoint. As before, the error rate is increased for very low and very high distances. Notably, this observation holds true for all tested models.</p>
<div id="cell-fig-appendix-sc-errors-by-relative-position-spair-maps" class="cell" data-execution_count="37">
<div class="cell-output cell-output-display">
<div id="fig-appendix-sc-errors-by-relative-position-spair-maps" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-appendix-sc-errors-by-relative-position-spair-maps-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-appendix-sc-errors-by-relative-position-spair-maps-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-appendix-sc-errors-by-relative-position-spair-maps-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;30: Semantic correspondence error rate (<span class="math inline">\(1 - \text{PCK}@0.1_{\text{bbox}}\)</span>) on <a href="#sec-datasets-spair"> SPair-71k</a> over the relative position of the prediction to the source keypoint for different blocks and time steps. Bin size is 32 px. Bright colors indicate high error rates and white indicates no samples at that relative position.
</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Semantic Correspondence on <a href="#sec-datasets-spair"> SPair-71k</a>.</strong> <a href="#fig-appendix-sc-errors-by-relative-position-spair-maps" class="quarto-xref">Figure&nbsp;30</a> shows the semantic correspondence error rate over the relative position of the prediction to the source keypoint, similar to <a href="#fig-appendix-sc-errors-by-relative-position-full-1" class="quarto-xref">Figure&nbsp;29 (a)</a>, but over different blocks and time steps, and on the <a href="#sec-datasets-spair"> SPair-71k</a> dataset instead of on our synthetic dataset. Some blocks (<code>down[2]</code> - <code>up[0]</code>) show a grid-like pattern with an increased error rate at the center, and e.g.&nbsp;for very high noise levels (time step 500), some blocks also show a slightly increased error rate at the center. But more generally, and especially at the more relevant <code>up[1]</code> block, we do not observe a clear increase in error rate at the center, which is a difference between our synthetic dataset and the <a href="#sec-datasets-spair"> SPair-71k</a> dataset. This indicates that the positional bias might be less pronounced when using real world data, such as the <a href="#sec-datasets-spair"> SPair-71k</a> dataset. A general observation is that the error rate tends to increase with increased distance, which is a trend that might explain the missing observation of the increase in error rate at the center. This trend is likely caused by the uneven distribution of foreground objects and keypoints in the <a href="#sec-datasets-spair"> SPair-71k</a> dataset, due to the use of real world images. An additional reason why our observation of the increase in error rate at the center is not visible here, is the spatial averaging used to create the error rate maps. The bin size is 32 px, so smaller details might be averaged out.</p>
<div id="cell-fig-appendix-sc-errors-by-relative-position-spair-lines" class="cell" data-execution_count="38">
<div class="cell-output cell-output-display">
<div id="fig-appendix-sc-errors-by-relative-position-spair-lines" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-appendix-sc-errors-by-relative-position-spair-lines-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-appendix-sc-errors-by-relative-position-spair-lines-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-appendix-sc-errors-by-relative-position-spair-lines-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;31: Semantic correspondence error rate (<span class="math inline">\(1 - \text{PCK}@0.1_{\text{bbox}}\)</span>) over the relative distance between the prediction and the source keypoint for different blocks and time steps. This is computed with SD-1.5 on the <a href="#sec-datasets-spair"> SPair-71k</a> dataset.
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-appendix-sc-errors-by-relative-position-spair-lines" class="quarto-xref">Figure&nbsp;31</a> shows the same data as <a href="#fig-appendix-sc-errors-by-relative-position-spair-maps" class="quarto-xref">Figure&nbsp;30</a>, but plotted over the distance relative to the source keypoint instead of the relative position. This plot shows that for most blocks and time steps, the error rate is increased for very low distances and then drops sharply. However, the general trend is an increase in error rate with increased distance, which we already observed in the previous plot. The initial spike in error rate for very low distances shows that the positional embedding also causes issues when using the <a href="#sec-datasets-spair"> SPair-71k</a> dataset, and not only for our synthetic dataset. However, as it doesn’t show up a in the error rate map in <a href="#fig-appendix-sc-errors-by-relative-position-spair-maps" class="quarto-xref">Figure&nbsp;30</a>, the phenomenon seems to be less pronounced than in our synthetic dataset.</p>
</section>
<section id="sec-appendix-texture-color-bias" class="level2">
<h2 class="anchored" data-anchor-id="sec-appendix-texture-color-bias">Texture and Color Bias</h2>
<p>This section provides additional examples and information related to the texture and color bias experiments in <a href="#sec-texture-color-bias" class="quarto-xref">Section&nbsp;5.2</a>.</p>
<div id="cell-fig-appendix-texture-color-bias-examples-1" class="cell" data-execution_count="39">
<div class="cell-output cell-output-display">
<div id="fig-appendix-texture-color-bias-examples-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-appendix-texture-color-bias-examples-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-appendix-texture-color-bias-examples-1-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-appendix-texture-color-bias-examples-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;32: Examples of texture and color bias in the representations of <code>conv-in</code> to <code>mid</code>. Continued in <a href="#fig-appendix-texture-color-bias-examples-2" class="quarto-xref">Figure&nbsp;33</a>.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="cell-fig-appendix-texture-color-bias-examples-2" class="cell" data-execution_count="40">
<div class="cell-output cell-output-display">
<div id="fig-appendix-texture-color-bias-examples-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-appendix-texture-color-bias-examples-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-appendix-texture-color-bias-examples-2-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-appendix-texture-color-bias-examples-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;33: Examples of texture and color bias in the representations of <code>up[0]</code> to <code>conv-out</code>. For each representation token from the respective block for the target image, the most similar representation token from the source is selected using cosine similarity. This mapping is visualized by transferring the image pixels of the source image (columns 1, 3, 5) and pixels of the colorwheel (columns 2, 4, 6) from their source location to the location defined by the mapping.
</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Dense Correspondence Examples for All Blocks.</strong> <a href="#fig-appendix-texture-color-bias-examples-1" class="quarto-xref">Figure&nbsp;32</a> and <a href="#fig-appendix-texture-color-bias-examples-2" class="quarto-xref">Figure&nbsp;33</a> visualize the texture and color bias in all blocks of SD-1.5, compared to only a few in <a href="#fig-texture-color-bias-examples" class="quarto-xref">Figure&nbsp;15</a>. As discussed in <a href="#sec-texture-color-bias" class="quarto-xref">Section&nbsp;5.2</a>, we can observe a higher color and texture bias in the upper blocks (<code>conv-in</code>, <code>down[0]</code>, <code>down[1]</code>, <code>up[1]</code>, <code>up[2]</code>, <code>up[3]</code>), and less so in the lower blocks (<code>down[2]</code>, <code>down[3]</code>, <code>mid</code>, <code>up[0]</code>). The separation into upper and lower blocks is not a clear-cut, but rather a trend. <code>conv-out</code> is, as expected, not at all usable for dense correspondence, as explained in <a href="#sec-methods-representation-extraction" class="quarto-xref">Section&nbsp;2.2</a>.</p>
<div class="cell" data-execution_count="41">
<div id="fig-appendix-texture-bias-dense-correspondence-blur" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="41">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-appendix-texture-bias-dense-correspondence-blur-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div id="fig-appendix-texture-bias-dense-correspondence-blur-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-appendix-texture-bias-dense-correspondence-blur-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-appendix-texture-bias-dense-correspondence-blur-output-1.svg" class="img-fluid figure-img" data-ref-parent="fig-appendix-texture-bias-dense-correspondence-blur">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-appendix-texture-bias-dense-correspondence-blur-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Example of the gradual blurring of the image
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div id="fig-appendix-texture-bias-dense-correspondence-blur-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-appendix-texture-bias-dense-correspondence-blur-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-appendix-texture-bias-dense-correspondence-blur-output-2.svg" class="img-fluid figure-img" data-ref-parent="fig-appendix-texture-bias-dense-correspondence-blur">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-appendix-texture-bias-dense-correspondence-blur-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Accuracy, and change relative to the initial accuracy, for dense correspondence
</figcaption>
</figure>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-appendix-texture-bias-dense-correspondence-blur-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;34: Dense correspondence results on the <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a>, see (b), when blurring the image as visualized in (a). The radius of the Gaussian blur is gradually increased from 0 to 16 pixels. The ideal mapping (100% accuracy) would be if for each token in the target image, the token with the highest cosine similarity in the source image is at the same spatial location.
</figcaption>
</figure>
</div>
</div>
<div class="cell" data-execution_count="42">
<div id="fig-appendix-texture-bias-dense-correspondence-noise" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="42">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-appendix-texture-bias-dense-correspondence-noise-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div id="fig-appendix-texture-bias-dense-correspondence-noise-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-appendix-texture-bias-dense-correspondence-noise-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-appendix-texture-bias-dense-correspondence-noise-output-1.svg" class="img-fluid figure-img" data-ref-parent="fig-appendix-texture-bias-dense-correspondence-noise">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-appendix-texture-bias-dense-correspondence-noise-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Example of gradual noising of the image
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div id="fig-appendix-texture-bias-dense-correspondence-noise-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-appendix-texture-bias-dense-correspondence-noise-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-appendix-texture-bias-dense-correspondence-noise-output-2.svg" class="img-fluid figure-img" data-ref-parent="fig-appendix-texture-bias-dense-correspondence-noise">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-appendix-texture-bias-dense-correspondence-noise-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Accuracy, and change relative to the initial accuracy, for dense correspondence
</figcaption>
</figure>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-appendix-texture-bias-dense-correspondence-noise-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;35: Dense correspondence results on the <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a>, see (b), when noising the image as visualized in (a). The interpolation step <span class="math inline">\(i\)</span> indicates the amount of noise from no noise to 50% uniform noise, i.e.&nbsp;<span class="math inline">\(x' = (1-\frac{i}{2})\cdot x + \frac{i}{2}\cdot n\)</span> where <span class="math inline">\(x\)</span> is the original image and <span class="math inline">\(n\sim U[0,255)^{512\times512\times3}\)</span>. The ideal mapping (100% accuracy) would be if for each token in the target image, the token with the highest cosine similarity in the source image is at the same spatial location.
</figcaption>
</figure>
</div>
</div>
<p><strong>Dense Correspondence for Blurring and Noising.</strong> <a href="#fig-appendix-texture-bias-dense-correspondence-blur" class="quarto-xref">Figure&nbsp;34</a> and <a href="#fig-appendix-texture-bias-dense-correspondence-noise" class="quarto-xref">Figure&nbsp;35</a> show that the dense correspondence accuracy decreases when the image is blurred or noised, with the decrease being more pronounced for the upper blocks. Especially blurring the image primarily degrades fine texture details, while only slightly impacting color, thus showing texture bias somewhat independently of color bias.</p>
<div class="cell" data-execution_count="43">
<div id="fig-appendix-texture-color-bias-dense-correspondence-with-offset" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="43">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-appendix-texture-color-bias-dense-correspondence-with-offset-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div id="fig-appendix-texture-color-bias-dense-correspondence-with-offset-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-appendix-texture-color-bias-dense-correspondence-with-offset-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-appendix-texture-color-bias-dense-correspondence-with-offset-output-1.svg" class="img-fluid figure-img" data-ref-parent="fig-appendix-texture-color-bias-dense-correspondence-with-offset">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-appendix-texture-color-bias-dense-correspondence-with-offset-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Color channel permutation (RGB <span class="math inline">\(\rightarrow\)</span> BGR)
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div id="fig-appendix-texture-color-bias-dense-correspondence-with-offset-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-appendix-texture-color-bias-dense-correspondence-with-offset-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-appendix-texture-color-bias-dense-correspondence-with-offset-output-2.svg" class="img-fluid figure-img" data-ref-parent="fig-appendix-texture-color-bias-dense-correspondence-with-offset">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-appendix-texture-color-bias-dense-correspondence-with-offset-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Texture overlay
</figcaption>
</figure>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-appendix-texture-color-bias-dense-correspondence-with-offset-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;36: Dense correspondence results on the <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a>, similar to <a href="#fig-color-bias-rgb-bgr-dc" class="quarto-xref">Figure&nbsp;16</a> and <a href="#fig-texture-bias-texture-overlay-dc" class="quarto-xref">Figure&nbsp;17</a>. The difference is that the source and target images are crops of the original images, where the source image is in the top left and the target image in the bottom right, with an overlap in the middle. The dense correspondence accuracy is computed only for the overlapping region.
</figcaption>
</figure>
</div>
</div>
<p><strong>Dense Correspondence over Differently Cropped Images.</strong> In <a href="#fig-appendix-texture-color-bias-dense-correspondence-with-offset" class="quarto-xref">Figure&nbsp;36</a>, we can see the dense correspondence accuracy when a transform is applied to the image, for <a href="#fig-appendix-texture-color-bias-dense-correspondence-with-offset-1" class="quarto-xref">Figure&nbsp;36 (a)</a>, the color channels are permuted (just as in <a href="#fig-color-bias-rgb-bgr-dc" class="quarto-xref">Figure&nbsp;16</a>), while for <a href="#fig-appendix-texture-color-bias-dense-correspondence-with-offset-2" class="quarto-xref">Figure&nbsp;36 (b)</a>, an image is overlaid (just as in <a href="#fig-texture-bias-texture-overlay-dc" class="quarto-xref">Figure&nbsp;17</a>). Additionally, the source and target images are cropped versions of the original images, where the source image is in the top left and the target image in the bottom right, with an overlap in the middle. When comparing <a href="#fig-appendix-texture-color-bias-dense-correspondence-with-offset-1" class="quarto-xref">Figure&nbsp;36 (a)</a> with <a href="#fig-color-bias-rgb-bgr-dc" class="quarto-xref">Figure&nbsp;16</a>, we see that the accuracy is decreased for some blocks, most visibly for <code>up[0]</code>. This was to be expected, as the absolute positional embedding (discussed in <a href="#sec-position-bias" class="quarto-xref">Section&nbsp;5.1</a>) is not directly helpful for this dense correspondence task anymore. The relative change in accuracy, however, is very similar, indicating that the insight of lower color bias in the lower blocks might be independent of positional embedding. When comparing the dense correspondence results for the texture overlay in <a href="#fig-appendix-texture-color-bias-dense-correspondence-with-offset-2" class="quarto-xref">Figure&nbsp;36 (b)</a> with <a href="#fig-texture-bias-texture-overlay-dc" class="quarto-xref">Figure&nbsp;17</a>, a similar picture emerges, with partly lower accuracy, and a similar trend for the relative change in accuracy. In conclusion, <a href="#fig-appendix-texture-color-bias-dense-correspondence-with-offset" class="quarto-xref">Figure&nbsp;36</a> supports the observation of color and texture bias in the upper blocks and suggests that the cause of the better performance of the lower blocks cannot be fully explained by positional embedding.</p>
</section>
<section id="sec-appendix-corner-anomalies" class="level2">
<h2 class="anchored" data-anchor-id="sec-appendix-corner-anomalies">Corner and Border Anomalies</h2>
<p><a href="#fig-appendix-border-corner-similarity" class="quarto-xref">Figure&nbsp;37</a> shows the relative cosine similarities of the corners, borders, and other tokens in the representations of different models over the blocks. The results for SD-1.5 are shown and described in <a href="#fig-border-corner-similarity" class="quarto-xref">Figure&nbsp;20</a>, in <a href="#sec-anomalies-corner" class="quarto-xref">Section&nbsp;5.3.1</a>. The results for SD-2.1, SD-Turbo, SDXL, and SDXL-Turbo paint an overall similar picture as the results for SD-1.5, but with some differences. The similarity among corner tokens is generally higher than the similarity among all tokens. For SD-2.1 and SD-Turbo, the <code>up[0]</code> representations are a notable outlier, where this trend does not hold, but where the similarity is instead significantly decreased.</p>
<div id="fig-appendix-border-corner-similarity" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-appendix-border-corner-similarity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div id="cell-fig-appendix-border-corner-similarity-over-blocks-sd21" class="cell quarto-layout-cell-subref quarto-layout-cell" data-execution_count="44" data-ref-parent="fig-appendix-border-corner-similarity" style="flex-basis: 48.0%;justify-content: flex-start;">
<div class="cell-output cell-output-display quarto-layout-cell-subref" data-ref-parent="fig-appendix-border-corner-similarity">
<div id="fig-appendix-border-corner-similarity-over-blocks-sd21" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-appendix-border-corner-similarity-over-blocks-sd21-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-appendix-border-corner-similarity-over-blocks-sd21-output-1.svg" class="img-fluid figure-img" data-ref-parent="fig-appendix-border-corner-similarity">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-appendix-border-corner-similarity-over-blocks-sd21-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) SD-2.1
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 4.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div id="cell-fig-appendix-border-corner-similarity-over-blocks-sd-turbo" class="cell quarto-layout-cell-subref quarto-layout-cell" data-execution_count="45" data-ref-parent="fig-appendix-border-corner-similarity" style="flex-basis: 48.0%;justify-content: flex-start;">
<div class="cell-output cell-output-display quarto-layout-cell-subref" data-ref-parent="fig-appendix-border-corner-similarity">
<div id="fig-appendix-border-corner-similarity-over-blocks-sd-turbo" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-appendix-border-corner-similarity-over-blocks-sd-turbo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-appendix-border-corner-similarity-over-blocks-sd-turbo-output-1.svg" class="img-fluid figure-img" data-ref-parent="fig-appendix-border-corner-similarity">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-appendix-border-corner-similarity-over-blocks-sd-turbo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) SD-Turbo
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div id="cell-fig-appendix-border-corner-similarity-over-blocks-sdxl" class="cell quarto-layout-cell-subref quarto-layout-cell" data-execution_count="46" data-ref-parent="fig-appendix-border-corner-similarity" style="flex-basis: 48.0%;justify-content: flex-start;">
<div class="cell-output cell-output-display quarto-layout-cell-subref" data-ref-parent="fig-appendix-border-corner-similarity">
<div id="fig-appendix-border-corner-similarity-over-blocks-sdxl" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-appendix-border-corner-similarity-over-blocks-sdxl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-appendix-border-corner-similarity-over-blocks-sdxl-output-1.svg" class="img-fluid figure-img" data-ref-parent="fig-appendix-border-corner-similarity">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-appendix-border-corner-similarity-over-blocks-sdxl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) SDXL
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 4.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div id="cell-fig-appendix-border-corner-similarity-over-blocks-sdxl-turbo" class="cell quarto-layout-cell-subref quarto-layout-cell" data-execution_count="47" data-ref-parent="fig-appendix-border-corner-similarity" style="flex-basis: 48.0%;justify-content: flex-start;">
<div class="cell-output cell-output-display quarto-layout-cell-subref" data-ref-parent="fig-appendix-border-corner-similarity">
<div id="fig-appendix-border-corner-similarity-over-blocks-sdxl-turbo" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-appendix-border-corner-similarity-over-blocks-sdxl-turbo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-appendix-border-corner-similarity-over-blocks-sdxl-turbo-output-1.svg" class="img-fluid figure-img" data-ref-parent="fig-appendix-border-corner-similarity">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-appendix-border-corner-similarity-over-blocks-sdxl-turbo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(d) SDXL-Turbo
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-appendix-border-corner-similarity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37: Relative cosine similarities in the token groups containing the corners, the borders, or the other tokens. A value above 1 indicates increased similarities among a token group, when the corners/borders are already in the corners/borders during representation extraction.
</figcaption>
</figure>
</div>
</section>
<section id="sec-appendix-repr-norms" class="level2">
<h2 class="anchored" data-anchor-id="sec-appendix-repr-norms">Representation Norms</h2>
<p>Investigating the norms of the extracted representations can provide insights into the representations and help to identify anomalies or interesting features. There are different norms that can be calculated for a given token, such as the Manhattan norm (L1), the Euclidean norm (L2), and maximum norm (L<span class="math inline">\(\infty\)</span>), as described in <a href="#sec-representation-similarities" class="quarto-xref">Section&nbsp;2.3</a>. We find similar results for these norms, and display the results for the L2 norm in the following figures.</p>
<p><strong>Norm Maps.</strong> In <a href="#fig-appendix-spatial-norm" class="quarto-xref">Figure&nbsp;38</a>, which shows the average norms over the 500 images of the <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a>, one can see that for many blocks and models, the border and/or the corners have a visibly higher or lower norm. For example, in <code>down[1]</code>, the top right corner in has a unusually high norm, or in <code>up[0]</code>, <code>up[1]</code>, and <code>up[2]</code> the borders and especially the corners have a relatively low norm. Also visible is a general trend of increasing norms towards the center, which is, however, most likely due to the tendency of foreground object having high norms and being located in the center. In contrast to this, the observations for borders and corners do not obviously correspond to some semantic meaning.</p>
<div id="cell-fig-appendix-spatial-norm" class="cell" data-execution_count="48">
<div class="cell-output cell-output-display">
<div id="fig-appendix-spatial-norm" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-appendix-spatial-norm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-appendix-spatial-norm-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-appendix-spatial-norm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;38: L2 norm of the representations of different models and blocks. Averaged over the 500 images of the <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a>. Darker colors indicate higher norms.
</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Norm Distributions.</strong> <a href="#fig-appendix-histogram-norm-similarity" class="quarto-xref">Figure&nbsp;39</a> shows the norm distributions for different blocks and time steps. They significantly differ between blocks, but mostly stay similar for different time steps. In the norm distributions of <code>up[1]</code> and <code>up[2]</code>, the number of outliers increases with higher time steps, which might be caused by the high-norm anomalies in the <code>up[1]</code> and <code>up[2]</code> blocks (see <a href="#sec-anomalies-high-norm" class="quarto-xref">Section&nbsp;5.3.2</a>).</p>
<div id="cell-fig-appendix-histogram-norm-similarity" class="cell" data-execution_count="49">
<div class="cell-output cell-output-display">
<div id="fig-appendix-histogram-norm-similarity" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-appendix-histogram-norm-similarity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-appendix-histogram-norm-similarity-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-appendix-histogram-norm-similarity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;39: 2D-Histograms of the relative frequency of token pairs in <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a> over their L2 norm and cosine similarity.
</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Similarity Distribution over Token Norms.</strong> <a href="#fig-appendix-norm-norm-scatter" class="quarto-xref">Figure&nbsp;40</a> shows the L2 norms of <code>up[1]</code> representation tokens of the <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a> in comparison with the norms of their most similar matches, by cosine similarity. Additionally, we plot the pairs of the tokens, where the first token has the highest norm in the corresponding representation. This scatter plot is accompanied by a histogram visualizing the distribution of the norms for both all pairs and the highest norm pairs. We find a quite varied and spread out distribution for the group of all pairs. However, for the highest norm tokens, this distribution is heavily biased towards other very high-norm tokens. This means that the tokens with the highest norm in a representation have often the highest cosine similarity to other tokens with a high norm. This effect matches well to the observation of high norm anomalies with non-semantic cosine similarities between each other (see <a href="#sec-anomalies-high-norm" class="quarto-xref">Section&nbsp;5.3.2</a>).</p>
<div id="cell-fig-appendix-norm-norm-scatter" class="cell" data-execution_count="50">
<div class="cell-output cell-output-display">
<div id="fig-appendix-norm-norm-scatter" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-appendix-norm-norm-scatter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-appendix-norm-norm-scatter-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-appendix-norm-norm-scatter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;40: Scatter plot of the L2 norms of tokens and the norm of their most similar matches. A random subset (<span class="math inline">\(10^4\)</span>) of all possible pairs of tokens in the <code>up[1]</code> representations of the 500 images of the <a href="#sec-datasets-imagenet-subset"> ImageNet Subset</a> is shown in blue and all pairs with the highest norm token are shown in orange.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>While the U-Net architecture is used for most SD models, it is not the only option, and the most recent models use a transformer instead, see <a href="#sec-future-work" class="quarto-xref">Section&nbsp;6.3</a>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Our code is available on GitHub: <a href="https://github.com/jonasloos/thesis">github.com/jonasloos/thesis</a>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The representation similarity explorer is available with precomputed representations at <a href="https://sd-similarities.jloos.de">sd-similarities.jloos.de</a>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>CompVis is the Computer Vision &amp; Learning Group at the Ludwig Maximilian University of Munich.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Runway AI, Inc.&nbsp;is a US-based company developing AI systems for audiovisual media.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Stability AI Ltd.&nbsp;is a company developing generative AI systems.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>CIFAR-10 on Hugging Face: <a href="https://huggingface.co/datasets/cifar10">huggingface.co/datasets/cifar10</a><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>CIFAR-100 on Hugging Face: <a href="https://huggingface.co/datasets/cifar100">huggingface.co/datasets/cifar100</a><a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>ImageNet-1k on Hugging Face: <a href="https://huggingface.co/datasets/imagenet-1k">huggingface.co/datasets/imagenet-1k</a><a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Tiny ImageNet on Hugging Face: <a href="https://huggingface.co/datasets/zh-plus/tiny-imagenet">huggingface.co/datasets/zh-plus/tiny-imagenet</a><a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>Our SPair-71k page on the Hugging Face Hub: <a href="https://huggingface.co/datasets/0jl/SPair-71k">huggingface.co/datasets/0jl/SPair-71k</a><a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>Our NYU Depth v2 page on the Hugging Face Hub: <a href="https://huggingface.co/datasets/0jl/NYUv2">huggingface.co/datasets/0jl/NYUv2</a><a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>The <code>sdhelper</code> package is available on GitHub: <a href="https://github.com/JonasLoos/sdhelper">github.com/JonasLoos/sdhelper</a> and can be installed with <code>pip install sdhelper</code>.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>Code and setup instructions are available on GitHub: <a href="https://github.com/JonasLoos/sd_representation_similarity_explorer">github.com/JonasLoos/sd_representation_similarity_explorer</a><a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>For example, for semantic correspondence on <a href="#sec-datasets-spair"> SPair-71k</a>, the number of keypoints per corner token is, depending on the block, 3-6 times below the average number of keypoints per token.<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p>More information about the Hydra cluster can be found at <a href="https://git.tu-berlin.de/ml-group/hydra/documentation">git.tu-berlin.de/ml-group/hydra/documentation</a><a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p>SD-1.1 Hugging Face repository: <a href="https://huggingface.co/CompVis/stable-diffusion-v1-1">huggingface.co/CompVis/stable-diffusion-v1-1</a><a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p>SD-1.2 Hugging Face repository: <a href="https://huggingface.co/CompVis/stable-diffusion-v1-2">huggingface.co/CompVis/stable-diffusion-v1-2</a><a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p>SD-1.3 Hugging Face repository: <a href="https://huggingface.co/CompVis/stable-diffusion-v1-3">huggingface.co/CompVis/stable-diffusion-v1-3</a><a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20"><p>SD-1.4 Hugging Face repository: <a href="https://huggingface.co/CompVis/stable-diffusion-v1-4">huggingface.co/CompVis/stable-diffusion-v1-4</a><a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21"><p>SD-1.5: The original Hugging Face repository by Runway was deleted. An alternative repository can be found at <a href="https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5">huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5</a><a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22"><p>SD-1.6 Announcement: <a href="https://platform.stability.ai/docs/release-notes#stable-image-v1-release">platform.stability.ai/docs/release-notes#stable-image-v1-release</a><a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn23"><p>SD-2.0 Hugging Face repository: <a href="https://huggingface.co/stabilityai/stable-diffusion-2-base">huggingface.co/stabilityai/stable-diffusion-2-base</a><a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn24"><p>SD-2.0 Hugging Face repository: <a href="https://huggingface.co/stabilityai/stable-diffusion-2">huggingface.co/stabilityai/stable-diffusion-2</a><a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25"><p>SD-2.1 Hugging Face repository: <a href="https://huggingface.co/stabilityai/stable-diffusion-2-1">huggingface.co/stabilityai/stable-diffusion-2-1</a><a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn26"><p>SD-Turbo Hugging Face repository: <a href="https://huggingface.co/stabilityai/sd-turbo">huggingface.co/stabilityai/sd-turbo</a><a href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn27"><p>SDXL Hugging Face repository: <a href="https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0">huggingface.co/stabilityai/stable-diffusion-xl-base-1.0</a><a href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn28"><p>SDXL-Turbo Hugging Face repository: <a href="https://huggingface.co/stabilityai/sdxl-turbo">huggingface.co/stabilityai/sdxl-turbo</a><a href="#fnref28" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn29"><p>SDXL-Lightning Hugging Face repository: <a href="https://huggingface.co/bytedance/sdxl-lightning">huggingface.co/bytedance/sdxl-lightning</a><a href="#fnref29" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>