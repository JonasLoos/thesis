{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sdhelper import SD\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"jonasloos/imagenet_subset\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SD()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [d['image'] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "block = 'mid_block'\n",
    "\n",
    "representations = sd.img2repr(images, extract_positions=[block], step=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h = images[0].size\n",
    "token_size = w // representations[0][block].shape[-1]\n",
    "representations_cropped = sd.img2repr([img.crop((token_size, token_size, w-token_size, h-token_size)) for img in images], extract_positions=[block], step=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'base: {representations[0][block].shape}')\n",
    "print(f'cropped: {representations_cropped[0][block].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_maps = torch.stack([x.cosine_similarity(x) for x in tqdm(representations)])\n",
    "similarity_maps_cropped = torch.stack([x.cosine_similarity(x) for x in tqdm(representations_cropped)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = similarity_maps.shape[1]\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(similarity_maps[:,1,1,:,:].mean(dim=0))\n",
    "plt.plot([.5,.5,n-1.5,n-1.5,.5], [.5,n-1.5,n-1.5,.5,.5], 'k-')\n",
    "plt.axis('off')\n",
    "plt.title('Full image')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(F.pad(similarity_maps_cropped[:,0,0,:,:].mean(dim=0), (1,1,1,1), value=torch.nan))\n",
    "plt.axis('off')\n",
    "plt.title('Cropped image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner_slices = [(a,b) for a in [0,-1] for b in [0,-1]]\n",
    "border_slices = [(slice(1,-1), 0), (slice(1,-1), -1), (0, slice(1,-1)), (-1, slice(1,-1))]\n",
    "other_slices  = [(slice(1,-1), slice(1,-1))]\n",
    "slices = [\n",
    "    ('corner', corner_slices),\n",
    "    ('border', border_slices),\n",
    "    ('other', other_slices),\n",
    "]\n",
    "\n",
    "similarity_maps_repr_cropped = F.pad(similarity_maps, [-1]*8, value=torch.nan)\n",
    "\n",
    "for sim_maps in [similarity_maps_cropped, similarity_maps_repr_cropped]:\n",
    "    n = similarity_maps.shape[1]\n",
    "    m = len(slices)\n",
    "\n",
    "    result = torch.zeros((m, m))\n",
    "    for i, (name1, slices1) in enumerate(slices):\n",
    "        for j, (name2, slices2) in enumerate(slices):\n",
    "            count = torch.stack([torch.ones((n,n,n,n))[s11,s12,s21,s22].sum() for s11, s12 in slices1 for s21, s22 in slices2]).sum() * len(sim_maps)\n",
    "            self_similarities = torch.stack([torch.ones((n,n))[s11,s12].sum() for s11, s12 in slices1]).sum() * len(sim_maps) if i == j else 0\n",
    "            result[i,j] = (torch.stack([sim_maps[:,s11,s12,s21,s22].sum() for s11, s12 in slices1 for s21, s22 in slices2]).sum() - self_similarities) / (count - self_similarities)\n",
    "\n",
    "    plt.imshow(result, vmin=0)\n",
    "    names = [x[0] for x in slices]\n",
    "    plt.xticks(ticks=range(len(slices)), labels=names, rotation=0)\n",
    "    plt.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n",
    "\n",
    "    plt.yticks(ticks=range(len(slices)), labels=names)\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            plt.text(j, i, f'{result[i,j]:.4f}', ha='center', va='center', color='white' if result[i,j] < result.max()/2 else 'black')\n",
    "\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner_slices = [(a,b) for a in [0,-1] for b in [0,-1]]\n",
    "border_slices = [(slice(1,-1), 0), (slice(1,-1), -1), (0, slice(1,-1)), (-1, slice(1,-1))]\n",
    "other_slices  = [(slice(1,-1), slice(1,-1))]\n",
    "slices = [\n",
    "    ('corner', corner_slices),\n",
    "    ('border', border_slices),\n",
    "    ('other', other_slices),\n",
    "]\n",
    "\n",
    "def calc_group_similarities(block, images):\n",
    "    representations = sd.img2repr(images, extract_positions=[block], step=50)\n",
    "    w, h = images[0].size\n",
    "    token_size = w // representations[0][block].shape[-1]\n",
    "    representations_cropped = sd.img2repr([img.crop((token_size, token_size, w-token_size, h-token_size)) for img in images], extract_positions=[block], step=50)\n",
    "    similarity_maps = torch.stack([x.cosine_similarity(x) for x in representations])\n",
    "    similarity_maps_cropped = torch.stack([x.cosine_similarity(x) for x in representations_cropped])\n",
    "    similarity_maps_repr_cropped = F.pad(similarity_maps, [-1]*8, value=torch.nan)\n",
    "\n",
    "    results = []\n",
    "    for sim_maps in [similarity_maps_cropped, similarity_maps_repr_cropped]:\n",
    "        n = similarity_maps.shape[1]\n",
    "        m = len(slices)\n",
    "\n",
    "        result = torch.zeros((m, m))\n",
    "        for i, (_, slices1) in enumerate(slices):\n",
    "            for j, (_, slices2) in enumerate(slices):\n",
    "                count = torch.stack([torch.ones((n,n,n,n))[s11,s12,s21,s22].sum() for s11, s12 in slices1 for s21, s22 in slices2]).sum() * len(sim_maps)\n",
    "                self_similarities = torch.stack([torch.ones((n,n))[s11,s12].sum() for s11, s12 in slices1]).sum() * len(sim_maps) if i == j else 0\n",
    "                result[i,j] = (torch.stack([sim_maps[:,s11,s12,s21,s22].sum() for s11, s12 in slices1 for s21, s22 in slices2]).sum() - self_similarities) / (count - self_similarities)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "results = {}\n",
    "for block in tqdm(sd.available_extract_positions):\n",
    "    results[block] = calc_group_similarities(block, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_filtered = {k: v for k, v in results.items() if k not in ['conv_out']}\n",
    "xs = range(len(results_filtered))\n",
    "\n",
    "for i, group_name in enumerate(['corner', 'border', 'other']):\n",
    "    plt.plot(xs, [x[0][i,i]/x[1][i,i] for x in results_filtered.values()], label=group_name)\n",
    "\n",
    "plt.plot(xs, [1]*len(xs), 'k--', color='gray')\n",
    "plt.ylabel('Relative Similarity')\n",
    "plt.yscale('log')\n",
    "plt.yticks([0.5, 1, 2], ['0.5', '1', '2'])\n",
    "plt.gca().yaxis.set_minor_formatter(plt.NullFormatter())\n",
    "block_names = [x.replace('_blocks', '').replace('_block', '').replace('_', '-') for x in results_filtered.keys()]\n",
    "plt.xticks(ticks=xs, labels=block_names, rotation=90)\n",
    "plt.title('Similarity between groups of tokens')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# This shows that for many blocks, the tokens are more similar to each other if they are in the corner, as otherwise.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
