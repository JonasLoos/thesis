{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdhelper import SD\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoProcessor, CLIPModel, pipeline\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from piqa.ssim import ssim, SSIM\n",
    "from skimage.metrics import structural_similarity as sk_ssim\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SD('SDXL-Turbo', disable_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipEmbed:\n",
    "    def __init__(self, device):\n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.model.to(device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, images):\n",
    "        inputs = self.processor(images=images, return_tensors=\"pt\").to(self.model.device)\n",
    "        image_features = self.model.get_image_features(**inputs)\n",
    "        return image_features\n",
    "\n",
    "clip = ClipEmbed(sd.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_pipe = pipeline(task=\"mask-generation\", model=\"facebook/sam-vit-huge\", device=sd.device)  # MaskGenerationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 1\n",
    "extract_positions = sd.available_extract_positions\n",
    "# extract_positions = ['down_blocks[1]','down_blocks[2]','mid_block','up_blocks[0]','up_blocks[1]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_patches(image, num_patches: int, patch_size: int):\n",
    "    '''create a tensor of patches from an image'''\n",
    "    tile_size = image.shape[0] // num_patches\n",
    "    assert len(image.shape) == 3, f'Expected 3D image, got {len(image.shape)}D'\n",
    "    assert image.shape[0] % num_patches == 0, f'Image size {image.shape[0]} not divisible by num_patches {num_patches}'\n",
    "    assert patch_size % 2 == 0, f'Patch size {patch_size} must be even'\n",
    "    assert patch_size > tile_size, f'Patch size {patch_size} must be larger than tile size {tile_size}'\n",
    "\n",
    "    patches = []\n",
    "    for i in range(num_patches):\n",
    "        patch_row = []\n",
    "        for j in range(num_patches):\n",
    "            x_start = i * tile_size - (patch_size - tile_size) // 2\n",
    "            x_end = x_start + patch_size\n",
    "            y_start = j * tile_size - (patch_size - tile_size) // 2\n",
    "            y_end = y_start + patch_size\n",
    "            if x_start < 0 or x_end > image.shape[0] or y_start < 0 or y_end > image.shape[1]:\n",
    "                # skip patches going out of bounds\n",
    "                patches.append(np.full((patch_size, patch_size, image.shape[2]), 0))\n",
    "                pass\n",
    "            else:\n",
    "                patch_row.append(image[x_start:x_end, y_start:y_end, :])\n",
    "        patches.append(patch_row)\n",
    "        \n",
    "    return torch.tensor(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create (n,n,n,n) tensors for each position\n",
    "def create_map(): return {pos: torch.zeros([(shape if isinstance(shape[0], int) else shape[0])[-1]**2]*2) for pos, shape in sd.representation_shapes.items()}\n",
    "maps = defaultdict(create_map)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in trange(num_images):\n",
    "        result = sd('a city scene', extract_positions=extract_positions)\n",
    "\n",
    "        # sam_segmentation = sam(result.result_image)\n",
    "        sam_masks = torch.tensor(sam_pipe(result.result_image)['masks'])\n",
    "        sam_embedding = next(sam_pipe.preprocess(result.result_image))['image_embeddings']  # has shape [1, 256, 64, 64]\n",
    "\n",
    "        for pos, repr in tqdm(list(result.representations.items())):\n",
    "            repr = repr[-1]  # use last step\n",
    "            if isinstance(repr, tuple):\n",
    "                # ignore skip connections\n",
    "                repr = repr[0]\n",
    "            repr = repr.reshape((repr.shape[1],-1)).permute(1,0)\n",
    "            n = int(repr.shape[0]**0.5)\n",
    "            img = np.array(result.result_image)\n",
    "            patches = make_patches(img, n, 100)\n",
    "            # TODO: edge patches are not valid and should be ignored\n",
    "\n",
    "            # calculate cosine similarity\n",
    "            cs = torch.einsum('ij,kj->ik', repr, repr) / (repr.norm(dim=1, keepdim=True) @ repr.norm(dim=1, keepdim=True).T)\n",
    "            maps['CS'][pos] += cs.cpu()\n",
    "\n",
    "            # calculate l1 distance\n",
    "            maps['L1'][pos] += torch.cdist(repr.float(), repr.float(), p=1).cpu()\n",
    "\n",
    "            # calculate l2 distance\n",
    "            maps['L2'][pos] += torch.cdist(repr.float(), repr.float(), p=2).cpu()\n",
    "\n",
    "            # calculate l1 distance between images\n",
    "            # tiled_image = img.reshape(n,img.shape[0]//n,n,img.shape[0]//n,3).transpose(0,2,1,3,4).reshape(n*n,-1)\n",
    "            torch_img_flat = torch.tensor(patches.reshape(n*n,-1), device=sd.device).float()\n",
    "            maps['L1_img'][pos] += torch.cdist(torch_img_flat, torch_img_flat, p=1).cpu()\n",
    "\n",
    "            # calculate l2 distance between images\n",
    "            maps['L2_img'][pos] += torch.cdist(torch_img_flat, torch_img_flat, p=2).cpu()\n",
    "\n",
    "            # calculate clip similarity\n",
    "            # torch_img = torch_img_flat.reshape((n*n,*[img.shape[0]//n]*2,3)).permute(0,3,1,2)\n",
    "            clip_embedding = clip(patches.reshape(n*n,*patches.shape[2:]).permute(0,3,1,2).to(sd.device))\n",
    "            maps['Clip'][pos] += torch.einsum('ij,kj->ik', clip_embedding, clip_embedding).cpu()\n",
    "\n",
    "            # calculate ssim\n",
    "            # try:\n",
    "            #     torch_img_ = torch_img.to(sd.device, dtype=torch.float32) / 255\n",
    "            #     ssim = SSIM().to(sd.device)\n",
    "            #     maps['SSIM'][pos] += torch.stack([ssim(torch_img_, torch_img_[i].expand_as(torch_img_)) for i in range(n*n)], 0).cpu()\n",
    "            # except:\n",
    "            #     pass\n",
    "\n",
    "            # sklearn ssim (sklearn)\n",
    "            # maps['SSIM2'][pos] += torch.tensor([[sk_ssim(torch_img_.cpu().numpy()[i], torch_img_.cpu().numpy()[j], gaussian_weights=True, channel_axis=0, data_range=1) for j in range(n*n)] for i in trange(n*n)])\n",
    "\n",
    "            # calculate segment anything mask based similarity\n",
    "            sam_classes = sam_masks.reshape(-1, n, img.shape[0]//n, n, img.shape[0]//n).float().mean((2,4)).reshape(-1,n*n)\n",
    "            maps['SAM'][pos] += torch.einsum('ji,jk->ik', sam_classes, sam_classes)\n",
    "\n",
    "            # calculate segment anything embedding based similarity\n",
    "            sam_embedding_ = sam_embedding.reshape(256, n, sam_embedding.shape[-1]//n, n, sam_embedding.shape[-1]//n).mean((2,4)).reshape(256,n*n)\n",
    "            maps['SAM_Emb'][pos] += torch.einsum('ji,jk->ik', sam_embedding_, sam_embedding_).cpu()\n",
    "\n",
    "\n",
    "# replace nan with 0 and normalize\n",
    "for name, m in maps.items():\n",
    "    for pos, t in m.items():\n",
    "        t[torch.isnan(t)] = 0\n",
    "        t /= num_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the pearson correlation between the different metrics\n",
    "\n",
    "def print_pearson():\n",
    "    for pos in extract_positions:\n",
    "        # go through all combinations of metrics\n",
    "        for a_name in maps:\n",
    "            for b_name in maps:\n",
    "                a = maps[a_name][pos]\n",
    "                b = maps[b_name][pos]\n",
    "                pearson = torch.corrcoef(torch.stack((a.flatten(), b.flatten())))[0,1]\n",
    "                print(f'{pos:<15}: {a_name:^6} vs {b_name:^6}: {pearson.item():+.2f}')\n",
    "\n",
    "# print_pearson()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the correlation matrices\n",
    "\n",
    "def plot_corr_matrices():\n",
    "    matrix = torch.zeros((len(extract_positions), len(maps), len(maps)))\n",
    "    for pos in extract_positions:\n",
    "            for i, a_name in enumerate(maps):\n",
    "                for j, b_name in enumerate(maps):\n",
    "                    a = maps[a_name][pos]\n",
    "                    b = maps[b_name][pos]\n",
    "                    pearson = torch.corrcoef(torch.stack((a.flatten(), b.flatten())))[0,1]\n",
    "                    matrix[extract_positions.index(pos), i, j] = pearson.item()\n",
    "\n",
    "    for i, pos in enumerate(extract_positions):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "        cax = ax.matshow(matrix[i], cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_xticks(range(len(maps)))\n",
    "        ax.set_xticklabels(maps)\n",
    "        ax.set_yticks(range(len(maps)))\n",
    "        ax.set_yticklabels(maps)\n",
    "        ax.set_title('Correlation at position ' + pos)\n",
    "        plt.show()\n",
    "\n",
    "# plot_corr_matrices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(distance, bandwidth):\n",
    "    \"\"\"Computes the Gaussian kernel for a given distance and bandwidth using PyTorch.\"\"\"\n",
    "    return (1 / (bandwidth * torch.sqrt(torch.tensor(2 * torch.pi)))) * torch.exp(-0.5 * ((distance / bandwidth) ** 2))\n",
    "\n",
    "def gaussian_binning(x, y, bin_width, bandwidth):\n",
    "    \"\"\"\n",
    "    Applies Gaussian binning to data points using PyTorch.\n",
    "\n",
    "    Parameters:\n",
    "    - x: torch.Tensor, the x-values of the data points.\n",
    "    - y: torch.Tensor, the y-values of the data points.\n",
    "    - bin_width: float, the width of the bins.\n",
    "    - bandwidth: float, the bandwidth for the Gaussian kernel.\n",
    "    \n",
    "    Returns:\n",
    "    - bin_centers: torch.Tensor, the centers of the bins.\n",
    "    - weighted_means: torch.Tensor, the weighted means of the y-values.\n",
    "    - weighted_stds: torch.Tensor, the weighted standard deviations of the y-values.\n",
    "    \"\"\"\n",
    "    # Ensure input is a float tensor for compatibility\n",
    "    x, y = x.float(), y.float()\n",
    "    \n",
    "    # Define bins and bin centers\n",
    "    bins = torch.arange(torch.min(x), torch.max(x) + bin_width, bin_width, device=x.device)\n",
    "    bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "    \n",
    "    # Initialize arrays for the weighted means and standard deviations\n",
    "    weighted_means = torch.zeros(len(bin_centers))\n",
    "    weighted_stds = torch.zeros(len(bin_centers))\n",
    "\n",
    "    # Compute weighted mean and std for each bin\n",
    "    for i, bin_center in enumerate(bin_centers):\n",
    "        distances = torch.abs(x - bin_center)\n",
    "        weights = gaussian_kernel(distances, bandwidth)\n",
    "        weighted_sum = torch.sum(weights * y)\n",
    "        sum_of_weights = torch.sum(weights)\n",
    "        weighted_mean = weighted_sum / sum_of_weights\n",
    "        weighted_means[i] = weighted_mean\n",
    "        \n",
    "        # Weighted standard deviation calculation\n",
    "        variance = torch.sum(weights * (y - weighted_mean) ** 2) / sum_of_weights\n",
    "        weighted_stds[i] = torch.sqrt(variance)\n",
    "\n",
    "    return bin_centers, weighted_means, weighted_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity_comparison(similarities: list | None = None):\n",
    "    for pos in extract_positions:\n",
    "        num_items = len(maps) - 1 if similarities is None else len(similarities)\n",
    "        plt.figure(figsize=(6*num_items, 5))\n",
    "        i = 0\n",
    "        for name, map in maps.items():\n",
    "            if name == 'CS': continue\n",
    "            if similarities is not None and name not in similarities: continue\n",
    "            i += 1\n",
    "\n",
    "            # setup plot\n",
    "            plt.subplot(1, num_items, i)\n",
    "            plt.title(name)\n",
    "            plt.xlabel('Cosine similarity')\n",
    "\n",
    "            # setup data\n",
    "            cs = maps['CS'][pos].cuda()\n",
    "            idx = cs.flatten().argsort()\n",
    "            cossim_sorted = cs.flatten()[idx]\n",
    "            y_sorted = map[pos].cuda().flatten()[idx]\n",
    "\n",
    "            # plot heatmap\n",
    "            plt.hist2d(cossim_sorted.cpu().numpy(), y_sorted.cpu().numpy(), bins=100, cmap='Greys', norm='log')\n",
    "            # plt.hist2d(cossim_sorted.cpu().numpy(), y_sorted.cpu().numpy(), bins=100, cmap='Greys')\n",
    "\n",
    "            # plot lines\n",
    "            bin_centers, weighted_means, weighted_stds = gaussian_binning(cossim_sorted, y_sorted, 0.01, 0.02)\n",
    "            bin_centers = bin_centers.cpu().numpy()\n",
    "            weighted_means = weighted_means.cpu().numpy()\n",
    "            weighted_stds = weighted_stds.cpu().numpy()\n",
    "            plt.fill_between(bin_centers, weighted_means - weighted_stds, weighted_means + weighted_stds, alpha=0.5)\n",
    "            plt.plot(bin_centers, weighted_means, 'k-')\n",
    "\n",
    "            # set limits\n",
    "            plt.ylim(y_sorted.min().item(), y_sorted.max().item())\n",
    "            plt.xlim(-1, 1)\n",
    "\n",
    "        # show plot\n",
    "        plt.suptitle('Position ' + pos)\n",
    "        plt.show()\n",
    "\n",
    "plot_similarity_comparison()\n",
    "# plot_similarity_comparison(['Clip', 'SAM', 'SAM_Emb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
