{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SC over blocks and noise step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.autonotebook import tqdm\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['SD1.5', 'SD2.1', 'SD-Turbo', 'SDXL', 'SDXL-Turbo']\n",
    "resolutions = [256, 512, 768, 1024]\n",
    "blocks = ['conv_in','down_blocks[0]','down_blocks[1]','down_blocks[2]','down_blocks[3]','mid_block','up_blocks[0]','up_blocks[1]','up_blocks[2]','up_blocks[3]','conv_out']\n",
    "block_names = np.array([x.replace('_blocks', '').replace('_block', '').replace('_', '-') for x in blocks])\n",
    "sdxl_block_indices = np.array([i for i, b in enumerate(blocks) if '3' not in b])\n",
    "base_path = Path(f'PATH_TO_RESULTS/step50_vary_all_else/')\n",
    "\n",
    "data = np.zeros((len(models), len(resolutions), len(blocks), 88328, 12), dtype=int)\n",
    "for i, model in enumerate(tqdm(models, desc='models')):\n",
    "    for j, resolution in enumerate(resolutions):\n",
    "        for k, block in enumerate(blocks):\n",
    "            data_path = base_path / f'{model}-{block}-expand_and_resize-{resolution}-50.npy'\n",
    "            if not data_path.exists():\n",
    "                print(data_path.name, 'missing')\n",
    "                continue\n",
    "            tmp = np.load(data_path)\n",
    "            data[i,j,k,:,:] = tmp\n",
    "            # for correct, sx, sy, tx, ty, pred_x, pred_y, sn, sm, tn, tm, category_id in tmp:\n",
    "            #     pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pck = data[...,0].mean(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, len(models), figsize=(len(models)*2.5, 1*2.5))\n",
    "for i, model in enumerate(models):\n",
    "    colors = plt.cm.viridis(np.linspace(0.0, 0.9, len(resolutions)))\n",
    "    for j in range(len(resolutions)):\n",
    "        x = block_names[sdxl_block_indices] if 'SDXL' in model else block_names\n",
    "        y_idx = sdxl_block_indices if 'SDXL' in model else slice(None)\n",
    "        axs[i].plot(x, pck[i,j,y_idx]*100, color=colors[j], label=resolutions[j], marker='o', markersize=2, alpha=0.7)\n",
    "    axs[i].tick_params(labelsize=10, labelrotation=90)\n",
    "    # axs[i].set_xlabel('block')\n",
    "    axs[i].set_ylim(0)\n",
    "    axs[i].set_title(model)\n",
    "    axs[i].grid(axis='y', linestyle='--', alpha=0.3)\n",
    "    if i == 0:\n",
    "        axs[i].set_ylabel('PCK [%]')\n",
    "    else:\n",
    "        axs[i].set_yticklabels([])\n",
    "\n",
    "# Get and set global min/max y values across all subplots\n",
    "y_min = min(ax.get_ylim()[0] for ax in axs)\n",
    "y_max = max(ax.get_ylim()[1] for ax in axs)\n",
    "for ax in axs:\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "\n",
    "axs[-1].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'sc_pck_spair_step50_vary_all_else.npy', pck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error rate over prediction position relative to source position\n",
    "\n",
    "model_idx = 0\n",
    "lim = 0.5\n",
    "downscale = 4\n",
    "\n",
    "n = len(blocks)\n",
    "m = len(resolutions)\n",
    "error_rates = []\n",
    "for i, resolution in enumerate(tqdm(resolutions, desc='resolutions')):\n",
    "    error_rates.append(np.zeros((n, 2*resolution//downscale, 2*resolution//downscale)))\n",
    "    for j, block in enumerate(blocks):\n",
    "        tmp = data[model_idx, i, j]\n",
    "        error_counts = np.zeros((2*resolution, 2*resolution), dtype=int)\n",
    "        total_counts = np.zeros((2*resolution, 2*resolution), dtype=int)\n",
    "        for correct, sx, sy, tx, ty, pred_x, pred_y, sn, sm, tn, tm, category_id in tmp:\n",
    "            error_counts[sx-pred_x+resolution, sy-pred_y+resolution] += not correct\n",
    "            total_counts[sx-pred_x+resolution, sy-pred_y+resolution] += 1\n",
    "\n",
    "        error_count_scaled = error_counts.reshape(len(error_counts)//downscale, downscale, len(error_counts)//downscale, downscale).mean(axis=(1,3))\n",
    "        total_count_scaled = total_counts.reshape(len(total_counts)//downscale, downscale, len(total_counts)//downscale, downscale).mean(axis=(1,3))\n",
    "        error_rates[i][j,:,:] = error_count_scaled / total_count_scaled\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(n, m, figsize=(m*1, n*1))\n",
    "for i, block_name in enumerate(block_names):\n",
    "    axs[i,0].text(-0.8, 0.5, block_name, ha='right', va='center', transform=axs[i,0].transAxes)\n",
    "    for j, resolution in enumerate(resolutions):\n",
    "        lim_scaled_rel = int((1-lim)*resolution)//downscale\n",
    "        axs[i,j].imshow(error_rates[j][i,lim_scaled_rel:-lim_scaled_rel, lim_scaled_rel:-lim_scaled_rel], origin='lower', extent=(-resolution, resolution, -resolution, resolution))\n",
    "        axs[i,j].tick_params(labelsize=8)\n",
    "        if i != n-1: axs[i,j].set_xticks([])\n",
    "        else: axs[i,j].set_xticks([-resolution/2, 0, resolution/2])\n",
    "        if j != 0: axs[i,j].set_yticks([])\n",
    "        else: axs[i,j].set_yticks([-resolution/2, 0, resolution/2])\n",
    "        if i == n-1: axs[-1,j].text(0.5, -0.5, resolutions[j], ha='center', va='top', transform=axs[-1,j].transAxes)\n",
    "\n",
    "# x/y-labels\n",
    "axs[0,0].text(-1.8, -n/2, 'block | error rate', ha='right', va='center', transform=axs[0,0].transAxes, fontsize=12, rotation=90)\n",
    "axs[-1,0].text(m/2+1, -1.0, 'noise step | distance [px]', ha='center', va='top', transform=axs[-1,0].transAxes, fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "# np.save('sc_errors_by_relative_position_spair_maps_SD1.5.npy', error_rates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: adjust\n",
    "\n",
    "# error rate over prediction position relative to source position\n",
    "\n",
    "lim = 128\n",
    "downscale = 16\n",
    "lim_scaled_rel = (512-lim)//downscale\n",
    "\n",
    "# compute error rates\n",
    "error_rates = np.zeros((len(blocks), len(noise_steps), int((2**.5*512)**.5)))\n",
    "for i, block in enumerate(tqdm(blocks, desc='blocks')):\n",
    "    for j, noise_step in enumerate(noise_steps):\n",
    "        tmp = data[i, j]\n",
    "\n",
    "        error_counts = np.zeros(int((2**.5*512)**.5), dtype=int)\n",
    "        total_counts = np.zeros(int((2**.5*512)**.5), dtype=int)\n",
    "        for correct, sx, sy, tx, ty, pred_x, pred_y, sn, sm, tn, tm, category_id in tmp:\n",
    "            dist = ((sx-pred_x)**2 + (sy-pred_y)**2)**.25\n",
    "            error_counts[int(dist)] += not correct\n",
    "            total_counts[int(dist)] += 1\n",
    "\n",
    "        error_rates[i,j,:] = error_counts / total_counts\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(len(blocks), len(noise_steps), figsize=(len(noise_steps)*1, len(blocks)*1))\n",
    "for i, block in enumerate(blocks):\n",
    "    axs[i,0].text(-0.8, 0.5, block_names[i], ha='right', va='center', transform=axs[i,0].transAxes)\n",
    "    for j, noise_step in enumerate(noise_steps):\n",
    "        error_rate = error_rates[i,j,:]\n",
    "\n",
    "        axs[i,j].plot(np.arange(len(error_rate))**2, error_rate)\n",
    "        axs[i,j].set_xlim(-5, 105)\n",
    "        if i != len(blocks)-1: axs[i,j].set_xticks([])\n",
    "        else: axs[i,j].tick_params(labelsize=8)\n",
    "        if j != 0: axs[i,j].set_yticks([])\n",
    "        else: axs[i,j].tick_params(labelsize=8)\n",
    "        if i == len(blocks)-1: axs[-1,j].text(0.5, -0.5, noise_steps[j], ha='center', va='top', transform=axs[-1,j].transAxes)\n",
    "\n",
    "# set same ylim for each row (relative to the third lowest value)\n",
    "for i in range(len(blocks)):\n",
    "    ymin = 1 - (1 - sorted(ax.get_ylim()[0] for ax in axs[i,:])[2]) * 1.1\n",
    "    for ax in axs[i,:]:\n",
    "        ax.set_ylim(ymin, 1 + (1 - ymin) * 0.1)\n",
    "\n",
    "# x/y-labels\n",
    "axs[0,0].text(-1.8, -len(blocks)/2, 'block | error rate', ha='right', va='center', transform=axs[0,0].transAxes, fontsize=12, rotation=90)\n",
    "axs[-1,0].text(len(noise_steps)/2, -1.0, 'noise step | relative distance [px]', ha='center', va='top', transform=axs[-1,0].transAxes, fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "# np.save('sc_errors_by_relative_position_spair_lines_SD1.5.npy', error_rates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: adjust\n",
    "\n",
    "noise_index = 3\n",
    "\n",
    "block_names = [x.replace('_blocks', '').replace('_block', '').replace('_', '-') for x in blocks]\n",
    "colors = plt.cm.rainbow(np.linspace(0.0, 1.0, len(blocks)))\n",
    "fig, ax = plt.subplots()\n",
    "for i, block in enumerate(blocks):\n",
    "    tmp = data[i, noise_index]\n",
    "    error_counts = np.zeros(int((2**.5*512)**.5), dtype=int)\n",
    "    total_counts = np.zeros(int((2**.5*512)**.5), dtype=int)\n",
    "    for correct, sx, sy, tx, ty, pred_x, pred_y, sn, sm, tn, tm, category_id in tmp:\n",
    "        dist = ((sx-pred_x)**2 + (sy-pred_y)**2)**.25\n",
    "        error_counts[int(dist)] += not correct\n",
    "        total_counts[int(dist)] += 1\n",
    "    error_rate = error_counts / total_counts\n",
    "    ax.plot(np.arange(len(error_rate))**2, error_rate, label=block_names[i], alpha=0.7, color=colors[i])\n",
    "    ax.set_xlim(-5, 55)\n",
    "\n",
    "    # print error rate stats\n",
    "    print(block_names[i])\n",
    "    lowest_error_idx = error_rate[:-1].argmin()\n",
    "    print(f'min error rate at {lowest_error_idx**2} px')\n",
    "    for i in range(lowest_error_idx+1):\n",
    "        print(f'  {i**2} <= dist < {(i+1)**2} px: error rate {error_rate[i]:.2%}, count {total_counts[i]}')\n",
    "    extra_errors = sum((error_rate[i]-error_rate[lowest_error_idx])*total_counts[i] for i in range(lowest_error_idx))\n",
    "    print(f'-> extra errors: {extra_errors} ({extra_errors/total_counts.sum():.4%})')\n",
    "    print()\n",
    "ax.legend(loc='upper right', fontsize=8)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: adjust\n",
    "\n",
    "tmp = data[7, 3]  # up_blocks[3], noise_step 50\n",
    "error_counts = np.zeros(int((2**.5*512)**.5), dtype=int)\n",
    "total_counts = np.zeros(int((2**.5*512)**.5), dtype=int)\n",
    "for correct, sx, sy, tx, ty, pred_x, pred_y, sn, sm, tn, tm, category_id in tmp:\n",
    "    dist = ((sx-pred_x)**2 + (sy-pred_y)**2)**.25\n",
    "    error_counts[int(dist)] += not correct\n",
    "    total_counts[int(dist)] += 1\n",
    "error_rate = error_counts / total_counts\n",
    "\n",
    "plt.plot(np.arange(len(error_rate))**2, error_rate)\n",
    "plt.show()\n",
    "\n",
    "lowest_error_idx = error_rate[:-1].argmin()\n",
    "print(f'min error rate at {lowest_error_idx**2} px')\n",
    "for i in range(lowest_error_idx+1):\n",
    "    print(f'  {i**2} <= dist < {(i+1)**2} px: error rate {error_rate[i]:.2%}, count {total_counts[i]}')\n",
    "extra_errors = sum((error_rate[i]-error_rate[lowest_error_idx])*total_counts[i] for i in range(lowest_error_idx))\n",
    "print(f'-> extra errors: {extra_errors} ({extra_errors/total_counts.sum():.4%})')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: adjust\n",
    "\n",
    "# error rate over prediction position relative to target position \n",
    "\n",
    "lim = 128\n",
    "downscale = 16\n",
    "lim_scaled_rel = (512-lim)//downscale\n",
    "\n",
    "fig, axs = plt.subplots(len(blocks), len(noise_steps), figsize=(len(blocks)*1, len(noise_steps)*1))\n",
    "for i, block in enumerate(tqdm(blocks, desc='blocks')):\n",
    "    axs[i,0].text(-0.8, 0.5, block_names[i], ha='right', va='center', transform=axs[i,0].transAxes)\n",
    "    for j, noise_step in enumerate(noise_steps):\n",
    "        tmp = data[i, j]\n",
    "\n",
    "        error_counts = np.zeros((2*512, 2*512), dtype=int)\n",
    "        total_counts = np.zeros((2*512, 2*512), dtype=int)\n",
    "        for correct, sx, sy, tx, ty, pred_x, pred_y, sn, sm, tn, tm, category_id in tmp:\n",
    "            error_counts[tx-pred_x+512, ty-pred_y+512] += not correct\n",
    "            total_counts[tx-pred_x+512, ty-pred_y+512] += 1\n",
    "\n",
    "        error_count_scaled = error_counts.reshape(len(error_counts)//downscale, downscale, len(error_counts)//downscale, downscale).mean(axis=(1,3))\n",
    "        total_count_scaled = total_counts.reshape(len(total_counts)//downscale, downscale, len(total_counts)//downscale, downscale).mean(axis=(1,3))\n",
    "        error_rate_scaled = error_count_scaled / total_count_scaled\n",
    "\n",
    "        axs[i,j].imshow(error_rate_scaled[lim_scaled_rel:-lim_scaled_rel, lim_scaled_rel:-lim_scaled_rel], origin='lower', extent=(-lim, lim, -lim, lim))\n",
    "        if i != len(blocks)-1: axs[i,j].set_xticks([])\n",
    "        else: axs[i,j].tick_params(labelsize=8)\n",
    "        if j != 0: axs[i,j].set_yticks([])\n",
    "        else: axs[i,j].tick_params(labelsize=8)\n",
    "        if i == len(blocks)-1: axs[-1,j].text(0.5, -0.5, noise_steps[j], ha='center', va='top', transform=axs[-1,j].transAxes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texture bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spair_data = datasets.load_dataset('0jl/SPair-71k', 'data', split='train', trust_remote_code=True)\n",
    "spair_pairs = datasets.load_dataset('0jl/SPair-71k', 'pairs', split='test', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spair_images = [x['img'] for x in tqdm(spair_data)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spair_images_max_x = 0\n",
    "spair_images_max_y = 0\n",
    "for img in spair_images:\n",
    "    spair_images_max_x = max(spair_images_max_x, img.size[0])\n",
    "    spair_images_max_y = max(spair_images_max_y, img.size[1])\n",
    "\n",
    "spair_images_data = np.zeros((len(spair_images), spair_images_max_y, spair_images_max_x, 3), dtype=np.uint8)\n",
    "for i, img in enumerate(tqdm(spair_images)):\n",
    "    spair_images_data[i, :img.size[1], :img.size[0], :] = np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spair_kps_list = []\n",
    "for pair in tqdm(spair_pairs):\n",
    "    for (sx, sy), (tx, ty) in zip(pair['src_kps'], pair['trg_kps']):\n",
    "        spair_kps_list.append((pair['src_data_index'], sx, sy, pair['trg_data_index'], tx, ty))\n",
    "\n",
    "spair_kps = np.array(spair_kps_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape, spair_kps.shape, spair_images_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: adjust\n",
    "\n",
    "src_colors = spair_images_data[*spair_kps[:,[0,2,1]].T]\n",
    "trg_colors = spair_images_data[*spair_kps[:,[3,5,4]].T]\n",
    "\n",
    "src_colors.shape, trg_colors.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: adjust\n",
    "\n",
    "color_diffs = np.abs(src_colors - trg_colors).mean(axis=-1)\n",
    "fig, axs = plt.subplots(len(blocks), 1, figsize=(4,2*len(blocks)))\n",
    "for i in range(len(blocks)):\n",
    "    correct_at_50 = data[i,3,:,0]\n",
    "    axs[i].hist(color_diffs[correct_at_50==1], bins=np.linspace(0, 256, 257), density=True, label='correct')\n",
    "    axs[i].hist(color_diffs[correct_at_50==0], bins=np.linspace(0, 256, 257), density=True, alpha=0.5, label='wrong')\n",
    "    axs[i].legend()\n",
    "    axs[i].set_title(block_names[i])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# if there is some color bias in some layers, we maybe would expect to be more correct for smaller color differences\n",
    "# this doesn't seem to be the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: adjust\n",
    "\n",
    "d=7\n",
    "spair_images_data_padded = np.zeros((spair_images_data.shape[0], spair_images_data.shape[1]+2*d, spair_images_data.shape[2]+2*d, 3), dtype=spair_images_data.dtype)\n",
    "spair_images_data_padded[:,d:-d,d:-d,:] = spair_images_data\n",
    "src_regions = np.array([spair_images_data_padded[i, y:y+2*d+1, x:x+2*d+1] for i,x,y in spair_kps[:,[0,1,2]]])\n",
    "trg_regions = np.array([spair_images_data_padded[i, y:y+2*d+1, x:x+2*d+1] for i,x,y in spair_kps[:,[3,4,5]]])\n",
    "\n",
    "src_regions.shape, trg_regions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: adjust\n",
    "\n",
    "texture_diffs = np.abs(src_regions - trg_regions).mean(axis=-1).mean(axis=-1).mean(axis=-1)\n",
    "fig, axs = plt.subplots(len(blocks), 1, figsize=(4,2*len(blocks)))\n",
    "for i in range(len(blocks)):\n",
    "    correct_at_50 = data[i,3,:,0]\n",
    "    axs[i].hist(color_diffs[correct_at_50==1], bins=np.linspace(0, 256, 257), density=True, label='correct')\n",
    "    axs[i].hist(color_diffs[correct_at_50==0], bins=np.linspace(0, 256, 257), density=True, alpha=0.5, label='wrong')\n",
    "    axs[i].legend()\n",
    "    axs[i].set_title(block_names[i])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# if there is some texture bias in some layers, we maybe would expect to be more correct for smaller texture differences\n",
    "# this doesn't seem to be the case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
